#!/usr/bin/env python3
"""
è°ƒè¯•æ–‡æ¡£åˆ†å—é—®é¢˜
"""
import os
import sqlite3
import json
from llama_index.core.readers import SimpleDirectoryReader
from llama_index.core.node_parser import SentenceSplitter

def debug_chunking_process():
    """è°ƒè¯•æ–‡æ¡£åˆ†å—è¿‡ç¨‹"""
    print("ğŸ” è°ƒè¯•æ–‡æ¡£åˆ†å—è¿‡ç¨‹")
    print("=" * 60)
    
    # 1. è¯»å–åŸå§‹æ–‡æ¡£
    data_dir = os.environ.get("DATA_DIR", "data")
    reader = SimpleDirectoryReader(data_dir, recursive=True)
    documents = reader.load_data()
    
    print(f"ğŸ“„ è¯»å–åˆ° {len(documents)} ä¸ªæ–‡æ¡£:")
    for i, doc in enumerate(documents):
        print(f"  {i+1}. {doc.metadata.get('file_name', 'Unknown')} - {len(doc.text)} å­—ç¬¦")
        print(f"     å†…å®¹é¢„è§ˆ: {doc.text[:100]}...")
        print(f"     doc_id: {doc.doc_id}")
        print()
    
    # 2. æµ‹è¯•åˆ†å—å™¨
    parser = SentenceSplitter()
    nodes = parser.get_nodes_from_documents(documents)
    
    print(f"ğŸ§© åˆ†å—ç»“æœ: {len(nodes)} ä¸ªèŠ‚ç‚¹")
    print()
    
    # 3. æŒ‰æ–‡æ¡£åˆ†ç»„åˆ†æ
    doc_groups = {}
    for node in nodes:
        ref_doc_id = node.ref_doc_id
        if ref_doc_id not in doc_groups:
            doc_groups[ref_doc_id] = []
        doc_groups[ref_doc_id].append(node)
    
    print("ğŸ“Š æŒ‰æ–‡æ¡£åˆ†ç»„çš„èŠ‚ç‚¹:")
    for doc_id, doc_nodes in doc_groups.items():
        # æ‰¾åˆ°å¯¹åº”çš„åŸå§‹æ–‡æ¡£
        original_doc = next((d for d in documents if d.doc_id == doc_id), None)
        file_name = original_doc.metadata.get('file_name', 'Unknown') if original_doc else 'Unknown'
        
        print(f"ğŸ“„ {file_name} (doc_id: {doc_id[:20]}...):")
        print(f"   åŸå§‹æ–‡æ¡£é•¿åº¦: {len(original_doc.text) if original_doc else 'Unknown'} å­—ç¬¦")
        print(f"   åˆ†å—æ•°é‡: {len(doc_nodes)}")
        
        for i, node in enumerate(doc_nodes):
            print(f"   å— {i}: {len(node.text)} å­—ç¬¦ - {node.text[:50]}...")
            print(f"        node_id: {node.node_id[:20]}...")
        print()

def check_database_vs_actual():
    """å¯¹æ¯”æ•°æ®åº“ä¸­çš„æ•°æ®å’Œå®é™…åˆ†å—ç»“æœ"""
    print("ğŸ” å¯¹æ¯”æ•°æ®åº“æ•°æ®å’Œå®é™…åˆ†å—")
    print("=" * 60)
    
    # æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ•°æ®
    with sqlite3.connect('storage/docstore.db') as conn:
        cursor = conn.execute('''
            SELECT file_name, COUNT(*) as db_chunks,
                   COUNT(DISTINCT LENGTH(data)) as unique_lengths,
                   MIN(LENGTH(data)) as min_length,
                   MAX(LENGTH(data)) as max_length
            FROM documents 
            GROUP BY file_name
        ''')
        
        print("ğŸ“Š æ•°æ®åº“ä¸­çš„æ•°æ®:")
        print("æ–‡ä»¶å                | å—æ•° | å”¯ä¸€é•¿åº¦æ•° | æœ€å°é•¿åº¦ | æœ€å¤§é•¿åº¦")
        print("-" * 70)
        
        for row in cursor.fetchall():
            file_name, db_chunks, unique_lengths, min_length, max_length = row
            print(f"{file_name:<20} | {db_chunks:<4} | {unique_lengths:<10} | {min_length:<8} | {max_length}")
        
        print()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒå†…å®¹çš„å—
        cursor = conn.execute('''
            SELECT file_name, data, COUNT(*) as count
            FROM documents 
            GROUP BY file_name, data
            HAVING COUNT(*) > 1
        ''')
        
        duplicates = cursor.fetchall()
        if duplicates:
            print("âš ï¸  å‘ç°é‡å¤çš„æ–‡æ¡£å—:")
            for file_name, data, count in duplicates:
                content_preview = json.loads(data).get('text', '')[:100] if data else ''
                print(f"  {file_name}: {count} ä¸ªç›¸åŒçš„å— - {content_preview}...")
        else:
            print("âœ… æ²¡æœ‰å‘ç°é‡å¤çš„æ–‡æ¡£å—")

def analyze_chunk_index_logic():
    """åˆ†æ chunk_index é€»è¾‘"""
    print(f"\nğŸ” åˆ†æ chunk_index é€»è¾‘é—®é¢˜")
    print("=" * 60)
    
    print("ğŸ“‹ generate.py ä¸­çš„é€»è¾‘:")
    print("1. è¯»å–æ‰€æœ‰æ–‡æ¡£")
    print("2. ä½¿ç”¨ SentenceSplitter åˆ†å—")
    print("3. ä¸ºæ¯ä¸ªæ–‡æ¡£æ‰¾åˆ°å¯¹åº”çš„èŠ‚ç‚¹: doc_nodes = [node for node in nodes if node.ref_doc_id == document.doc_id]")
    print("4. ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ†é… chunk_index: 'chunk_index': i")
    
    print(f"\nğŸ¤” å¯èƒ½çš„é—®é¢˜:")
    print("1. å¦‚æœæ–‡æ¡£æ²¡æœ‰è¢«æ­£ç¡®åˆ†å—ï¼Œæ¯ä¸ªæ–‡æ¡£åªæœ‰ä¸€ä¸ªèŠ‚ç‚¹")
    print("2. é‡å¤è¿è¡Œå¯¼è‡´ç›¸åŒå†…å®¹è¢«å¤šæ¬¡æ·»åŠ ")
    print("3. æ•°æ®åº“ä¸­çš„ chunk_index éƒ½æ˜¯ 0ï¼Œè¯´æ˜æ¯ä¸ªæ–‡æ¡£ç»„åªæœ‰ä¸€ä¸ªèŠ‚ç‚¹")
    
    print(f"\nğŸ’¡ éªŒè¯æ–¹æ³•:")
    print("1. æ£€æŸ¥å®é™…çš„åˆ†å—ç»“æœ")
    print("2. ç¡®è®¤æ˜¯å¦çœŸçš„æœ‰å¤šä¸ªå—")
    print("3. æ£€æŸ¥é‡å¤æ•°æ®çš„æ¥æº")

def suggest_solution():
    """å»ºè®®è§£å†³æ–¹æ¡ˆ"""
    print(f"\nğŸ’¡ è§£å†³æ–¹æ¡ˆ")
    print("=" * 60)
    
    print("ğŸ”§ ç«‹å³è§£å†³:")
    print("1. é‡ç½®æ•°æ®åº“æ¸…é™¤é‡å¤æ•°æ®")
    print("2. æ£€æŸ¥æ–‡æ¡£åˆ†å—è®¾ç½®")
    print("3. ç¡®ä¿ generate åªè¿è¡Œä¸€æ¬¡")
    
    print(f"\nğŸ”§ é•¿æœŸæ”¹è¿›:")
    print("1. æ·»åŠ å»é‡é€»è¾‘ï¼Œé¿å…é‡å¤å¤„ç†ç›¸åŒæ–‡æ¡£")
    print("2. æ”¹è¿› chunk_index çš„è®¡ç®—å’ŒéªŒè¯")
    print("3. æ·»åŠ æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥")

def main():
    """ä¸»è°ƒè¯•æµç¨‹"""
    debug_chunking_process()
    check_database_vs_actual()
    analyze_chunk_index_logic()
    suggest_solution()

if __name__ == "__main__":
    main()
