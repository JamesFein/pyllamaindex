#!/usr/bin/env python3
"""
æµ‹è¯•é‡å¤æ–‡æœ¬å—çš„å¤„ç†æœºåˆ¶
"""
import os
import tempfile
import logging
from llama_index.core.schema import TextNode
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.readers import SimpleDirectoryReader
from app.storage_config import get_storage_context

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_test_files_with_duplicate_content():
    """åˆ›å»ºåŒ…å«é‡å¤å†…å®¹çš„æµ‹è¯•æ–‡ä»¶"""
    test_dir = "test_duplicate_data"
    os.makedirs(test_dir, exist_ok=True)
    
    # åˆ›å»ºä¸¤ä¸ªæ–‡ä»¶ï¼ŒåŒ…å«ç›¸åŒçš„æ–‡æœ¬å—
    duplicate_content = """
è¿™æ˜¯ä¸€æ®µé‡å¤çš„æ–‡æœ¬å†…å®¹ã€‚
è¿™æ®µå†…å®¹ä¼šå‡ºç°åœ¨å¤šä¸ªæ–‡ä»¶ä¸­ã€‚
ç”¨äºæµ‹è¯•ç³»ç»Ÿå¦‚ä½•å¤„ç†é‡å¤çš„æ–‡æœ¬å—ã€‚
"""
    
    # æ–‡ä»¶1
    with open(os.path.join(test_dir, "file1.txt"), "w", encoding="utf-8") as f:
        f.write("æ–‡ä»¶1çš„å¼€å¤´å†…å®¹ã€‚\n")
        f.write(duplicate_content)
        f.write("æ–‡ä»¶1çš„ç»“å°¾å†…å®¹ã€‚\n")
    
    # æ–‡ä»¶2
    with open(os.path.join(test_dir, "file2.txt"), "w", encoding="utf-8") as f:
        f.write("æ–‡ä»¶2çš„å¼€å¤´å†…å®¹ã€‚\n")
        f.write(duplicate_content)
        f.write("æ–‡ä»¶2çš„ç»“å°¾å†…å®¹ã€‚\n")
    
    # æ–‡ä»¶3 - å®Œå…¨ç›¸åŒçš„å†…å®¹
    with open(os.path.join(test_dir, "file3.txt"), "w", encoding="utf-8") as f:
        f.write(duplicate_content)
    
    return test_dir

def analyze_node_generation():
    """åˆ†æèŠ‚ç‚¹ç”Ÿæˆè¿‡ç¨‹"""
    logger.info("ğŸ” åˆ†æèŠ‚ç‚¹ç”Ÿæˆè¿‡ç¨‹...")
    
    test_dir = create_test_files_with_duplicate_content()
    
    try:
        # è¯»å–æ–‡æ¡£
        reader = SimpleDirectoryReader(test_dir)
        documents = reader.load_data()
        
        logger.info(f"ğŸ“„ è¯»å–äº† {len(documents)} ä¸ªæ–‡æ¡£")
        
        # è§£æä¸ºèŠ‚ç‚¹
        parser = SentenceSplitter(chunk_size=100, chunk_overlap=20)
        nodes = parser.get_nodes_from_documents(documents)
        
        logger.info(f"ğŸ“ ç”Ÿæˆäº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
        
        # åˆ†æèŠ‚ç‚¹å†…å®¹å’ŒID
        content_to_nodes = {}
        node_ids = []
        node_hashes = []
        
        for i, node in enumerate(nodes):
            content = node.text.strip()
            node_id = node.node_id
            node_hash = getattr(node, 'hash', 'No hash')
            
            logger.info(f"èŠ‚ç‚¹ {i+1}:")
            logger.info(f"  ID: {node_id}")
            logger.info(f"  Hash: {node_hash}")
            logger.info(f"  å†…å®¹: {content[:50]}...")
            logger.info(f"  æ¥æº: {node.metadata.get('file_name', 'Unknown')}")
            
            # è®°å½•ç›¸åŒå†…å®¹çš„èŠ‚ç‚¹
            if content in content_to_nodes:
                content_to_nodes[content].append((i+1, node_id, node.metadata.get('file_name', 'Unknown')))
            else:
                content_to_nodes[content] = [(i+1, node_id, node.metadata.get('file_name', 'Unknown'))]
            
            node_ids.append(node_id)
            node_hashes.append(node_hash)
        
        # åˆ†æé‡å¤å†…å®¹
        logger.info("\nğŸ” é‡å¤å†…å®¹åˆ†æ:")
        duplicate_found = False
        for content, node_list in content_to_nodes.items():
            if len(node_list) > 1:
                duplicate_found = True
                logger.info(f"âš ï¸  å‘ç°é‡å¤å†…å®¹ (å‡ºç° {len(node_list)} æ¬¡):")
                logger.info(f"   å†…å®¹: {content[:50]}...")
                for node_num, node_id, file_name in node_list:
                    logger.info(f"   - èŠ‚ç‚¹ {node_num} (ID: {node_id}) æ¥è‡ª {file_name}")
        
        if not duplicate_found:
            logger.info("âœ… æ²¡æœ‰å‘ç°é‡å¤å†…å®¹")
        
        # åˆ†æIDå’ŒHashçš„å”¯ä¸€æ€§
        logger.info(f"\nğŸ” IDå”¯ä¸€æ€§åˆ†æ:")
        unique_ids = set(node_ids)
        logger.info(f"   æ€»èŠ‚ç‚¹æ•°: {len(node_ids)}")
        logger.info(f"   å”¯ä¸€IDæ•°: {len(unique_ids)}")
        if len(node_ids) == len(unique_ids):
            logger.info("âœ… æ‰€æœ‰èŠ‚ç‚¹IDéƒ½æ˜¯å”¯ä¸€çš„")
        else:
            logger.info("âš ï¸  å‘ç°é‡å¤çš„èŠ‚ç‚¹ID")
        
        return nodes, content_to_nodes
        
    finally:
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        import shutil
        shutil.rmtree(test_dir, ignore_errors=True)

def test_storage_behavior():
    """æµ‹è¯•å­˜å‚¨è¡Œä¸º"""
    logger.info("ğŸ” æµ‹è¯•å­˜å‚¨è¡Œä¸º...")
    
    # åˆ›å»ºæµ‹è¯•å­˜å‚¨
    test_storage_dir = "test_storage"
    storage_context = get_storage_context(test_storage_dir)
    
    try:
        # åˆ›å»ºä¸¤ä¸ªå†…å®¹ç›¸åŒä½†IDä¸åŒçš„èŠ‚ç‚¹
        node1 = TextNode(
            text="è¿™æ˜¯é‡å¤çš„æµ‹è¯•å†…å®¹",
            metadata={"source": "file1.txt"}
        )
        
        node2 = TextNode(
            text="è¿™æ˜¯é‡å¤çš„æµ‹è¯•å†…å®¹",  # ç›¸åŒå†…å®¹
            metadata={"source": "file2.txt"}
        )
        
        logger.info(f"èŠ‚ç‚¹1 ID: {node1.node_id}")
        logger.info(f"èŠ‚ç‚¹2 ID: {node2.node_id}")
        logger.info(f"å†…å®¹ç›¸åŒ: {node1.text == node2.text}")
        logger.info(f"IDç›¸åŒ: {node1.node_id == node2.node_id}")
        
        # æ·»åŠ åˆ°å­˜å‚¨
        storage_context.docstore.add_documents([node1, node2])
        
        # æ£€æŸ¥å­˜å‚¨ç»“æœ
        stored_node1 = storage_context.docstore.get_document(node1.node_id)
        stored_node2 = storage_context.docstore.get_document(node2.node_id)
        
        logger.info("âœ… ä¸¤ä¸ªèŠ‚ç‚¹éƒ½æˆåŠŸå­˜å‚¨")
        logger.info(f"å­˜å‚¨çš„èŠ‚ç‚¹1: {stored_node1.text[:30]}...")
        logger.info(f"å­˜å‚¨çš„èŠ‚ç‚¹2: {stored_node2.text[:30]}...")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ å­˜å‚¨æµ‹è¯•å¤±è´¥: {e}")
        return False
        
    finally:
        # æ¸…ç†æµ‹è¯•å­˜å‚¨
        import shutil
        shutil.rmtree(test_storage_dir, ignore_errors=True)

def analyze_vector_store_behavior():
    """åˆ†æå‘é‡å­˜å‚¨è¡Œä¸º"""
    logger.info("ğŸ” åˆ†æå‘é‡å­˜å‚¨è¡Œä¸º...")
    
    # å¯¹äºç›¸åŒå†…å®¹çš„æ–‡æœ¬å—ï¼š
    # 1. LlamaIndexä¼šä¸ºæ¯ä¸ªèŠ‚ç‚¹ç”Ÿæˆå”¯ä¸€çš„node_id
    # 2. å³ä½¿å†…å®¹ç›¸åŒï¼Œä¹Ÿä¼šè¢«è§†ä¸ºä¸åŒçš„èŠ‚ç‚¹
    # 3. ä¼šç”Ÿæˆç›¸åŒæˆ–éå¸¸ç›¸ä¼¼çš„å‘é‡è¡¨ç¤º
    # 4. åœ¨æ£€ç´¢æ—¶å¯èƒ½ä¼šè¿”å›å¤šä¸ªç›¸ä¼¼çš„ç»“æœ
    
    logger.info("ğŸ“Š å‘é‡å­˜å‚¨è¡Œä¸ºåˆ†æ:")
    logger.info("1. âœ… æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰å”¯ä¸€çš„node_idï¼Œå³ä½¿å†…å®¹ç›¸åŒ")
    logger.info("2. âœ… ç›¸åŒå†…å®¹ä¼šç”Ÿæˆç›¸åŒ/ç›¸ä¼¼çš„å‘é‡")
    logger.info("3. âš ï¸  æ£€ç´¢æ—¶å¯èƒ½è¿”å›å¤šä¸ªç›¸åŒå†…å®¹çš„ç»“æœ")
    logger.info("4. âš ï¸  ä¼šå ç”¨é¢å¤–çš„å­˜å‚¨ç©ºé—´")
    logger.info("5. âš ï¸  å¯èƒ½å½±å“æ£€ç´¢ç»“æœçš„å¤šæ ·æ€§")

def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("ğŸ§ª é‡å¤æ–‡æœ¬å—å¤„ç†æœºåˆ¶æµ‹è¯•")
    print("=" * 50)
    
    # 1. åˆ†æèŠ‚ç‚¹ç”Ÿæˆ
    nodes, content_analysis = analyze_node_generation()
    
    print()
    
    # 2. æµ‹è¯•å­˜å‚¨è¡Œä¸º
    test_storage_behavior()
    
    print()
    
    # 3. åˆ†æå‘é‡å­˜å‚¨è¡Œä¸º
    analyze_vector_store_behavior()
    
    print()
    print("ğŸ“‹ æ€»ç»“:")
    print("=" * 50)
    print("ğŸ” å½“æ–°æ–‡æœ¬å—ä¸æ—§æ–‡æœ¬å—å†…å®¹å®Œå…¨ç›¸åŒæ—¶:")
    print("1. âœ… ç³»ç»Ÿä¼šä¸ºæ¯ä¸ªæ–‡æœ¬å—ç”Ÿæˆå”¯ä¸€çš„node_id")
    print("2. âœ… ä¸¤ä¸ªæ–‡æœ¬å—éƒ½ä¼šè¢«å­˜å‚¨ï¼ˆä¸ä¼šå»é‡ï¼‰")
    print("3. âœ… ä¼šç”Ÿæˆç›¸åŒæˆ–éå¸¸ç›¸ä¼¼çš„å‘é‡è¡¨ç¤º")
    print("4. âš ï¸  æ£€ç´¢æ—¶å¯èƒ½è¿”å›å¤šä¸ªç›¸åŒå†…å®¹çš„ç»“æœ")
    print("5. âš ï¸  ä¼šå ç”¨é¢å¤–çš„å­˜å‚¨ç©ºé—´å’Œè®¡ç®—èµ„æº")
    print()
    print("ğŸ’¡ å»ºè®®:")
    print("- å¦‚æœéœ€è¦å»é‡ï¼Œåº”åœ¨æ–‡æ¡£é¢„å¤„ç†é˜¶æ®µå®ç°")
    print("- å¯ä»¥è€ƒè™‘åŸºäºå†…å®¹hashçš„å»é‡æœºåˆ¶")
    print("- æˆ–è€…åœ¨æ£€ç´¢åè¿›è¡Œç»“æœå»é‡")

if __name__ == "__main__":
    main()
