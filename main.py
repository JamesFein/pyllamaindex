import logging
import os

from app.settings import init_settings
from app.workflow import create_workflow
from app.index import get_index
from dotenv import load_dotenv
from llama_index.server import LlamaIndexServer, UIConfig
from fastapi.responses import JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from fastapi import UploadFile, File, HTTPException
from typing import List
import asyncio
import uuid
import shutil

logger = logging.getLogger("uvicorn")

# A path to a directory where the customized UI code is stored
COMPONENT_DIR = "components"


def create_app():
    app = LlamaIndexServer(
        workflow_factory=create_workflow,  # A factory function that creates a new workflow for each request
        ui_config=UIConfig(
            enabled=False,  # ç¦ç”¨é»˜è®¤UI
        ),
        logger=logger,
        env="dev",
    )

    # æ·»åŠ è‡ªå®šä¹‰ä¸­é—´ä»¶æ¥å¤„ç†sourcesæ³¨è§£
    from fastapi import Request, Response
    from starlette.middleware.base import BaseHTTPMiddleware
    import re
    import json

    class SourcesAnnotationMiddleware(BaseHTTPMiddleware):
        async def dispatch(self, request: Request, call_next):
            response = await call_next(request)

            # åªå¤„ç†chat APIçš„å“åº”
            if request.url.path == "/api/chat" and request.method == "POST":
                # å¦‚æœæ˜¯æµå¼å“åº”ï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹å“åº”å†…å®¹
                if response.headers.get("content-type", "").startswith("text/plain"):
                    # è¯»å–åŸå§‹å“åº”å†…å®¹
                    body = b""
                    async for chunk in response.body_iterator:
                        body += chunk

                    # è§£ç å“åº”å†…å®¹
                    content = body.decode('utf-8')

                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¼•ç”¨
                    citation_pattern = r'\[citation:([^\]]+)\]'
                    citation_ids = re.findall(citation_pattern, content)

                    if citation_ids:
                        # è·å–ç´¢å¼•ä»¥è·å–æºèŠ‚ç‚¹
                        try:
                            from app.index import get_index
                            index = get_index()
                            if index:
                                nodes = []
                                for citation_id in citation_ids:
                                    try:
                                        node = index.docstore.get_node(citation_id)
                                        nodes.append({
                                            "id": node.node_id,
                                            "text": node.text,
                                            "metadata": node.metadata,
                                            "score": getattr(node, 'score', None)
                                        })
                                    except:
                                        continue

                                if nodes:
                                    # æ·»åŠ sourcesæ³¨è§£åˆ°å“åº”ä¸­
                                    sources_annotation = {
                                        "type": "sources",
                                        "data": {"nodes": nodes}
                                    }

                                    # ä¿®æ”¹å“åº”å†…å®¹ï¼Œæ·»åŠ sourcesæ³¨è§£
                                    content += f'\n8:[{json.dumps(sources_annotation)}]\n'
                        except Exception as e:
                            logger.error(f"Error adding sources annotation: {e}")

                    # åˆ›å»ºæ–°çš„å“åº”
                    return Response(
                        content=content,
                        status_code=response.status_code,
                        headers=dict(response.headers),
                        media_type=response.media_type
                    )

            return response

    app.add_middleware(SourcesAnnotationMiddleware)

    # æ·»åŠ CORSä¸­é—´ä»¶
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["http://localhost:3000", "http://127.0.0.1:3000", "http://localhost:3001", "http://127.0.0.1:3001"],  # Next.jså¼€å‘æœåŠ¡å™¨
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    # You can also add custom FastAPI routes to app
    app.add_api_route("/api/health", lambda: {"message": "OK"}, status_code=200)

    #==============================AI æ·»åŠ å¼•ç”¨APIç«¯ç‚¹ start
    async def get_citation_endpoint(citation_id: str):
        """è·å–å¼•ç”¨å†…å®¹çš„APIç«¯ç‚¹"""
        try:
            # ä»ç´¢å¼•ä¸­è·å–èŠ‚ç‚¹
            index = get_index()
            if not index:
                return JSONResponse(
                    status_code=404,
                    content={"error": "Index not found"}
                )
            
            # å°è¯•ä»æ–‡æ¡£å­˜å‚¨ä¸­è·å–èŠ‚ç‚¹
            docstore = index.docstore
            
            # é¦–å…ˆå°è¯•ç›´æ¥è·å–
            try:
                node = docstore.get_node(citation_id)
                return {
                    "id": node.node_id,
                    "text": node.text,
                    "metadata": node.metadata
                }
            except:
                # å¦‚æœç›´æ¥è·å–å¤±è´¥ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
                all_nodes = docstore.get_all_nodes()
                matching_nodes = [node for node in all_nodes if citation_id in node.node_id]
                
                if matching_nodes:
                    node = matching_nodes[0]
                    return {
                        "id": node.node_id,
                        "text": node.text,
                        "metadata": node.metadata
                    }
                else:
                    return JSONResponse(
                        status_code=404,
                        content={"error": f"Citation with ID {citation_id} not found"}
                    )
        except Exception as e:
            logger.error(f"Error retrieving citation: {str(e)}")
            return JSONResponse(
                status_code=500,
                content={"error": f"Error retrieving citation: {str(e)}"}
            )

    # æ³¨å†Œå¼•ç”¨APIç«¯ç‚¹
    app.add_api_route("/api/citation/{citation_id}", get_citation_endpoint, methods=["GET"])
    # ==============================AI æ·»åŠ å¼•ç”¨APIç«¯ç‚¹ end

    # ==============================AI æ·»åŠ æ–‡æ¡£ç®¡ç†APIç«¯ç‚¹ start
    async def get_documents_endpoint():
        """è·å–æ–‡æ¡£åˆ—è¡¨çš„APIç«¯ç‚¹ - è¿”å›æ–‡ä»¶çº§åˆ«çš„æ–‡æ¡£ï¼Œè€Œä¸æ˜¯æ–‡æœ¬å—"""
        try:
            from app.storage_config import load_storage_context
            from app.index import STORAGE_DIR

            storage_context = load_storage_context(STORAGE_DIR)
            if not storage_context:
                return JSONResponse(
                    status_code=404,
                    content={"error": "Storage context not found"}
                )

            # è·å–æ–‡ä»¶åˆ—è¡¨ï¼ˆè€Œä¸æ˜¯æ–‡æ¡£å—åˆ—è¡¨ï¼‰
            files = storage_context.docstore.get_files_with_metadata()

            return JSONResponse(content={
                "documents": files,  # ä¿æŒAPIå…¼å®¹æ€§ï¼Œä½†å®é™…è¿”å›çš„æ˜¯æ–‡ä»¶
                "total": len(files)
            })

        except Exception as e:
            logger.error(f"Error getting documents: {e}")
            return JSONResponse(
                status_code=500,
                content={"error": f"Failed to get documents: {str(e)}"}
            )

    async def upload_documents_endpoint(files: List[UploadFile] = File(...)):
        """ä¸Šä¼ æ–‡æ¡£çš„APIç«¯ç‚¹ - ä½¿ç”¨æ–°çš„æ–‡ä»¶ç®¡ç†ç»“æ„"""
        try:
            import hashlib
            from app.storage_config import get_storage_context
            from app.index import STORAGE_DIR
            from llama_index.core.readers import SimpleDirectoryReader
            from llama_index.core.node_parser import SentenceSplitter
            from llama_index.core.indices import VectorStoreIndex

            # ç¡®ä¿dataç›®å½•å­˜åœ¨
            data_dir = os.environ.get("DATA_DIR", "data")
            os.makedirs(data_dir, exist_ok=True)

            upload_results = []

            for file in files:
                try:
                    # ç›´æ¥ä½¿ç”¨åŸå§‹æ–‡ä»¶åï¼ŒåŒåæ–‡ä»¶è§†ä¸ºåŒä¸€ä»½æ–‡ä»¶
                    file_path = os.path.join(data_dir, file.filename)

                    # ä½¿ç”¨æ–‡ä»¶åçš„å“ˆå¸Œä½œä¸ºæ–‡ä»¶IDï¼Œç¡®ä¿åŒåæ–‡ä»¶æœ‰ç›¸åŒçš„ID
                    file_id = f"file_{hashlib.md5(file.filename.encode()).hexdigest()[:8]}"

                    # è·å–å­˜å‚¨ä¸Šä¸‹æ–‡
                    storage_context = get_storage_context(STORAGE_DIR)

                    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨åŒåæ–‡ä»¶ï¼Œå¦‚æœå­˜åœ¨åˆ™å®Œå…¨åˆ é™¤æ—§æ–‡ä»¶
                    existing_file_info = storage_context.docstore.get_file_info(file_id)
                    if existing_file_info:
                        logger.info(f"Found existing file with same name: {file.filename}, deleting old version...")

                        # 1. åˆ é™¤æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ—§æ–‡ä»¶
                        old_file_path = existing_file_info['path']
                        if os.path.exists(old_file_path):
                            os.remove(old_file_path)
                            logger.info(f"Deleted old file: {old_file_path}")

                        # 2. åˆ é™¤æ•°æ®åº“ä¸­çš„æ–‡ä»¶è®°å½•å’Œæ‰€æœ‰ç›¸å…³æ–‡æ¡£å—
                        success, doc_ids_to_delete = storage_context.docstore.delete_file_and_chunks(file_id)
                        logger.info(f"Deleted old file record and chunks for: {file.filename}")

                        # 3. ğŸ”§ ä¿®å¤ï¼šåˆ é™¤ ChromaDB ä¸­çš„å‘é‡æ•°æ®
                        if doc_ids_to_delete:
                            try:
                                # è·å– ChromaDB é›†åˆ
                                chroma_collection = storage_context.vector_store._collection
                                # åˆ é™¤å¯¹åº”çš„å‘é‡
                                chroma_collection.delete(ids=doc_ids_to_delete)
                                logger.info(f"Deleted {len(doc_ids_to_delete)} vectors from ChromaDB")
                            except Exception as e:
                                logger.warning(f"Failed to delete vectors from ChromaDB: {e}")

                        # 4. é‡æ–°æŒä¹…åŒ–å­˜å‚¨ä¸Šä¸‹æ–‡ä»¥æ¸…ç†å‘é‡ç´¢å¼•
                        storage_context.persist(STORAGE_DIR)

                    # ä¿å­˜æ–°æ–‡ä»¶
                    with open(file_path, "wb") as buffer:
                        shutil.copyfileobj(file.file, buffer)

                    # è·å–æ–‡ä»¶ä¿¡æ¯
                    file_size = os.path.getsize(file_path)
                    file_type = file.content_type or "text/plain"

                    # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
                    file_hash = hashlib.md5()
                    with open(file_path, "rb") as f:
                        for chunk in iter(lambda: f.read(4096), b""):
                            file_hash.update(chunk)
                    file_hash_str = file_hash.hexdigest()

                    # 1. å…ˆæ·»åŠ æ–‡ä»¶è®°å½•åˆ°filesè¡¨
                    success = storage_context.docstore.add_file_record(
                        file_id=file_id,
                        file_name=file.filename,
                        file_path=file_path,
                        file_size=file_size,
                        file_type=file_type,
                        file_hash=file_hash_str
                    )

                    if not success:
                        upload_results.append({
                            "filename": file.filename,
                            "status": "error",
                            "message": "Failed to create file record"
                        })
                        os.remove(file_path)
                        continue

                    # 2. å¤„ç†æ–‡æ¡£å†…å®¹
                    reader = SimpleDirectoryReader(input_files=[file_path])
                    documents = reader.load_data()

                    if not documents:
                        upload_results.append({
                            "filename": file.filename,
                            "status": "error",
                            "message": "Failed to read document content"
                        })
                        # æ¸…ç†æ–‡ä»¶è®°å½•å’Œæ–‡ä»¶
                        storage_context.docstore.delete_file_and_chunks(file_id)
                        if os.path.exists(file_path):
                            os.remove(file_path)
                        continue

                    # 3. è§£æä¸ºèŠ‚ç‚¹
                    parser = SentenceSplitter()
                    nodes = parser.get_nodes_from_documents(documents)

                    # 4. ä¸ºæ¯ä¸ªèŠ‚ç‚¹æ·»åŠ æ–‡ä»¶å…³è”ä¿¡æ¯
                    for chunk_index, node in enumerate(nodes):
                        # æ·»åŠ æ–‡ä»¶å…ƒæ•°æ®
                        if hasattr(node, 'metadata'):
                            node.metadata.update({
                                'file_id': file_id,
                                'file_name': file.filename,
                                'file_size': file_size,
                                'file_type': file_type,
                                'file_path': file_path,
                                'chunk_index': chunk_index  # ğŸ”§ ä¿®å¤ï¼šå¯¹æ¯ä¸ªæ–‡ä»¶ä» 0 å¼€å§‹
                            })
                        else:
                            node.metadata = {
                                'file_id': file_id,
                                'file_name': file.filename,
                                'file_size': file_size,
                                'file_type': file_type,
                                'file_path': file_path,
                                'chunk_index': chunk_index  # ğŸ”§ ä¿®å¤ï¼šå¯¹æ¯ä¸ªæ–‡ä»¶ä» 0 å¼€å§‹
                            }

                    # 5. æ·»åŠ åˆ°æ–‡æ¡£å­˜å‚¨ï¼ˆè¿™ä¼šæ›´æ–°documentsè¡¨ï¼‰
                    # ğŸ”§ ä¿®å¤ï¼šä¸ä¼ é€’ file_metadataï¼Œé¿å…è¦†ç›– node çš„ metadata ä¸­çš„ chunk_index
                    storage_context.docstore.add_documents(nodes)

                    # 6. æ›´æ–°å‘é‡ç´¢å¼•
                    VectorStoreIndex(nodes, storage_context=storage_context)
                    storage_context.persist(STORAGE_DIR)

                    upload_results.append({
                        "filename": file.filename,
                        "status": "success",
                        "message": f"Successfully processed {len(nodes)} chunks",
                        "chunks": len(nodes),
                        "file_id": file_id
                    })

                except Exception as e:
                    logger.error(f"Error processing file {file.filename}: {e}")
                    upload_results.append({
                        "filename": file.filename,
                        "status": "error",
                        "message": str(e)
                    })

                    # æ¸…ç†å¤±è´¥çš„æ–‡ä»¶
                    if 'file_path' in locals() and os.path.exists(file_path):
                        os.remove(file_path)

            return JSONResponse(content={
                "results": upload_results,
                "total_files": len(files),
                "successful": len([r for r in upload_results if r["status"] == "success"])
            })

        except Exception as e:
            logger.error(f"Error uploading documents: {e}")
            return JSONResponse(
                status_code=500,
                content={"error": f"Failed to upload documents: {str(e)}"}
            )

    async def delete_document_endpoint(file_id: str):
        """åˆ é™¤æ–‡æ¡£çš„APIç«¯ç‚¹ - åˆ é™¤æ•´ä¸ªæ–‡ä»¶åŠå…¶æ‰€æœ‰æ–‡æœ¬å—"""
        try:
            from app.storage_config import load_storage_context
            from app.index import STORAGE_DIR

            storage_context = load_storage_context(STORAGE_DIR)
            if not storage_context:
                return JSONResponse(
                    status_code=404,
                    content={"error": "Storage context not found"}
                )

            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_info = storage_context.docstore.get_file_info(file_id)
            if not file_info:
                return JSONResponse(
                    status_code=404,
                    content={"error": "File not found"}
                )

            # åˆ é™¤æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ–‡ä»¶
            file_path = file_info['path']
            if os.path.exists(file_path):
                os.remove(file_path)
                logger.info(f"Deleted file: {file_path}")

            # ä»æ•°æ®åº“åˆ é™¤æ–‡ä»¶è®°å½•å’Œæ‰€æœ‰ç›¸å…³çš„æ–‡æ¡£å—
            success, doc_ids_to_delete = storage_context.docstore.delete_file_and_chunks(file_id)

            # ğŸ”§ ä¿®å¤ï¼šåˆ é™¤ ChromaDB ä¸­çš„å‘é‡æ•°æ®
            if doc_ids_to_delete:
                try:
                    chroma_collection = storage_context.vector_store._collection
                    chroma_collection.delete(ids=doc_ids_to_delete)
                    logger.info(f"Deleted {len(doc_ids_to_delete)} vectors from ChromaDB")
                except Exception as e:
                    logger.warning(f"Failed to delete vectors from ChromaDB: {e}")

            if success:
                # é‡æ–°æŒä¹…åŒ–å­˜å‚¨ä¸Šä¸‹æ–‡
                storage_context.persist(STORAGE_DIR)

                return JSONResponse(content={
                    "message": "File and all chunks deleted successfully",
                    "file_id": file_id,
                    "file_name": file_info['name']
                })
            else:
                return JSONResponse(
                    status_code=404,
                    content={"error": "File not found in database"}
                )

        except Exception as e:
            logger.error(f"Error deleting file {file_id}: {e}")
            return JSONResponse(
                status_code=500,
                content={"error": f"Failed to delete file: {str(e)}"}
            )

    # æ³¨å†Œæ–‡æ¡£ç®¡ç†APIç«¯ç‚¹
    app.add_api_route("/api/documents", get_documents_endpoint, methods=["GET"])
    app.add_api_route("/api/documents/upload", upload_documents_endpoint, methods=["POST"])
    app.add_api_route("/api/documents/{file_id}", delete_document_endpoint, methods=["DELETE"])
    # ==============================AI æ·»åŠ æ–‡æ¡£ç®¡ç†APIç«¯ç‚¹ end

    # æ£€æŸ¥æ˜¯å¦æœ‰Next.jsæ„å»ºçš„æ–‡ä»¶
    nextjs_build_dir = os.path.join(COMPONENT_DIR, "out")
    if os.path.exists(nextjs_build_dir):
        # ç”Ÿäº§æ¨¡å¼ï¼šæŒ‚è½½Next.jsæ„å»ºçš„é™æ€æ–‡ä»¶
        app.mount("/", StaticFiles(directory=nextjs_build_dir, html=True), name="nextjs-build")
        logger.info(f"Serving Next.js build from {nextjs_build_dir}")
    else:
        # å¼€å‘æ¨¡å¼ï¼šNext.jså¼€å‘æœåŠ¡å™¨è¿è¡Œåœ¨3000ç«¯å£
        # è¿™é‡Œåªæä¾›APIæœåŠ¡ï¼Œå‰ç«¯ç”±Next.js dev serveræä¾›
        logger.info("Development mode: Next.js should be running on port 3000")

    return app





load_dotenv()
init_settings()
app = create_app()
