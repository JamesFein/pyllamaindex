#!/usr/bin/env python3
"""
æ”¹è¿›çš„ç´¢å¼•ç”Ÿæˆè„šæœ¬ - è§£å†³ chunk_index å’Œé‡å¤æ•°æ®é—®é¢˜
"""
import logging
import os
import hashlib
from dotenv import load_dotenv

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger()

def generate_index_improved():
    """
    æ”¹è¿›çš„ç´¢å¼•ç”Ÿæˆå‡½æ•°ï¼Œè§£å†³ chunk_index å’Œé‡å¤æ•°æ®é—®é¢˜
    """
    from app.index import STORAGE_DIR
    from app.settings import init_settings
    from app.storage_config import get_storage_context
    from llama_index.core.indices import VectorStoreIndex
    from llama_index.core.readers import SimpleDirectoryReader
    from llama_index.core.node_parser import SentenceSplitter

    load_dotenv()
    init_settings()

    logger.info("ğŸ”„ å¼€å§‹æ”¹è¿›çš„ç´¢å¼•ç”Ÿæˆï¼ˆè§£å†³ chunk_index é—®é¢˜ï¼‰")

    # 1. æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®
    storage_context = get_storage_context(STORAGE_DIR)
    
    # æ£€æŸ¥ç°æœ‰æ–‡æ¡£æ•°é‡
    try:
        existing_docs = storage_context.docstore.get_all_documents()
        if existing_docs:
            logger.warning(f"âš ï¸  å‘ç° {len(existing_docs)} ä¸ªç°æœ‰æ–‡æ¡£")
            logger.warning("å»ºè®®å…ˆè¿è¡Œ reset_data_complete.py æ¸…ç†æ•°æ®")
            
            user_input = input("æ˜¯å¦ç»§ç»­ï¼Ÿè¿™å¯èƒ½å¯¼è‡´é‡å¤æ•°æ® (y/N): ")
            if user_input.lower() != 'y':
                logger.info("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
                return
    except:
        logger.info("âœ… æ²¡æœ‰ç°æœ‰æ•°æ®ï¼Œç»§ç»­ç”Ÿæˆ")

    # 2. è¯»å–æ–‡æ¡£
    data_dir = os.environ.get("DATA_DIR", "data")
    reader = SimpleDirectoryReader(data_dir, recursive=True)
    documents = reader.load_data()
    
    logger.info(f"ğŸ“„ è¯»å–åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
    
    if not documents:
        logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥ data ç›®å½•")
        return

    # 3. é…ç½®åˆ†å—å™¨ï¼ˆæ›´ç»†ç²’åº¦çš„åˆ†å—ï¼‰
    parser = SentenceSplitter(
        chunk_size=512,      # è¾ƒå°çš„å—å¤§å°
        chunk_overlap=50,    # å—ä¹‹é—´çš„é‡å 
        separator=" "        # æŒ‰ç©ºæ ¼åˆ†å‰²
    )
    
    # 4. å¤„ç†æ¯ä¸ªæ–‡æ¡£
    all_processed_nodes = []
    
    for document in documents:
        file_path = document.metadata.get('file_path', '')
        file_name = document.metadata.get('file_name', os.path.basename(file_path))
        
        logger.info(f"ğŸ“ å¤„ç†æ–‡æ¡£: {file_name}")
        
        # ç”Ÿæˆæ–‡ä»¶IDï¼ˆåŸºäºæ–‡ä»¶åçš„å“ˆå¸Œï¼‰
        file_id = f"file_{hashlib.md5(file_name.encode()).hexdigest()[:8]}"
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        try:
            file_stats = os.stat(file_path) if file_path and os.path.exists(file_path) else None
            file_size = file_stats.st_size if file_stats else len(document.text.encode('utf-8'))
        except:
            file_size = len(document.text.encode('utf-8'))
        
        file_type = "text/plain"
        
        # æ·»åŠ æ–‡ä»¶è®°å½•
        storage_context.docstore.add_file_record(
            file_id=file_id,
            file_name=file_name,
            file_path=file_path,
            file_size=file_size,
            file_type=file_type,
            file_hash=""
        )
        
        # å¯¹å•ä¸ªæ–‡æ¡£è¿›è¡Œåˆ†å—
        doc_nodes = parser.get_nodes_from_documents([document])
        
        logger.info(f"  ğŸ“Š æ–‡æ¡£ {file_name} åˆ†æˆ {len(doc_nodes)} ä¸ªå—")
        
        # ä¸ºæ¯ä¸ªå—æ·»åŠ æ­£ç¡®çš„å…ƒæ•°æ®å’Œ chunk_index
        for chunk_index, node in enumerate(doc_nodes):
            if not hasattr(node, 'metadata'):
                node.metadata = {}
            
            # æ·»åŠ å®Œæ•´çš„å…ƒæ•°æ®
            node.metadata.update({
                'file_id': file_id,
                'file_name': file_name,
                'file_size': file_size,
                'file_type': file_type,
                'chunk_index': chunk_index,  # æ­£ç¡®çš„å—ç´¢å¼•
                'total_chunks': len(doc_nodes),  # æ€»å—æ•°
                'chunk_size': len(node.text)     # å½“å‰å—å¤§å°
            })
            
            logger.debug(f"    å— {chunk_index}: {len(node.text)} å­—ç¬¦")
            all_processed_nodes.append(node)
    
    logger.info(f"ğŸ§© æ€»å…±å¤„ç†äº† {len(all_processed_nodes)} ä¸ªæ–‡æ¡£å—")
    
    # 5. éªŒè¯ chunk_index çš„æ­£ç¡®æ€§
    logger.info("ğŸ” éªŒè¯ chunk_index åˆ†é…...")
    file_chunks = {}
    for node in all_processed_nodes:
        file_name = node.metadata.get('file_name', 'Unknown')
        chunk_index = node.metadata.get('chunk_index', -1)
        
        if file_name not in file_chunks:
            file_chunks[file_name] = []
        file_chunks[file_name].append(chunk_index)
    
    for file_name, chunks in file_chunks.items():
        chunks.sort()
        expected = list(range(len(chunks)))
        if chunks == expected:
            logger.info(f"  âœ… {file_name}: chunk_index æ­£ç¡® {chunks}")
        else:
            logger.error(f"  âŒ {file_name}: chunk_index é”™è¯¯ {chunks}, æœŸæœ› {expected}")
    
    # 6. æ·»åŠ åˆ°æ–‡æ¡£å­˜å‚¨
    storage_context.docstore.add_documents(all_processed_nodes)
    
    # 7. åˆ›å»ºå‘é‡ç´¢å¼•
    logger.info("ğŸ§  åˆ›å»ºå‘é‡ç´¢å¼•...")
    index = VectorStoreIndex(
        all_processed_nodes,
        storage_context=storage_context,
        show_progress=True,
    )
    
    # 8. æŒä¹…åŒ–å­˜å‚¨
    storage_context.persist(STORAGE_DIR)
    
    logger.info("ğŸ‰ æ”¹è¿›çš„ç´¢å¼•ç”Ÿæˆå®Œæˆï¼")
    logger.info(f"ğŸ“Š ç»Ÿè®¡:")
    logger.info(f"  - å¤„ç†æ–‡æ¡£æ•°: {len(documents)}")
    logger.info(f"  - ç”Ÿæˆå—æ•°: {len(all_processed_nodes)}")
    logger.info(f"  - å­˜å‚¨ä½ç½®: {STORAGE_DIR}")
    
    # 9. æœ€ç»ˆéªŒè¯
    logger.info("ğŸ” æœ€ç»ˆéªŒè¯...")
    verify_chunk_index_in_db()

def verify_chunk_index_in_db():
    """éªŒè¯æ•°æ®åº“ä¸­çš„ chunk_index"""
    import sqlite3
    
    try:
        with sqlite3.connect('storage/docstore.db') as conn:
            cursor = conn.execute('''
                SELECT file_name, chunk_index, COUNT(*) as count
                FROM documents 
                GROUP BY file_name, chunk_index 
                ORDER BY file_name, chunk_index
            ''')
            
            logger.info("ğŸ“Š æ•°æ®åº“ä¸­çš„ chunk_index åˆ†å¸ƒ:")
            current_file = None
            for row in cursor.fetchall():
                file_name, chunk_index, count = row
                if file_name != current_file:
                    logger.info(f"  ğŸ“„ {file_name}:")
                    current_file = file_name
                logger.info(f"    å— {chunk_index}: {count} ä¸ªè®°å½•")
                
                if count > 1:
                    logger.warning(f"    âš ï¸  å— {chunk_index} æœ‰é‡å¤è®°å½•ï¼")
    except Exception as e:
        logger.error(f"âŒ éªŒè¯å¤±è´¥: {e}")

if __name__ == "__main__":
    generate_index_improved()
