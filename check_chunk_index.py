#!/usr/bin/env python3
"""
æ£€æŸ¥ chunk_index çš„é—®é¢˜
"""
import sqlite3
import json

def check_chunk_index_issue():
    """æ£€æŸ¥ chunk_index çš„åˆ†å¸ƒå’Œé—®é¢˜"""
    print("ğŸ” æ£€æŸ¥ Documents è¡¨ä¸­çš„ chunk_index é—®é¢˜")
    print("=" * 60)
    
    with sqlite3.connect('storage/docstore.db') as conn:
        # 1. æ£€æŸ¥ chunk_index åˆ†å¸ƒ
        cursor = conn.execute('''
            SELECT file_name, chunk_index, COUNT(*) as count
            FROM documents 
            GROUP BY file_name, chunk_index 
            ORDER BY file_name, chunk_index
        ''')
        
        print("ğŸ“Š chunk_index åˆ†å¸ƒ:")
        print("æ–‡ä»¶å                | å—ç´¢å¼• | æ•°é‡")
        print("-" * 50)
        
        chunk_data = {}
        for row in cursor.fetchall():
            file_name, chunk_index, count = row
            print(f"{file_name:<20} | {chunk_index:<6} | {count}")
            
            if file_name not in chunk_data:
                chunk_data[file_name] = []
            chunk_data[file_name].append((chunk_index, count))
        
        # 2. åˆ†æé—®é¢˜
        print(f"\nğŸ” é—®é¢˜åˆ†æ:")
        
        total_docs = 0
        files_with_issue = 0
        
        for file_name, chunks in chunk_data.items():
            total_chunks = sum(count for _, count in chunks)
            total_docs += total_chunks
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å—çš„ç´¢å¼•éƒ½æ˜¯0
            all_zero = all(chunk_index == 0 for chunk_index, _ in chunks)
            
            if all_zero and total_chunks > 1:
                files_with_issue += 1
                print(f"âš ï¸  {file_name}: {total_chunks} ä¸ªå—ï¼Œä½† chunk_index éƒ½æ˜¯ 0")
            elif not all_zero:
                print(f"âœ… {file_name}: chunk_index æ­£å¸¸åˆ†å¸ƒ")
            else:
                print(f"â„¹ï¸  {file_name}: åªæœ‰ 1 ä¸ªå—ï¼Œchunk_index=0 æ­£å¸¸")
        
        print(f"\nğŸ“ˆ ç»Ÿè®¡:")
        print(f"æ€»æ–‡æ¡£å—æ•°: {total_docs}")
        print(f"æœ‰é—®é¢˜çš„æ–‡ä»¶æ•°: {files_with_issue}")
        
        # 3. æ£€æŸ¥å…·ä½“å†…å®¹
        print(f"\nğŸ“„ æ£€æŸ¥æ–‡æ¡£å†…å®¹é•¿åº¦:")
        cursor = conn.execute('''
            SELECT file_name, chunk_index, LENGTH(data) as data_length,
                   doc_id
            FROM documents 
            ORDER BY file_name, chunk_index
        ''')
        
        for row in cursor.fetchall():
            file_name, chunk_index, data_length, doc_id = row
            print(f"{file_name:<20} | å—{chunk_index} | {data_length:>6} å­—ç¬¦ | {doc_id[:20]}...")

def analyze_chunking_logic():
    """åˆ†æåˆ†å—é€»è¾‘çš„é—®é¢˜"""
    print(f"\nğŸ” åˆ†æåˆ†å—é€»è¾‘é—®é¢˜")
    print("=" * 60)
    
    print("ğŸ“‹ å¯èƒ½çš„åŸå› :")
    print("1. generate.py ä¸­çš„ chunk_index è®¡ç®—é€»è¾‘æœ‰è¯¯")
    print("2. æ¯æ¬¡è¿è¡Œ generate éƒ½é‡æ–°ç”Ÿæˆ node_idï¼Œå¯¼è‡´é‡å¤")
    print("3. æ–‡æ¡£åˆ†å—å™¨å¯èƒ½æ²¡æœ‰æ­£ç¡®åˆ†å—")
    print("4. chunk_index èµ‹å€¼é€»è¾‘é”™è¯¯")
    
    print(f"\nğŸ¯ æ­£ç¡®çš„ chunk_index åº”è¯¥æ˜¯:")
    print("- åŒä¸€ä¸ªæ–‡ä»¶çš„ä¸åŒå—åº”è¯¥æœ‰ä¸åŒçš„ chunk_index")
    print("- ç¬¬ä¸€ä¸ªå—: chunk_index = 0")
    print("- ç¬¬äºŒä¸ªå—: chunk_index = 1")
    print("- ç¬¬ä¸‰ä¸ªå—: chunk_index = 2")
    print("- ...")
    
    print(f"\nâš ï¸  å½“å‰é—®é¢˜çš„å½±å“:")
    print("1. æ— æ³•æ­£ç¡®è¯†åˆ«æ–‡æ¡£å—çš„é¡ºåº")
    print("2. å¯èƒ½å½±å“æ–‡æ¡£é‡å»ºå’Œæ˜¾ç¤º")
    print("3. å¼•ç”¨æ—¶æ— æ³•å‡†ç¡®å®šä½åˆ°å…·ä½“æ®µè½")
    print("4. å¯èƒ½å¯¼è‡´æœç´¢ç»“æœæ··ä¹±")

def check_generate_logic():
    """æ£€æŸ¥ generate.py ä¸­çš„é€»è¾‘"""
    print(f"\nğŸ” æ£€æŸ¥ generate.py ä¸­çš„åˆ†å—é€»è¾‘")
    print("=" * 60)
    
    try:
        with open('generate.py', 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æŸ¥æ‰¾ chunk_index ç›¸å…³çš„ä»£ç 
        lines = content.split('\n')
        chunk_index_lines = []
        
        for i, line in enumerate(lines):
            if 'chunk_index' in line.lower():
                chunk_index_lines.append((i+1, line.strip()))
        
        if chunk_index_lines:
            print("ğŸ“„ generate.py ä¸­çš„ chunk_index ç›¸å…³ä»£ç :")
            for line_num, line in chunk_index_lines:
                print(f"ç¬¬{line_num}è¡Œ: {line}")
        else:
            print("âŒ åœ¨ generate.py ä¸­æ²¡æœ‰æ‰¾åˆ° chunk_index ç›¸å…³ä»£ç ")
            
    except Exception as e:
        print(f"âŒ è¯»å– generate.py å¤±è´¥: {e}")

def suggest_fixes():
    """å»ºè®®ä¿®å¤æ–¹æ¡ˆ"""
    print(f"\nğŸ’¡ ä¿®å¤å»ºè®®")
    print("=" * 60)
    
    print("ğŸ”§ æ–¹æ¡ˆ1: ä¿®å¤ generate.py ä¸­çš„ chunk_index é€»è¾‘")
    print("- ç¡®ä¿ä¸ºåŒä¸€æ–‡ä»¶çš„ä¸åŒå—åˆ†é…é€’å¢çš„ chunk_index")
    print("- ä¿®æ”¹å¾ªç¯é€»è¾‘ï¼Œæ­£ç¡®è®¡ç®—å—ç´¢å¼•")
    
    print(f"\nğŸ”§ æ–¹æ¡ˆ2: é‡æ–°ç”Ÿæˆç´¢å¼•")
    print("- å…ˆé‡ç½®æ•°æ®åº“")
    print("- ä¿®å¤ä»£ç åé‡æ–°è¿è¡Œ generate")
    
    print(f"\nğŸ”§ æ–¹æ¡ˆ3: æ•°æ®åº“ä¿®å¤è„šæœ¬")
    print("- åˆ›å»ºè„šæœ¬é‡æ–°è®¡ç®— chunk_index")
    print("- åŸºäºæ–‡æ¡£å†…å®¹å’Œé¡ºåºé‡æ–°åˆ†é…ç´¢å¼•")

def main():
    """ä¸»æ£€æŸ¥æµç¨‹"""
    check_chunk_index_issue()
    analyze_chunking_logic()
    check_generate_logic()
    suggest_fixes()

if __name__ == "__main__":
    main()
