[
    {
        "label": "runpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "runpy",
        "description": "runpy",
        "detail": "runpy",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "site",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "site",
        "description": "site",
        "detail": "site",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices",
        "description": "llama_index.core.indices",
        "isExtraImport": true,
        "detail": "llama_index.core.indices",
        "documentation": {}
    },
    {
        "label": "ChatRequest",
        "importPath": "llama_index.server.api.models",
        "description": "llama_index.server.api.models",
        "isExtraImport": true,
        "detail": "llama_index.server.api.models",
        "documentation": {}
    },
    {
        "label": "ChatRequest",
        "importPath": "llama_index.server.api.models",
        "description": "llama_index.server.api.models",
        "isExtraImport": true,
        "detail": "llama_index.server.api.models",
        "documentation": {}
    },
    {
        "label": "load_storage_context",
        "importPath": "app.storage_config",
        "description": "app.storage_config",
        "isExtraImport": true,
        "detail": "app.storage_config",
        "documentation": {}
    },
    {
        "label": "get_storage_context",
        "importPath": "app.storage_config",
        "description": "app.storage_config",
        "isExtraImport": true,
        "detail": "app.storage_config",
        "documentation": {}
    },
    {
        "label": "get_storage_context",
        "importPath": "app.storage_config",
        "description": "app.storage_config",
        "isExtraImport": true,
        "detail": "app.storage_config",
        "documentation": {}
    },
    {
        "label": "get_storage_context",
        "importPath": "app.storage_config",
        "description": "app.storage_config",
        "isExtraImport": true,
        "detail": "app.storage_config",
        "documentation": {}
    },
    {
        "label": "get_storage_context",
        "importPath": "app.storage_config",
        "description": "app.storage_config",
        "isExtraImport": true,
        "detail": "app.storage_config",
        "documentation": {}
    },
    {
        "label": "load_storage_context",
        "importPath": "app.storage_config",
        "description": "app.storage_config",
        "isExtraImport": true,
        "detail": "app.storage_config",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbedding",
        "importPath": "llama_index.embeddings.openai",
        "description": "llama_index.embeddings.openai",
        "isExtraImport": true,
        "detail": "llama_index.embeddings.openai",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "llama_index.llms.openai",
        "description": "llama_index.llms.openai",
        "isExtraImport": true,
        "detail": "llama_index.llms.openai",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "llama_index.llms.openai",
        "description": "llama_index.llms.openai",
        "isExtraImport": true,
        "detail": "llama_index.llms.openai",
        "documentation": {}
    },
    {
        "label": "sqlite3",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sqlite3",
        "description": "sqlite3",
        "detail": "sqlite3",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "BaseDocumentStore",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "BaseIndexStore",
        "importPath": "llama_index.core.storage.index_store.types",
        "description": "llama_index.core.storage.index_store.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.index_store.types",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexStruct",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "ChromaVectorStore",
        "importPath": "llama_index.vector_stores.chroma",
        "description": "llama_index.vector_stores.chroma",
        "isExtraImport": true,
        "detail": "llama_index.vector_stores.chroma",
        "documentation": {}
    },
    {
        "label": "chromadb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "chromadb",
        "description": "chromadb",
        "detail": "chromadb",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "chromadb.config",
        "description": "chromadb.config",
        "isExtraImport": true,
        "detail": "chromadb.config",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "chromadb.config",
        "description": "chromadb.config",
        "isExtraImport": true,
        "detail": "chromadb.config",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "chromadb.config",
        "description": "chromadb.config",
        "isExtraImport": true,
        "detail": "chromadb.config",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "chromadb.config",
        "description": "chromadb.config",
        "isExtraImport": true,
        "detail": "chromadb.config",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "chromadb.config",
        "description": "chromadb.config",
        "isExtraImport": true,
        "detail": "chromadb.config",
        "documentation": {}
    },
    {
        "label": "SQLiteDocumentStore",
        "importPath": "app.sqlite_stores",
        "description": "app.sqlite_stores",
        "isExtraImport": true,
        "detail": "app.sqlite_stores",
        "documentation": {}
    },
    {
        "label": "SQLiteIndexStore",
        "importPath": "app.sqlite_stores",
        "description": "app.sqlite_stores",
        "isExtraImport": true,
        "detail": "app.sqlite_stores",
        "documentation": {}
    },
    {
        "label": "get_index",
        "importPath": "app.index",
        "description": "app.index",
        "isExtraImport": true,
        "detail": "app.index",
        "documentation": {}
    },
    {
        "label": "STORAGE_DIR",
        "importPath": "app.index",
        "description": "app.index",
        "isExtraImport": true,
        "detail": "app.index",
        "documentation": {}
    },
    {
        "label": "get_index",
        "importPath": "app.index",
        "description": "app.index",
        "isExtraImport": true,
        "detail": "app.index",
        "documentation": {}
    },
    {
        "label": "STORAGE_DIR",
        "importPath": "app.index",
        "description": "app.index",
        "isExtraImport": true,
        "detail": "app.index",
        "documentation": {}
    },
    {
        "label": "AgentWorkflow",
        "importPath": "llama_index.core.agent.workflow",
        "description": "llama_index.core.agent.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "get_query_engine_tool",
        "importPath": "llama_index.server.tools.index",
        "description": "llama_index.server.tools.index",
        "isExtraImport": true,
        "detail": "llama_index.server.tools.index",
        "documentation": {}
    },
    {
        "label": "CITATION_SYSTEM_PROMPT",
        "importPath": "llama_index.server.tools.index.citation",
        "description": "llama_index.server.tools.index.citation",
        "isExtraImport": true,
        "detail": "llama_index.server.tools.index.citation",
        "documentation": {}
    },
    {
        "label": "enable_citation",
        "importPath": "llama_index.server.tools.index.citation",
        "description": "llama_index.server.tools.index.citation",
        "isExtraImport": true,
        "detail": "llama_index.server.tools.index.citation",
        "documentation": {}
    },
    {
        "label": "TTFont",
        "importPath": "fontTools.ttLib",
        "description": "fontTools.ttLib",
        "isExtraImport": true,
        "detail": "fontTools.ttLib",
        "documentation": {}
    },
    {
        "label": "sfnt",
        "importPath": "fontTools.ttLib",
        "description": "fontTools.ttLib",
        "isExtraImport": true,
        "detail": "fontTools.ttLib",
        "documentation": {}
    },
    {
        "label": "TTFont",
        "importPath": "fontTools.ttLib",
        "description": "fontTools.ttLib",
        "isExtraImport": true,
        "detail": "fontTools.ttLib",
        "documentation": {}
    },
    {
        "label": "timestampNow",
        "importPath": "fontTools.misc.timeTools",
        "description": "fontTools.misc.timeTools",
        "isExtraImport": true,
        "detail": "fontTools.misc.timeTools",
        "documentation": {}
    },
    {
        "label": "collections",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections",
        "description": "collections",
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "parse_tfm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "parse_tfm",
        "description": "parse_tfm",
        "detail": "parse_tfm",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "SimpleDirectoryReader",
        "importPath": "llama_index.core.readers",
        "description": "llama_index.core.readers",
        "isExtraImport": true,
        "detail": "llama_index.core.readers",
        "documentation": {}
    },
    {
        "label": "SimpleDirectoryReader",
        "importPath": "llama_index.core.readers",
        "description": "llama_index.core.readers",
        "isExtraImport": true,
        "detail": "llama_index.core.readers",
        "documentation": {}
    },
    {
        "label": "SimpleDirectoryReader",
        "importPath": "llama_index.core.readers",
        "description": "llama_index.core.readers",
        "isExtraImport": true,
        "detail": "llama_index.core.readers",
        "documentation": {}
    },
    {
        "label": "SentenceSplitter",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "SentenceSplitter",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "SentenceSplitter",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "init_settings",
        "importPath": "app.settings",
        "description": "app.settings",
        "isExtraImport": true,
        "detail": "app.settings",
        "documentation": {}
    },
    {
        "label": "create_workflow",
        "importPath": "app.workflow",
        "description": "app.workflow",
        "isExtraImport": true,
        "detail": "app.workflow",
        "documentation": {}
    },
    {
        "label": "LlamaIndexServer",
        "importPath": "llama_index.server",
        "description": "llama_index.server",
        "isExtraImport": true,
        "detail": "llama_index.server",
        "documentation": {}
    },
    {
        "label": "UIConfig",
        "importPath": "llama_index.server",
        "description": "llama_index.server",
        "isExtraImport": true,
        "detail": "llama_index.server",
        "documentation": {}
    },
    {
        "label": "JSONResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "StaticFiles",
        "importPath": "fastapi.staticfiles",
        "description": "fastapi.staticfiles",
        "isExtraImport": true,
        "detail": "fastapi.staticfiles",
        "documentation": {}
    },
    {
        "label": "CORSMiddleware",
        "importPath": "fastapi.middleware.cors",
        "description": "fastapi.middleware.cors",
        "isExtraImport": true,
        "detail": "fastapi.middleware.cors",
        "documentation": {}
    },
    {
        "label": "UploadFile",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "File",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "traceback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "traceback",
        "description": "traceback",
        "detail": "traceback",
        "documentation": {}
    },
    {
        "label": "bin_dir",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "bin_dir = os.path.dirname(abs_file)\nbase = bin_dir[: -len(\"Scripts\") - 1]  # strip away the bin part from the __file__, plus the path separator\n# prepend bin to PATH (this file is inside the bin directory)\nos.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"app\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "base",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "base = bin_dir[: -len(\"Scripts\") - 1]  # strip away the bin part from the __file__, plus the path separator\n# prepend bin to PATH (this file is inside the bin directory)\nos.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"app\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PATH\"]",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"app\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"VIRTUAL_ENV\"]",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"app\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"VIRTUAL_ENV_PROMPT\"]",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"VIRTUAL_ENV_PROMPT\"] = \"app\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "prev_length",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "prev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.path[:]",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "sys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.real_prefix",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "sys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.prefix",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "sys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "get_index",
        "kind": 2,
        "importPath": "app.index",
        "description": "app.index",
        "peekOfCode": "def get_index(chat_request: Optional[ChatRequest] = None):\n    # ðŸ”§ æ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯\n    current_dir = os.getcwd()\n    logger.info(f\"ðŸ” get_index called from directory: {current_dir}\")\n    logger.info(f\"ðŸ” STORAGE_DIR: {STORAGE_DIR}\")\n    logger.info(f\"ðŸ” Full storage path: {os.path.abspath(STORAGE_DIR)}\")\n    # check if storage already exists\n    if not os.path.exists(STORAGE_DIR):\n        logger.error(f\"âŒ Storage directory does not exist: {os.path.abspath(STORAGE_DIR)}\")\n        return None",
        "detail": "app.index",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.index",
        "description": "app.index",
        "peekOfCode": "logger = logging.getLogger(\"uvicorn\")\nSTORAGE_DIR = \"storage\"\ndef get_index(chat_request: Optional[ChatRequest] = None):\n    # ðŸ”§ æ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯\n    current_dir = os.getcwd()\n    logger.info(f\"ðŸ” get_index called from directory: {current_dir}\")\n    logger.info(f\"ðŸ” STORAGE_DIR: {STORAGE_DIR}\")\n    logger.info(f\"ðŸ” Full storage path: {os.path.abspath(STORAGE_DIR)}\")\n    # check if storage already exists\n    if not os.path.exists(STORAGE_DIR):",
        "detail": "app.index",
        "documentation": {}
    },
    {
        "label": "STORAGE_DIR",
        "kind": 5,
        "importPath": "app.index",
        "description": "app.index",
        "peekOfCode": "STORAGE_DIR = \"storage\"\ndef get_index(chat_request: Optional[ChatRequest] = None):\n    # ðŸ”§ æ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯\n    current_dir = os.getcwd()\n    logger.info(f\"ðŸ” get_index called from directory: {current_dir}\")\n    logger.info(f\"ðŸ” STORAGE_DIR: {STORAGE_DIR}\")\n    logger.info(f\"ðŸ” Full storage path: {os.path.abspath(STORAGE_DIR)}\")\n    # check if storage already exists\n    if not os.path.exists(STORAGE_DIR):\n        logger.error(f\"âŒ Storage directory does not exist: {os.path.abspath(STORAGE_DIR)}\")",
        "detail": "app.index",
        "documentation": {}
    },
    {
        "label": "init_settings",
        "kind": 2,
        "importPath": "app.settings",
        "description": "app.settings",
        "peekOfCode": "def init_settings():\n    if os.getenv(\"OPENAI_API_KEY\") is None:\n        raise RuntimeError(\"OPENAI_API_KEY is missing in environment variables\")\n    Settings.llm = OpenAI(model=\"gpt-4o-mini\")\n    Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")",
        "detail": "app.settings",
        "documentation": {}
    },
    {
        "label": "SQLiteDocumentStore",
        "kind": 6,
        "importPath": "app.sqlite_stores",
        "description": "app.sqlite_stores",
        "peekOfCode": "class SQLiteDocumentStore(BaseDocumentStore):\n    \"\"\"SQLite-based document store for better performance and concurrency.\"\"\"\n    def __init__(self, db_path: str):\n        \"\"\"Initialize SQLite document store.\n        Args:\n            db_path: Path to SQLite database file\n        \"\"\"\n        self.db_path = db_path\n        logger.info(f\"ðŸ”¥ Initializing SQLiteDocumentStore at {db_path}\")\n        self._init_db()",
        "detail": "app.sqlite_stores",
        "documentation": {}
    },
    {
        "label": "SQLiteIndexStore",
        "kind": 6,
        "importPath": "app.sqlite_stores",
        "description": "app.sqlite_stores",
        "peekOfCode": "class SQLiteIndexStore(BaseIndexStore):\n    \"\"\"SQLite-based index store for better performance and concurrency.\"\"\"\n    def __init__(self, db_path: str):\n        \"\"\"Initialize SQLite index store.\n        Args:\n            db_path: Path to SQLite database file\n        \"\"\"\n        self.db_path = db_path\n        self._init_db()\n    def _init_db(self):",
        "detail": "app.sqlite_stores",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.sqlite_stores",
        "description": "app.sqlite_stores",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass SQLiteDocumentStore(BaseDocumentStore):\n    \"\"\"SQLite-based document store for better performance and concurrency.\"\"\"\n    def __init__(self, db_path: str):\n        \"\"\"Initialize SQLite document store.\n        Args:\n            db_path: Path to SQLite database file\n        \"\"\"\n        self.db_path = db_path\n        logger.info(f\"ðŸ”¥ Initializing SQLiteDocumentStore at {db_path}\")",
        "detail": "app.sqlite_stores",
        "documentation": {}
    },
    {
        "label": "get_storage_context",
        "kind": 2,
        "importPath": "app.storage_config",
        "description": "app.storage_config",
        "peekOfCode": "def get_storage_context(storage_dir: str = \"storage\") -> StorageContext:\n    \"\"\"\n    Create a storage context using SQLite for docstore/index store and ChromaDB for vector store.\n    Optimized for text-only processing - no graph store or image vector store.\n    Args:\n        storage_dir: Directory to store the databases\n    Returns:\n        StorageContext configured with SQLite and ChromaDB backends\n    \"\"\"\n    # Ensure storage directory exists",
        "detail": "app.storage_config",
        "documentation": {}
    },
    {
        "label": "load_storage_context",
        "kind": 2,
        "importPath": "app.storage_config",
        "description": "app.storage_config",
        "peekOfCode": "def load_storage_context(storage_dir: str = \"storage\") -> Optional[StorageContext]:\n    \"\"\"\n    Load existing storage context from ChromaDB and SQLite stores.\n    Args:\n        storage_dir: Directory containing the databases\n    Returns:\n        StorageContext if databases exist, None otherwise\n    \"\"\"\n    # Check if storage directory exists\n    if not os.path.exists(storage_dir):",
        "detail": "app.storage_config",
        "documentation": {}
    },
    {
        "label": "migrate_json_to_sqlite",
        "kind": 2,
        "importPath": "app.storage_config",
        "description": "app.storage_config",
        "peekOfCode": "def migrate_json_to_sqlite(storage_dir: str = \"storage\") -> bool:\n    \"\"\"\n    Migrate existing JSON storage to SQLite.\n    Args:\n        storage_dir: Directory containing the storage files\n    Returns:\n        True if migration was successful, False otherwise\n    \"\"\"\n    import json\n    import sqlite3",
        "detail": "app.storage_config",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.storage_config",
        "description": "app.storage_config",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef get_storage_context(storage_dir: str = \"storage\") -> StorageContext:\n    \"\"\"\n    Create a storage context using SQLite for docstore/index store and ChromaDB for vector store.\n    Optimized for text-only processing - no graph store or image vector store.\n    Args:\n        storage_dir: Directory to store the databases\n    Returns:\n        StorageContext configured with SQLite and ChromaDB backends\n    \"\"\"",
        "detail": "app.storage_config",
        "documentation": {}
    },
    {
        "label": "create_workflow",
        "kind": 2,
        "importPath": "app.workflow",
        "description": "app.workflow",
        "peekOfCode": "def create_workflow(chat_request: Optional[ChatRequest] = None) -> AgentWorkflow:\n    index = get_index(chat_request=chat_request)\n    if index is None:\n        raise RuntimeError(\n            \"Index not found! Please run `uv run generate` to index the data first.\"\n        )\n    # Create a query tool with citations enabled\n    query_tool = enable_citation(get_query_engine_tool(index=index))\n    # Define the system prompt for the agent\n    # Append the citation system prompt to the system prompt",
        "detail": "app.workflow",
        "documentation": {}
    },
    {
        "label": "_Known",
        "kind": 6,
        "importPath": "components.node_modules.flatted.python.flatted",
        "description": "components.node_modules.flatted.python.flatted",
        "peekOfCode": "class _Known:\n    def __init__(self):\n        self.key = []\n        self.value = []\nclass _String:\n    def __init__(self, value):\n        self.value = value\ndef _array_keys(value):\n    keys = []\n    i = 0",
        "detail": "components.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "_String",
        "kind": 6,
        "importPath": "components.node_modules.flatted.python.flatted",
        "description": "components.node_modules.flatted.python.flatted",
        "peekOfCode": "class _String:\n    def __init__(self, value):\n        self.value = value\ndef _array_keys(value):\n    keys = []\n    i = 0\n    for _ in value:\n        keys.append(i)\n        i += 1\n    return keys",
        "detail": "components.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "parse",
        "kind": 2,
        "importPath": "components.node_modules.flatted.python.flatted",
        "description": "components.node_modules.flatted.python.flatted",
        "peekOfCode": "def parse(value, *args, **kwargs):\n    json = _json.loads(value, *args, **kwargs)\n    wrapped = []\n    for value in json:\n        wrapped.append(_wrap(value))\n    input = []\n    for value in wrapped:\n        if isinstance(value, _String):\n            input.append(value.value)\n        else:",
        "detail": "components.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "stringify",
        "kind": 2,
        "importPath": "components.node_modules.flatted.python.flatted",
        "description": "components.node_modules.flatted.python.flatted",
        "peekOfCode": "def stringify(value, *args, **kwargs):\n    known = _Known()\n    input = []\n    output = []\n    i = int(_index(known, input, value))\n    while i < len(input):\n        output.append(_transform(known, input, input[i]))\n        i += 1\n    return _json.dumps(output, *args, **kwargs)",
        "detail": "components.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "sfnt.USE_ZOPFLI",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "sfnt.USE_ZOPFLI = True\nif len(sys.argv) < 2:\n    print(\"Usage: %s <font file>\" % sys.argv[0])\n    sys.exit(1)\nfont_file = sys.argv[1]\nfont_name = os.path.splitext(os.path.basename(font_file))[0]\nfont = TTFont(font_file, recalcBBoxes=False, recalcTimestamp=False)\n# fix timestamp to the epoch\nfont['head'].created = 0\nfont['head'].modified = 0",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font_file",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font_file = sys.argv[1]\nfont_name = os.path.splitext(os.path.basename(font_file))[0]\nfont = TTFont(font_file, recalcBBoxes=False, recalcTimestamp=False)\n# fix timestamp to the epoch\nfont['head'].created = 0\nfont['head'].modified = 0\n# remove fontforge timestamps\nif 'FFTM' in font:\n    del font['FFTM']\n# remove redundant GDEF table",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font_name",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font_name = os.path.splitext(os.path.basename(font_file))[0]\nfont = TTFont(font_file, recalcBBoxes=False, recalcTimestamp=False)\n# fix timestamp to the epoch\nfont['head'].created = 0\nfont['head'].modified = 0\n# remove fontforge timestamps\nif 'FFTM' in font:\n    del font['FFTM']\n# remove redundant GDEF table\nif 'GDEF' in font:",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font = TTFont(font_file, recalcBBoxes=False, recalcTimestamp=False)\n# fix timestamp to the epoch\nfont['head'].created = 0\nfont['head'].modified = 0\n# remove fontforge timestamps\nif 'FFTM' in font:\n    del font['FFTM']\n# remove redundant GDEF table\nif 'GDEF' in font:\n    del font['GDEF']",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font['head'].created",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font['head'].created = 0\nfont['head'].modified = 0\n# remove fontforge timestamps\nif 'FFTM' in font:\n    del font['FFTM']\n# remove redundant GDEF table\nif 'GDEF' in font:\n    del font['GDEF']\n# remove Macintosh table\n# https://developer.apple.com/fonts/TrueType-Reference-Manual/RM06/Chap6cmap.html",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font['head'].modified",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font['head'].modified = 0\n# remove fontforge timestamps\nif 'FFTM' in font:\n    del font['FFTM']\n# remove redundant GDEF table\nif 'GDEF' in font:\n    del font['GDEF']\n# remove Macintosh table\n# https://developer.apple.com/fonts/TrueType-Reference-Manual/RM06/Chap6cmap.html\nfont['name'].names = [record for record in font['name'].names if record.platformID != 1]",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font['name'].names",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font['name'].names = [record for record in font['name'].names if record.platformID != 1]\nfont['cmap'].tables = [table for table in font['cmap'].tables if table.platformID != 1]\n# fix OS/2 and hhea metrics\nglyf = font['glyf']\nascent = int(max(glyf[c].yMax for c in font.getGlyphOrder() if hasattr(glyf[c], \"yMax\")))\ndescent = -int(min(glyf[c].yMin for c in font.getGlyphOrder() if hasattr(glyf[c], \"yMin\")))\nfont['OS/2'].usWinAscent = ascent\nfont['OS/2'].usWinDescent = descent\nfont['hhea'].ascent = ascent\nfont['hhea'].descent = -descent",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font['cmap'].tables",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font['cmap'].tables = [table for table in font['cmap'].tables if table.platformID != 1]\n# fix OS/2 and hhea metrics\nglyf = font['glyf']\nascent = int(max(glyf[c].yMax for c in font.getGlyphOrder() if hasattr(glyf[c], \"yMax\")))\ndescent = -int(min(glyf[c].yMin for c in font.getGlyphOrder() if hasattr(glyf[c], \"yMin\")))\nfont['OS/2'].usWinAscent = ascent\nfont['OS/2'].usWinDescent = descent\nfont['hhea'].ascent = ascent\nfont['hhea'].descent = -descent\n# save TTF",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "glyf",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "glyf = font['glyf']\nascent = int(max(glyf[c].yMax for c in font.getGlyphOrder() if hasattr(glyf[c], \"yMax\")))\ndescent = -int(min(glyf[c].yMin for c in font.getGlyphOrder() if hasattr(glyf[c], \"yMin\")))\nfont['OS/2'].usWinAscent = ascent\nfont['OS/2'].usWinDescent = descent\nfont['hhea'].ascent = ascent\nfont['hhea'].descent = -descent\n# save TTF\nfont.save(font_file, reorderTables=None)\n# save WOFF",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "ascent",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "ascent = int(max(glyf[c].yMax for c in font.getGlyphOrder() if hasattr(glyf[c], \"yMax\")))\ndescent = -int(min(glyf[c].yMin for c in font.getGlyphOrder() if hasattr(glyf[c], \"yMin\")))\nfont['OS/2'].usWinAscent = ascent\nfont['OS/2'].usWinDescent = descent\nfont['hhea'].ascent = ascent\nfont['hhea'].descent = -descent\n# save TTF\nfont.save(font_file, reorderTables=None)\n# save WOFF\nfont.flavor = 'woff'",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "descent",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "descent = -int(min(glyf[c].yMin for c in font.getGlyphOrder() if hasattr(glyf[c], \"yMin\")))\nfont['OS/2'].usWinAscent = ascent\nfont['OS/2'].usWinDescent = descent\nfont['hhea'].ascent = ascent\nfont['hhea'].descent = -descent\n# save TTF\nfont.save(font_file, reorderTables=None)\n# save WOFF\nfont.flavor = 'woff'\nfont.save(os.path.join('woff', font_name + '.woff'), reorderTables=None)",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font['OS/2'].usWinAscent",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font['OS/2'].usWinAscent = ascent\nfont['OS/2'].usWinDescent = descent\nfont['hhea'].ascent = ascent\nfont['hhea'].descent = -descent\n# save TTF\nfont.save(font_file, reorderTables=None)\n# save WOFF\nfont.flavor = 'woff'\nfont.save(os.path.join('woff', font_name + '.woff'), reorderTables=None)\n# save WOFF2",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font['OS/2'].usWinDescent",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font['OS/2'].usWinDescent = descent\nfont['hhea'].ascent = ascent\nfont['hhea'].descent = -descent\n# save TTF\nfont.save(font_file, reorderTables=None)\n# save WOFF\nfont.flavor = 'woff'\nfont.save(os.path.join('woff', font_name + '.woff'), reorderTables=None)\n# save WOFF2\nfont.flavor = 'woff2'",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font['hhea'].ascent",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font['hhea'].ascent = ascent\nfont['hhea'].descent = -descent\n# save TTF\nfont.save(font_file, reorderTables=None)\n# save WOFF\nfont.flavor = 'woff'\nfont.save(os.path.join('woff', font_name + '.woff'), reorderTables=None)\n# save WOFF2\nfont.flavor = 'woff2'\nfont.save(os.path.join('woff2', font_name + '.woff2'), reorderTables=None)",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font['hhea'].descent",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font['hhea'].descent = -descent\n# save TTF\nfont.save(font_file, reorderTables=None)\n# save WOFF\nfont.flavor = 'woff'\nfont.save(os.path.join('woff', font_name + '.woff'), reorderTables=None)\n# save WOFF2\nfont.flavor = 'woff2'\nfont.save(os.path.join('woff2', font_name + '.woff2'), reorderTables=None)",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font.flavor",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font.flavor = 'woff'\nfont.save(os.path.join('woff', font_name + '.woff'), reorderTables=None)\n# save WOFF2\nfont.flavor = 'woff2'\nfont.save(os.path.join('woff2', font_name + '.woff2'), reorderTables=None)",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font.flavor",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font.flavor = 'woff2'\nfont.save(os.path.join('woff2', font_name + '.woff2'), reorderTables=None)",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "find_font_path",
        "kind": 2,
        "importPath": "components.node_modules.katex.src.metrics.extract_tfms",
        "description": "components.node_modules.katex.src.metrics.extract_tfms",
        "peekOfCode": "def find_font_path(font_name):\n    try:\n        font_path = subprocess.check_output(['kpsewhich', font_name])\n    except OSError:\n        raise RuntimeError(\"Couldn't find kpsewhich program, make sure you\" +\n                           \" have TeX installed\")\n    except subprocess.CalledProcessError:\n        raise RuntimeError(\"Couldn't find font metrics: '%s'\" % font_name)\n    return font_path.strip()\ndef main():",
        "detail": "components.node_modules.katex.src.metrics.extract_tfms",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "components.node_modules.katex.src.metrics.extract_tfms",
        "description": "components.node_modules.katex.src.metrics.extract_tfms",
        "peekOfCode": "def main():\n    mapping = json.load(sys.stdin)\n    fonts = [\n        'cmbsy10.tfm',\n        'cmbx10.tfm',\n        'cmbxti10.tfm',\n        'cmex10.tfm',\n        'cmmi10.tfm',\n        'cmmib10.tfm',\n        'cmr10.tfm',",
        "detail": "components.node_modules.katex.src.metrics.extract_tfms",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "components.node_modules.katex.src.metrics.extract_ttfs",
        "description": "components.node_modules.katex.src.metrics.extract_ttfs",
        "peekOfCode": "def main():\n    start_json = json.load(sys.stdin)\n    for font in start_json:\n        fontInfo = TTFont(\"../../fonts/KaTeX_\" + font + \".ttf\")\n        glyf = fontInfo[\"glyf\"]\n        widths = fontInfo.getGlyphSet()\n        unitsPerEm = float(fontInfo[\"head\"].unitsPerEm)\n        # We keep ALL Unicode cmaps, not just fontInfo[\"cmap\"].getcmap(3, 1).\n        # This is playing it extra safe, since it reports inconsistencies.\n        # Platform 0 is Unicode, platform 3 is Windows. For platform 3,",
        "detail": "components.node_modules.katex.src.metrics.extract_ttfs",
        "documentation": {}
    },
    {
        "label": "metrics_to_extract",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.metrics.extract_ttfs",
        "description": "components.node_modules.katex.src.metrics.extract_ttfs",
        "peekOfCode": "metrics_to_extract = {\n    # Font name\n    \"AMS-Regular\": {\n        u\"\\u21e2\": None,  # \\dashrightarrow\n        u\"\\u21e0\": None,  # \\dashleftarrow\n    },\n    \"Main-Regular\": {\n        # Skew and italic metrics can't be easily parsed from the TTF. Instead,\n        # we map each character to a \"base character\", which is a character\n        # from the same font with correct italic and skew metrics. A character",
        "detail": "components.node_modules.katex.src.metrics.extract_ttfs",
        "documentation": {}
    },
    {
        "label": "props",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.metrics.format_json",
        "description": "components.node_modules.katex.src.metrics.format_json",
        "peekOfCode": "props = ['depth', 'height', 'italic', 'skew']\nif len(sys.argv) > 1:\n    if sys.argv[1] == '--width':\n        props.append('width')\ndata = json.load(sys.stdin)\nsys.stdout.write(\n  \"// This file is GENERATED by buildMetrics.sh. DO NOT MODIFY.\\n\")\nsep = \"export default {\\n    \"\nfor font in sorted(data):\n    sys.stdout.write(sep + json.dumps(font))",
        "detail": "components.node_modules.katex.src.metrics.format_json",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.metrics.format_json",
        "description": "components.node_modules.katex.src.metrics.format_json",
        "peekOfCode": "data = json.load(sys.stdin)\nsys.stdout.write(\n  \"// This file is GENERATED by buildMetrics.sh. DO NOT MODIFY.\\n\")\nsep = \"export default {\\n    \"\nfor font in sorted(data):\n    sys.stdout.write(sep + json.dumps(font))\n    sep = \": {\\n        \"\n    for glyph in sorted(data[font], key=int):\n        sys.stdout.write(sep + json.dumps(glyph) + \": \")\n        values = [value if value != 0.0 else 0 for value in",
        "detail": "components.node_modules.katex.src.metrics.format_json",
        "documentation": {}
    },
    {
        "label": "sep",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.metrics.format_json",
        "description": "components.node_modules.katex.src.metrics.format_json",
        "peekOfCode": "sep = \"export default {\\n    \"\nfor font in sorted(data):\n    sys.stdout.write(sep + json.dumps(font))\n    sep = \": {\\n        \"\n    for glyph in sorted(data[font], key=int):\n        sys.stdout.write(sep + json.dumps(glyph) + \": \")\n        values = [value if value != 0.0 else 0 for value in\n                  [data[font][glyph][key] for key in props]]\n        sys.stdout.write(json.dumps(values))\n        sep = \",\\n        \"",
        "detail": "components.node_modules.katex.src.metrics.format_json",
        "documentation": {}
    },
    {
        "label": "CharInfoWord",
        "kind": 6,
        "importPath": "components.node_modules.katex.src.metrics.parse_tfm",
        "description": "components.node_modules.katex.src.metrics.parse_tfm",
        "peekOfCode": "class CharInfoWord(object):\n    def __init__(self, word):\n        b1, b2, b3, b4 = (word >> 24,\n                          (word & 0xff0000) >> 16,\n                          (word & 0xff00) >> 8,\n                          word & 0xff)\n        self.width_index = b1\n        self.height_index = b2 >> 4\n        self.depth_index = b2 & 0x0f\n        self.italic_index = (b3 & 0b11111100) >> 2",
        "detail": "components.node_modules.katex.src.metrics.parse_tfm",
        "documentation": {}
    },
    {
        "label": "LigKernProgram",
        "kind": 6,
        "importPath": "components.node_modules.katex.src.metrics.parse_tfm",
        "description": "components.node_modules.katex.src.metrics.parse_tfm",
        "peekOfCode": "class LigKernProgram(object):\n    def __init__(self, program):\n        self.program = program\n    def execute(self, start, next_char):\n        curr_instruction = start\n        while True:\n            instruction = self.program[curr_instruction]\n            (skip, inst_next_char, op, remainder) = instruction\n            if inst_next_char == next_char:\n                if op < 128:",
        "detail": "components.node_modules.katex.src.metrics.parse_tfm",
        "documentation": {}
    },
    {
        "label": "TfmCharMetrics",
        "kind": 6,
        "importPath": "components.node_modules.katex.src.metrics.parse_tfm",
        "description": "components.node_modules.katex.src.metrics.parse_tfm",
        "peekOfCode": "class TfmCharMetrics(object):\n    def __init__(self, width, height, depth, italic, kern_table):\n        self.width = width\n        self.height = height\n        self.depth = depth\n        self.italic_correction = italic\n        self.kern_table = kern_table\nclass TfmFile(object):\n    def __init__(self, start_char, end_char, char_info, width_table,\n                 height_table, depth_table, italic_table, ligkern_table,",
        "detail": "components.node_modules.katex.src.metrics.parse_tfm",
        "documentation": {}
    },
    {
        "label": "TfmFile",
        "kind": 6,
        "importPath": "components.node_modules.katex.src.metrics.parse_tfm",
        "description": "components.node_modules.katex.src.metrics.parse_tfm",
        "peekOfCode": "class TfmFile(object):\n    def __init__(self, start_char, end_char, char_info, width_table,\n                 height_table, depth_table, italic_table, ligkern_table,\n                 kern_table):\n        self.start_char = start_char\n        self.end_char = end_char\n        self.char_info = char_info\n        self.width_table = width_table\n        self.height_table = height_table\n        self.depth_table = depth_table",
        "detail": "components.node_modules.katex.src.metrics.parse_tfm",
        "documentation": {}
    },
    {
        "label": "TfmReader",
        "kind": 6,
        "importPath": "components.node_modules.katex.src.metrics.parse_tfm",
        "description": "components.node_modules.katex.src.metrics.parse_tfm",
        "peekOfCode": "class TfmReader(object):\n    def __init__(self, f):\n        self.f = f\n    def read_byte(self):\n        return ord(self.f.read(1))\n    def read_halfword(self):\n        b1 = self.read_byte()\n        b2 = self.read_byte()\n        return (b1 << 8) | b2\n    def read_word(self):",
        "detail": "components.node_modules.katex.src.metrics.parse_tfm",
        "documentation": {}
    },
    {
        "label": "read_tfm_file",
        "kind": 2,
        "importPath": "components.node_modules.katex.src.metrics.parse_tfm",
        "description": "components.node_modules.katex.src.metrics.parse_tfm",
        "peekOfCode": "def read_tfm_file(file_name):\n    with open(file_name, 'rb') as f:\n        reader = TfmReader(f)\n        # file_size\n        reader.read_halfword()\n        header_size = reader.read_halfword()\n        start_char = reader.read_halfword()\n        end_char = reader.read_halfword()\n        width_table_size = reader.read_halfword()\n        height_table_size = reader.read_halfword()",
        "detail": "components.node_modules.katex.src.metrics.parse_tfm",
        "documentation": {}
    },
    {
        "label": "analyze_chroma_database",
        "kind": 2,
        "importPath": "analyze_chroma_database",
        "description": "analyze_chroma_database",
        "peekOfCode": "def analyze_chroma_database():\n    \"\"\"åˆ†æž ChromaDB SQLite æ•°æ®åº“ç»“æž„\"\"\"\n    print(\"ðŸ” åˆ†æž ChromaDB æ•°æ®åº“ç»“æž„\")\n    print(\"=\" * 80)\n    chroma_db_path = \"storage/chroma_db_new/chroma.sqlite3\"\n    if not os.path.exists(chroma_db_path):\n        print(f\"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {chroma_db_path}\")\n        return None\n    # è¿žæŽ¥æ•°æ®åº“\n    conn = sqlite3.connect(chroma_db_path)",
        "detail": "analyze_chroma_database",
        "documentation": {}
    },
    {
        "label": "analyze_table",
        "kind": 2,
        "importPath": "analyze_chroma_database",
        "description": "analyze_chroma_database",
        "peekOfCode": "def analyze_table(cursor, table_name):\n    \"\"\"åˆ†æžå•ä¸ªè¡¨çš„ç»“æž„å’Œæ•°æ®\"\"\"\n    table_info = {\n        'columns': [],\n        'indexes': [],\n        'row_count': 0,\n        'sample_data': []\n    }\n    try:\n        # èŽ·å–è¡¨ç»“æž„",
        "detail": "analyze_chroma_database",
        "documentation": {}
    },
    {
        "label": "generate_markdown_report",
        "kind": 2,
        "importPath": "analyze_chroma_database",
        "description": "analyze_chroma_database",
        "peekOfCode": "def generate_markdown_report(database_info):\n    \"\"\"ç”Ÿæˆ Markdown æ ¼å¼çš„åˆ†æžæŠ¥å‘Š\"\"\"\n    md_content = f\"\"\"# ChromaDB æ•°æ®åº“ç»“æž„åˆ†æžæŠ¥å‘Š\n## ðŸ“Š åŸºæœ¬ä¿¡æ¯\n- **æ•°æ®åº“æ–‡ä»¶**: `{database_info['file_path']}`\n- **åˆ†æžæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n- **æ€»è¡¨æ•°**: {database_info['total_tables']}\n## ðŸ“‹ è¡¨ç»“æž„è¯¦ç»†è¯´æ˜Ž\n\"\"\"\n    for table_name, table_info in database_info['tables'].items():",
        "detail": "analyze_chroma_database",
        "documentation": {}
    },
    {
        "label": "guess_table_purpose",
        "kind": 2,
        "importPath": "analyze_chroma_database",
        "description": "analyze_chroma_database",
        "peekOfCode": "def guess_table_purpose(table_name, table_info):\n    \"\"\"æ ¹æ®è¡¨åå’Œç»“æž„æŽ¨æµ‹è¡¨çš„ç”¨é€”\"\"\"\n    purposes = {\n        'collections': \"\"\"\n**é›†åˆç®¡ç†è¡¨** - å­˜å‚¨ ChromaDB ä¸­çš„å‘é‡é›†åˆä¿¡æ¯\n- ç®¡ç†ä¸åŒçš„æ–‡æ¡£é›†åˆ\n- æ¯ä¸ªé›†åˆå¯ä»¥æœ‰ç‹¬ç«‹çš„é…ç½®å’Œå…ƒæ•°æ®\n- æ”¯æŒå¤šç§Ÿæˆ·å’Œæ•°æ®éš”ç¦»\n        \"\"\",\n        'embeddings': \"\"\"",
        "detail": "analyze_chroma_database",
        "documentation": {}
    },
    {
        "label": "guess_column_purpose",
        "kind": 2,
        "importPath": "analyze_chroma_database",
        "description": "analyze_chroma_database",
        "peekOfCode": "def guess_column_purpose(table_name, col_name, col_type):\n    \"\"\"æ ¹æ®å­—æ®µåå’Œç±»åž‹æŽ¨æµ‹å­—æ®µç”¨é€”\"\"\"\n    # é€šç”¨å­—æ®µç”¨é€”\n    common_purposes = {\n        'id': 'å”¯ä¸€æ ‡è¯†ç¬¦',\n        'uuid': 'å…¨å±€å”¯ä¸€æ ‡è¯†ç¬¦',\n        'name': 'åç§°',\n        'metadata': 'å…ƒæ•°æ®ä¿¡æ¯',\n        'created_at': 'åˆ›å»ºæ—¶é—´',\n        'updated_at': 'æ›´æ–°æ—¶é—´',",
        "detail": "analyze_chroma_database",
        "documentation": {}
    },
    {
        "label": "analyze_data_field",
        "kind": 2,
        "importPath": "analyze_data_field",
        "description": "analyze_data_field",
        "peekOfCode": "def analyze_data_field():\n    \"\"\"åˆ†æž data å­—æ®µçš„å†…å®¹å’Œç»“æž„\"\"\"\n    print(\"ðŸ” åˆ†æž Documents è¡¨çš„ data å­—æ®µ\")\n    print(\"=\" * 60)\n    if not os.path.exists('storage/docstore.db'):\n        print(\"âŒ docstore.db ä¸å­˜åœ¨\")\n        return\n    conn = sqlite3.connect('storage/docstore.db')\n    cursor = conn.execute('SELECT doc_id, data FROM documents LIMIT 1')\n    row = cursor.fetchone()",
        "detail": "analyze_data_field",
        "documentation": {}
    },
    {
        "label": "analyze_unicode_issue",
        "kind": 2,
        "importPath": "analyze_data_field",
        "description": "analyze_data_field",
        "peekOfCode": "def analyze_unicode_issue():\n    \"\"\"ä¸“é—¨åˆ†æž Unicode ç¼–ç é—®é¢˜\"\"\"\n    print(\"\\nðŸ”¤ Unicode ç¼–ç é—®é¢˜åˆ†æž\")\n    print(\"=\" * 60)\n    conn = sqlite3.connect('storage/docstore.db')\n    cursor = conn.execute('SELECT doc_id, data FROM documents LIMIT 3')\n    rows = cursor.fetchall()\n    for i, (doc_id, data_json) in enumerate(rows, 1):\n        try:\n            data = json.loads(data_json)",
        "detail": "analyze_data_field",
        "documentation": {}
    },
    {
        "label": "suggest_solutions",
        "kind": 2,
        "importPath": "analyze_data_field",
        "description": "analyze_data_field",
        "peekOfCode": "def suggest_solutions():\n    \"\"\"æå‡ºè§£å†³æ–¹æ¡ˆå»ºè®®\"\"\"\n    print(\"\\nðŸ’¡ è§£å†³æ–¹æ¡ˆå»ºè®®\")\n    print(\"=\" * 60)\n    print(\"ðŸŽ¯ data å­—æ®µä¼˜åŒ–æ–¹æ¡ˆ:\")\n    print(\"1. ä¿æŒ LlamaIndex æ ‡å‡†æ ¼å¼ï¼Œç¡®ä¿å…¼å®¹æ€§\")\n    print(\"2. åœ¨åŽç«¯ API ä¸­è§£æž data å­—æ®µï¼Œæå–éœ€è¦çš„ä¿¡æ¯\")\n    print(\"3. å‰ç«¯åªæŽ¥æ”¶å¤„ç†åŽçš„ç®€åŒ–æ•°æ®ï¼Œé¿å…å¤æ‚çš„ Unicode å¤„ç†\")\n    print(\"\\nðŸ”§ Unicode å¤„ç†æ–¹æ¡ˆ:\")\n    print(\"1. åŽç«¯è´Ÿè´£ Unicode è§£ç ï¼Œç¡®ä¿æ–‡æœ¬æ­£ç¡®æ˜¾ç¤º\")",
        "detail": "analyze_data_field",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "analyze_data_field",
        "description": "analyze_data_field",
        "peekOfCode": "def main():\n    \"\"\"ä¸»å‡½æ•°\"\"\"\n    analyze_data_field()\n    analyze_unicode_issue()\n    suggest_solutions()\nif __name__ == \"__main__\":\n    main()",
        "detail": "analyze_data_field",
        "documentation": {}
    },
    {
        "label": "analyze_id_relationship",
        "kind": 2,
        "importPath": "analyze_id_relationship",
        "description": "analyze_id_relationship",
        "peekOfCode": "def analyze_id_relationship():\n    \"\"\"åˆ†æžIDå…³ç³»\"\"\"\n    print(\"ðŸ” åˆ†æž LlamaIndex ID å…³ç³»\")\n    print(\"=\" * 60)\n    # 1. æ£€æŸ¥ docstore.db ä¸­çš„æ•°æ®\n    docstore_path = 'storage/docstore.db'\n    if os.path.exists(docstore_path):\n        print(\"\\n=== DocStore æ•°æ®åˆ†æž ===\")\n        conn = sqlite3.connect(docstore_path)\n        # æŸ¥çœ‹ documents è¡¨",
        "detail": "analyze_id_relationship",
        "documentation": {}
    },
    {
        "label": "analyze_relationships",
        "kind": 2,
        "importPath": "analyze_relationships",
        "description": "analyze_relationships",
        "peekOfCode": "def analyze_relationships():\n    \"\"\"åˆ†æžèŠ‚ç‚¹å…³ç³»\"\"\"\n    target_id = \"10316370-d5dc-4a54-ac60-7e853b805328\"\n    print(f\"ðŸ” åˆ†æžèŠ‚ç‚¹å…³ç³»ï¼Œè¿½è¸ªIDæ¥æº\")\n    print(\"=\" * 60)\n    # 1. ä»Ž DocStore èŽ·å–å®Œæ•´çš„èŠ‚ç‚¹æ•°æ®\n    print(\"\\n=== 1. DocStore ä¸­çš„èŠ‚ç‚¹å…³ç³» ===\")\n    docstore_path = 'storage/docstore.db'\n    if os.path.exists(docstore_path):\n        conn = sqlite3.connect(docstore_path)",
        "detail": "analyze_relationships",
        "documentation": {}
    },
    {
        "label": "analyze_storage_directory",
        "kind": 2,
        "importPath": "analyze_storage_files",
        "description": "analyze_storage_files",
        "peekOfCode": "def analyze_storage_directory():\n    \"\"\"åˆ†æžå­˜å‚¨ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶\"\"\"\n    storage_dir = Path(\"storage\")\n    print(\"ðŸ“ å­˜å‚¨ç›®å½•åˆ†æžæŠ¥å‘Š\")\n    print(\"=\" * 60)\n    # 1. åˆ†æžæ‰€æœ‰æ–‡ä»¶\n    print(\"ðŸ“‹ æ–‡ä»¶æ¸…å•:\")\n    for file_path in storage_dir.rglob(\"*\"):\n        if file_path.is_file():\n            size = file_path.stat().st_size",
        "detail": "analyze_storage_files",
        "documentation": {}
    },
    {
        "label": "analyze_sqlite_db",
        "kind": 2,
        "importPath": "analyze_storage_files",
        "description": "analyze_storage_files",
        "peekOfCode": "def analyze_sqlite_db(db_path):\n    \"\"\"åˆ†æžSQLiteæ•°æ®åº“\"\"\"\n    try:\n        with sqlite3.connect(db_path) as conn:\n            cursor = conn.cursor()\n            # èŽ·å–è¡¨åˆ—è¡¨\n            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n            tables = [row[0] for row in cursor.fetchall()]\n            print(f\"    è¡¨æ•°é‡: {len(tables)}\")\n            for table in tables:",
        "detail": "analyze_storage_files",
        "documentation": {}
    },
    {
        "label": "analyze_json_file",
        "kind": 2,
        "importPath": "analyze_storage_files",
        "description": "analyze_storage_files",
        "peekOfCode": "def analyze_json_file(json_path):\n    \"\"\"åˆ†æžJSONæ–‡ä»¶\"\"\"\n    try:\n        with open(json_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        print(f\"    æ–‡ä»¶å¤§å°: {json_path.stat().st_size:,} bytes\")\n        print(f\"    JSONç»“æž„:\")\n        def analyze_json_structure(obj, indent=6):\n            if isinstance(obj, dict):\n                print(f\"{' ' * indent}å­—å…¸ - {len(obj)} ä¸ªé”®:\")",
        "detail": "analyze_storage_files",
        "documentation": {}
    },
    {
        "label": "check_file_usage",
        "kind": 2,
        "importPath": "analyze_storage_files",
        "description": "analyze_storage_files",
        "peekOfCode": "def check_file_usage():\n    \"\"\"æ£€æŸ¥æ–‡ä»¶çš„å®žé™…ä½¿ç”¨æƒ…å†µ\"\"\"\n    print(\"ðŸ” æ–‡ä»¶ä½¿ç”¨æƒ…å†µåˆ†æž:\")\n    print(\"=\" * 60)\n    # æ£€æŸ¥ä»£ç ä¸­å¯¹è¿™äº›æ–‡ä»¶çš„å¼•ç”¨\n    storage_files = [\n        \"docstore.db\",\n        \"index_store.db\", \n        \"chroma.sqlite3\",\n        \"graph_store.json\",",
        "detail": "analyze_storage_files",
        "documentation": {}
    },
    {
        "label": "analyze_docstore",
        "kind": 2,
        "importPath": "analyze_storage_issues",
        "description": "analyze_storage_issues",
        "peekOfCode": "def analyze_docstore():\n    \"\"\"åˆ†æž docstore.db çš„é—®é¢˜\"\"\"\n    print(\"ðŸ” åˆ†æž DOCSTORE.DB\")\n    print(\"=\" * 60)\n    if not os.path.exists('storage/docstore.db'):\n        print(\"âŒ docstore.db ä¸å­˜åœ¨\")\n        return\n    conn = sqlite3.connect('storage/docstore.db')\n    # 1. æ£€æŸ¥ documents è¡¨ç»“æž„\n    print(\"ðŸ“‹ Documents è¡¨ç»“æž„:\")",
        "detail": "analyze_storage_issues",
        "documentation": {}
    },
    {
        "label": "analyze_chroma",
        "kind": 2,
        "importPath": "analyze_storage_issues",
        "description": "analyze_storage_issues",
        "peekOfCode": "def analyze_chroma():\n    \"\"\"åˆ†æž ChromaDB çš„é—®é¢˜\"\"\"\n    print(\"\\nðŸ§  åˆ†æž CHROMADB\")\n    print(\"=\" * 60)\n    chroma_path = 'storage/chroma_db_new'\n    if not os.path.exists(chroma_path):\n        print(\"âŒ ChromaDB ä¸å­˜åœ¨\")\n        return\n    try:\n        client = chromadb.PersistentClient(",
        "detail": "analyze_storage_issues",
        "documentation": {}
    },
    {
        "label": "analyze_id_consistency",
        "kind": 2,
        "importPath": "analyze_storage_issues",
        "description": "analyze_storage_issues",
        "peekOfCode": "def analyze_id_consistency():\n    \"\"\"åˆ†æž docstore å’Œ chroma ä¹‹é—´çš„ ID ä¸€è‡´æ€§\"\"\"\n    print(\"\\nðŸ”— åˆ†æž ID ä¸€è‡´æ€§\")\n    print(\"=\" * 60)\n    # èŽ·å– docstore ä¸­çš„æ‰€æœ‰ doc_id\n    docstore_ids = set()\n    if os.path.exists('storage/docstore.db'):\n        conn = sqlite3.connect('storage/docstore.db')\n        cursor = conn.execute(\"SELECT doc_id FROM documents\")\n        docstore_ids = {row[0] for row in cursor.fetchall()}",
        "detail": "analyze_storage_issues",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "analyze_storage_issues",
        "description": "analyze_storage_issues",
        "peekOfCode": "def main():\n    \"\"\"ä¸»å‡½æ•°\"\"\"\n    print(\"ðŸ” å­˜å‚¨é€»è¾‘é—®é¢˜åˆ†æž\")\n    print(\"=\" * 80)\n    analyze_docstore()\n    analyze_chroma()\n    analyze_id_consistency()\n    print(\"\\nðŸ’¡ é—®é¢˜æ€»ç»“:\")\n    print(\"1. æ£€æŸ¥ documents è¡¨æ˜¯å¦çœŸçš„ç”¨äºŽå­˜å‚¨æ–‡æœ¬å—ä¿¡æ¯\")\n    print(\"2. æ£€æŸ¥ chunk_index æ˜¯å¦æ­£ç¡®è®¾ç½®\")",
        "detail": "analyze_storage_issues",
        "documentation": {}
    },
    {
        "label": "check_chunk_index_issue",
        "kind": 2,
        "importPath": "check_chunk_index",
        "description": "check_chunk_index",
        "peekOfCode": "def check_chunk_index_issue():\n    \"\"\"æ£€æŸ¥ chunk_index çš„åˆ†å¸ƒå’Œé—®é¢˜\"\"\"\n    print(\"ðŸ” æ£€æŸ¥ Documents è¡¨ä¸­çš„ chunk_index é—®é¢˜\")\n    print(\"=\" * 60)\n    with sqlite3.connect('storage/docstore.db') as conn:\n        # 1. æ£€æŸ¥ chunk_index åˆ†å¸ƒ\n        cursor = conn.execute('''\n            SELECT file_name, chunk_index, COUNT(*) as count\n            FROM documents \n            GROUP BY file_name, chunk_index ",
        "detail": "check_chunk_index",
        "documentation": {}
    },
    {
        "label": "analyze_chunking_logic",
        "kind": 2,
        "importPath": "check_chunk_index",
        "description": "check_chunk_index",
        "peekOfCode": "def analyze_chunking_logic():\n    \"\"\"åˆ†æžåˆ†å—é€»è¾‘çš„é—®é¢˜\"\"\"\n    print(f\"\\nðŸ” åˆ†æžåˆ†å—é€»è¾‘é—®é¢˜\")\n    print(\"=\" * 60)\n    print(\"ðŸ“‹ å¯èƒ½çš„åŽŸå› :\")\n    print(\"1. generate.py ä¸­çš„ chunk_index è®¡ç®—é€»è¾‘æœ‰è¯¯\")\n    print(\"2. æ¯æ¬¡è¿è¡Œ generate éƒ½é‡æ–°ç”Ÿæˆ node_idï¼Œå¯¼è‡´é‡å¤\")\n    print(\"3. æ–‡æ¡£åˆ†å—å™¨å¯èƒ½æ²¡æœ‰æ­£ç¡®åˆ†å—\")\n    print(\"4. chunk_index èµ‹å€¼é€»è¾‘é”™è¯¯\")\n    print(f\"\\nðŸŽ¯ æ­£ç¡®çš„ chunk_index åº”è¯¥æ˜¯:\")",
        "detail": "check_chunk_index",
        "documentation": {}
    },
    {
        "label": "check_generate_logic",
        "kind": 2,
        "importPath": "check_chunk_index",
        "description": "check_chunk_index",
        "peekOfCode": "def check_generate_logic():\n    \"\"\"æ£€æŸ¥ generate.py ä¸­çš„é€»è¾‘\"\"\"\n    print(f\"\\nðŸ” æ£€æŸ¥ generate.py ä¸­çš„åˆ†å—é€»è¾‘\")\n    print(\"=\" * 60)\n    try:\n        with open('generate.py', 'r', encoding='utf-8') as f:\n            content = f.read()\n        # æŸ¥æ‰¾ chunk_index ç›¸å…³çš„ä»£ç \n        lines = content.split('\\n')\n        chunk_index_lines = []",
        "detail": "check_chunk_index",
        "documentation": {}
    },
    {
        "label": "suggest_fixes",
        "kind": 2,
        "importPath": "check_chunk_index",
        "description": "check_chunk_index",
        "peekOfCode": "def suggest_fixes():\n    \"\"\"å»ºè®®ä¿®å¤æ–¹æ¡ˆ\"\"\"\n    print(f\"\\nðŸ’¡ ä¿®å¤å»ºè®®\")\n    print(\"=\" * 60)\n    print(\"ðŸ”§ æ–¹æ¡ˆ1: ä¿®å¤ generate.py ä¸­çš„ chunk_index é€»è¾‘\")\n    print(\"- ç¡®ä¿ä¸ºåŒä¸€æ–‡ä»¶çš„ä¸åŒå—åˆ†é…é€’å¢žçš„ chunk_index\")\n    print(\"- ä¿®æ”¹å¾ªçŽ¯é€»è¾‘ï¼Œæ­£ç¡®è®¡ç®—å—ç´¢å¼•\")\n    print(f\"\\nðŸ”§ æ–¹æ¡ˆ2: é‡æ–°ç”Ÿæˆç´¢å¼•\")\n    print(\"- å…ˆé‡ç½®æ•°æ®åº“\")\n    print(\"- ä¿®å¤ä»£ç åŽé‡æ–°è¿è¡Œ generate\")",
        "detail": "check_chunk_index",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "check_chunk_index",
        "description": "check_chunk_index",
        "peekOfCode": "def main():\n    \"\"\"ä¸»æ£€æŸ¥æµç¨‹\"\"\"\n    check_chunk_index_issue()\n    analyze_chunking_logic()\n    check_generate_logic()\n    suggest_fixes()\nif __name__ == \"__main__\":\n    main()",
        "detail": "check_chunk_index",
        "documentation": {}
    },
    {
        "label": "check_database_status",
        "kind": 2,
        "importPath": "check_db_duplicates",
        "description": "check_db_duplicates",
        "peekOfCode": "def check_database_status():\n    \"\"\"æ£€æŸ¥æ•°æ®åº“çŠ¶æ€å’Œé‡å¤æ•°æ®\"\"\"\n    db_path = \"storage/docstore.db\"\n    if not os.path.exists(db_path):\n        print(f\"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}\")\n        return\n    print(f\"ðŸ“Š æ£€æŸ¥æ•°æ®åº“: {db_path}\")\n    print(\"=\" * 50)\n    with sqlite3.connect(db_path) as conn:\n        cursor = conn.cursor()",
        "detail": "check_db_duplicates",
        "documentation": {}
    },
    {
        "label": "suggest_cleanup_actions",
        "kind": 2,
        "importPath": "check_db_duplicates",
        "description": "check_db_duplicates",
        "peekOfCode": "def suggest_cleanup_actions():\n    \"\"\"å»ºè®®æ¸…ç†æ“ä½œ\"\"\"\n    print(\"ðŸ› ï¸  æ•°æ®åº“æ¸…ç†å»ºè®®:\")\n    print(\"=\" * 50)\n    print(\"1. ðŸ”„ å®Œå…¨é‡ç½®æ•°æ®åº“ (æŽ¨è)\")\n    print(\"   - åˆ é™¤çŽ°æœ‰æ•°æ®åº“æ–‡ä»¶\")\n    print(\"   - é‡æ–°åˆå§‹åŒ–å¹²å‡€çš„æ•°æ®åº“\")\n    print(\"   - é‡æ–°ç´¢å¼•æ‰€æœ‰æ–‡æ¡£\")\n    print()\n    print(\"2. ðŸ§¹ æ¸…ç†é‡å¤æ•°æ®\")",
        "detail": "check_db_duplicates",
        "documentation": {}
    },
    {
        "label": "check_sqlite_data",
        "kind": 2,
        "importPath": "check_duplicates",
        "description": "check_duplicates",
        "peekOfCode": "def check_sqlite_data():\n    \"\"\"æ£€æŸ¥ SQLite æ•°æ®åº“ä¸­çš„æ•°æ®\"\"\"\n    print(\"ðŸ” æ£€æŸ¥ SQLite æ•°æ®åº“...\")\n    # æ£€æŸ¥ documents è¡¨\n    with sqlite3.connect('storage/docstore.db') as conn:\n        cursor = conn.execute('SELECT COUNT(*) FROM documents')\n        doc_count = cursor.fetchone()[0]\n        print(f'ðŸ“Š Documents è¡¨ä¸­æœ‰ {doc_count} æ¡è®°å½•')\n        # æŒ‰æ–‡ä»¶åˆ†ç»„ç»Ÿè®¡\n        cursor = conn.execute('''",
        "detail": "check_duplicates",
        "documentation": {}
    },
    {
        "label": "check_chromadb_data",
        "kind": 2,
        "importPath": "check_duplicates",
        "description": "check_duplicates",
        "peekOfCode": "def check_chromadb_data():\n    \"\"\"æ£€æŸ¥ ChromaDB ä¸­çš„å‘é‡æ•°æ®\"\"\"\n    print(\"\\nðŸ” æ£€æŸ¥ ChromaDB å‘é‡æ•°æ®...\")\n    try:\n        chroma_client = chromadb.PersistentClient(\n            path='storage/chroma_db_new',\n            settings=ChromaSettings(\n                anonymized_telemetry=False,\n                allow_reset=True\n            )",
        "detail": "check_duplicates",
        "documentation": {}
    },
    {
        "label": "analyze_duplication_issue",
        "kind": 2,
        "importPath": "check_duplicates",
        "description": "check_duplicates",
        "peekOfCode": "def analyze_duplication_issue():\n    \"\"\"åˆ†æžé‡å¤æ•°æ®é—®é¢˜\"\"\"\n    print(\"\\nðŸ” åˆ†æžé‡å¤æ•°æ®é—®é¢˜...\")\n    # æ£€æŸ¥ generate.py çš„é€»è¾‘\n    print(\"ðŸ“‹ Generate å‘½ä»¤çš„è¡Œä¸º:\")\n    print(\"1. æ¯æ¬¡è¿è¡Œéƒ½ä¼šé‡æ–°è¯»å– data ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶\")\n    print(\"2. ä¸ºæ¯ä¸ªæ–‡æ¡£ç”Ÿæˆæ–°çš„ node_id (UUID)\")\n    print(\"3. ä½¿ç”¨ INSERT OR REPLACE æ›´æ–° SQLite æ•°æ®\")\n    print(\"4. å‘ ChromaDB æ·»åŠ æ–°çš„å‘é‡ï¼ˆå¯èƒ½é‡å¤ï¼‰\")\n    # æ£€æŸ¥å½“å‰çš„é‡å¤æƒ…å†µ",
        "detail": "check_duplicates",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "check_duplicates",
        "description": "check_duplicates",
        "peekOfCode": "def main():\n    \"\"\"ä¸»æ£€æŸ¥æµç¨‹\"\"\"\n    print(\"ðŸ” å¼€å§‹æ£€æŸ¥æ•°æ®åº“é‡å¤æ•°æ®\")\n    print(\"=\" * 60)\n    check_sqlite_data()\n    check_chromadb_data()\n    analyze_duplication_issue()\n    print(\"\\n\" + \"=\" * 60)\n    print(\"ðŸŽ¯ ç»“è®º:\")\n    print(\"é‡å¤è¿è¡Œ 'uv run generate' ä¼šå¯¼è‡´:\")",
        "detail": "check_duplicates",
        "documentation": {}
    },
    {
        "label": "check_embedding_metadata",
        "kind": 2,
        "importPath": "check_embedding_metadata",
        "description": "check_embedding_metadata",
        "peekOfCode": "def check_embedding_metadata():\n    chroma_db_path = 'storage/chroma_db_new/chroma.sqlite3'\n    if not os.path.exists(chroma_db_path):\n        print(f\"âŒ ChromaDB æ–‡ä»¶ä¸å­˜åœ¨: {chroma_db_path}\")\n        return\n    try:\n        conn = sqlite3.connect(chroma_db_path)\n        # 1. æŸ¥çœ‹æ‰€æœ‰è¡¨\n        print(\"=== æ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨ ===\")\n        cursor = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")",
        "detail": "check_embedding_metadata",
        "documentation": {}
    },
    {
        "label": "check_id_inconsistency",
        "kind": 2,
        "importPath": "check_id_inconsistency",
        "description": "check_id_inconsistency",
        "peekOfCode": "def check_id_inconsistency():\n    \"\"\"æ£€æŸ¥IDä¸ä¸€è‡´é—®é¢˜\"\"\"\n    print(\"ðŸš¨ æ£€æŸ¥ DocStore å’Œ ChromaDB ä¹‹é—´çš„ ID ä¸ä¸€è‡´é—®é¢˜\")\n    print(\"=\" * 70)\n    # 1. ä»Ž DocStore èŽ·å–æ•°æ®\n    docstore_path = 'storage/docstore.db'\n    docstore_ids = {}\n    if os.path.exists(docstore_path):\n        conn = sqlite3.connect(docstore_path)\n        cursor = conn.execute(\"SELECT doc_id, file_name, chunk_index FROM documents ORDER BY file_name, chunk_index\")",
        "detail": "check_id_inconsistency",
        "documentation": {}
    },
    {
        "label": "analyze_storage_files",
        "kind": 2,
        "importPath": "cleanup_storage",
        "description": "cleanup_storage",
        "peekOfCode": "def analyze_storage_files():\n    \"\"\"åˆ†æžå­˜å‚¨ç›®å½•ä¸­çš„æ–‡ä»¶\"\"\"\n    storage_dir = \"storage\"\n    print(\"ðŸ“ å­˜å‚¨ç›®å½•æ–‡ä»¶åˆ†æž\")\n    print(\"=\" * 60)\n    if not os.path.exists(storage_dir):\n        print(\"âŒ å­˜å‚¨ç›®å½•ä¸å­˜åœ¨\")\n        return\n    # å½“å‰æ­£åœ¨ä½¿ç”¨çš„æ–‡ä»¶ï¼ˆæ ¹æ® storage_config.pyï¼‰\n    current_files = {",
        "detail": "cleanup_storage",
        "documentation": {}
    },
    {
        "label": "get_size_info",
        "kind": 2,
        "importPath": "cleanup_storage",
        "description": "cleanup_storage",
        "peekOfCode": "def get_size_info(path):\n    \"\"\"èŽ·å–æ–‡ä»¶æˆ–ç›®å½•å¤§å°ä¿¡æ¯\"\"\"\n    try:\n        if os.path.isfile(path):\n            size = os.path.getsize(path)\n            return format_size(size)\n        elif os.path.isdir(path):\n            total_size = 0\n            for dirpath, dirnames, filenames in os.walk(path):\n                for filename in filenames:",
        "detail": "cleanup_storage",
        "documentation": {}
    },
    {
        "label": "format_size",
        "kind": 2,
        "importPath": "cleanup_storage",
        "description": "cleanup_storage",
        "peekOfCode": "def format_size(size_bytes):\n    \"\"\"æ ¼å¼åŒ–æ–‡ä»¶å¤§å°\"\"\"\n    if size_bytes == 0:\n        return \"0 B\"\n    for unit in ['B', 'KB', 'MB', 'GB']:\n        if size_bytes < 1024.0:\n            return f\"{size_bytes:.1f} {unit}\"\n        size_bytes /= 1024.0\n    return f\"{size_bytes:.1f} TB\"\ndef cleanup_storage():",
        "detail": "cleanup_storage",
        "documentation": {}
    },
    {
        "label": "cleanup_storage",
        "kind": 2,
        "importPath": "cleanup_storage",
        "description": "cleanup_storage",
        "peekOfCode": "def cleanup_storage():\n    \"\"\"æ¸…ç†å­˜å‚¨ç›®å½•\"\"\"\n    storage_dir = \"storage\"\n    print(\"\\nðŸ§¹ å¼€å§‹æ¸…ç†å­˜å‚¨ç›®å½•\")\n    print(\"=\" * 60)\n    # è¦åˆ é™¤çš„æ–‡ä»¶å’Œç›®å½•\n    items_to_delete = []\n    for item in os.listdir(storage_dir):\n        item_path = os.path.join(storage_dir, item)\n        # æ£€æŸ¥æ˜¯å¦åº”è¯¥åˆ é™¤",
        "detail": "cleanup_storage",
        "documentation": {}
    },
    {
        "label": "verify_current_setup",
        "kind": 2,
        "importPath": "cleanup_storage",
        "description": "cleanup_storage",
        "peekOfCode": "def verify_current_setup():\n    \"\"\"éªŒè¯å½“å‰è®¾ç½®æ˜¯å¦æ­£å¸¸\"\"\"\n    print(\"\\nðŸ” éªŒè¯å½“å‰å­˜å‚¨è®¾ç½®\")\n    print(\"=\" * 60)\n    storage_dir = \"storage\"\n    required_files = {\n        \"chroma_db_new\": \"ChromaDB å‘é‡æ•°æ®åº“ç›®å½•\",\n        \"docstore.db\": \"SQLite æ–‡æ¡£å­˜å‚¨\",\n        \"index_store.db\": \"SQLite ç´¢å¼•å­˜å‚¨\"\n    }",
        "detail": "cleanup_storage",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "cleanup_storage",
        "description": "cleanup_storage",
        "peekOfCode": "def main():\n    \"\"\"ä¸»æ¸…ç†æµç¨‹\"\"\"\n    print(\"ðŸ§¹ å­˜å‚¨ç›®å½•æ¸…ç†å·¥å…·\")\n    print(\"=\" * 60)\n    # 1. åˆ†æžæ–‡ä»¶\n    analyze_storage_files()\n    # 2. æ¸…ç†æ–‡ä»¶\n    cleanup_storage()\n    # 3. éªŒè¯è®¾ç½®\n    verify_current_setup()",
        "detail": "cleanup_storage",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "cleanup_storage",
        "description": "cleanup_storage",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef analyze_storage_files():\n    \"\"\"åˆ†æžå­˜å‚¨ç›®å½•ä¸­çš„æ–‡ä»¶\"\"\"\n    storage_dir = \"storage\"\n    print(\"ðŸ“ å­˜å‚¨ç›®å½•æ–‡ä»¶åˆ†æž\")\n    print(\"=\" * 60)\n    if not os.path.exists(storage_dir):\n        print(\"âŒ å­˜å‚¨ç›®å½•ä¸å­˜åœ¨\")\n        return\n    # å½“å‰æ­£åœ¨ä½¿ç”¨çš„æ–‡ä»¶ï¼ˆæ ¹æ® storage_config.pyï¼‰",
        "detail": "cleanup_storage",
        "documentation": {}
    },
    {
        "label": "clear_all_data",
        "kind": 2,
        "importPath": "clear_and_test",
        "description": "clear_and_test",
        "peekOfCode": "def clear_all_data():\n    \"\"\"æ¸…ç†æ‰€æœ‰æ•°æ®\"\"\"\n    print(\"ðŸ§¹ æ¸…ç†æ‰€æœ‰æ•°æ®\")\n    print(\"=\" * 60)\n    print(\"âš ï¸  è¯·å…ˆåœæ­¢æœåŠ¡å™¨ï¼Œç„¶åŽæ‰‹åŠ¨åˆ é™¤ä»¥ä¸‹ç›®å½•å’Œæ–‡ä»¶:\")\n    print(f\"1. åˆ é™¤å­˜å‚¨ç›®å½•: {STORAGE_DIR}\")\n    print(\"2. åˆ é™¤ data ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶\")\n    print(\"3. ç„¶åŽé‡æ–°è¿è¡Œæ­¤è„šæœ¬\")\n    # æ£€æŸ¥æ˜¯å¦å·²æ¸…ç†\n    if os.path.exists(STORAGE_DIR):",
        "detail": "clear_and_test",
        "documentation": {}
    },
    {
        "label": "create_test_files",
        "kind": 2,
        "importPath": "clear_and_test",
        "description": "clear_and_test",
        "peekOfCode": "def create_test_files():\n    \"\"\"åˆ›å»ºæµ‹è¯•æ–‡ä»¶\"\"\"\n    print(\"\\nðŸ“ åˆ›å»ºæµ‹è¯•æ–‡ä»¶\")\n    print(\"=\" * 60)\n    data_dir = \"data\"\n    os.makedirs(data_dir, exist_ok=True)\n    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶1\n    test_file1 = os.path.join(data_dir, \"test1.txt\")\n    with open(test_file1, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"\"\"ç¬¬ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£",
        "detail": "clear_and_test",
        "documentation": {}
    },
    {
        "label": "test_upload_and_check",
        "kind": 2,
        "importPath": "clear_and_test",
        "description": "clear_and_test",
        "peekOfCode": "def test_upload_and_check():\n    \"\"\"æµ‹è¯•ä¸Šä¼ å¹¶æ£€æŸ¥ç»“æžœ\"\"\"\n    print(\"\\nðŸ§ª æµ‹è¯•ä¸Šä¼ å¹¶æ£€æŸ¥ç»“æžœ\")\n    print(\"=\" * 60)\n    # è¿™é‡Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨ä¸Šä¼ æ–‡ä»¶ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨è„šæœ¬ä¸­\n    print(\"è¯·é€šè¿‡ä»¥ä¸‹æ­¥éª¤æµ‹è¯•:\")\n    print(\"1. å¯åŠ¨æœåŠ¡å™¨: uv fastapi run dev\")\n    print(\"2. ä¸Šä¼  data/test1.txt æ–‡ä»¶\")\n    print(\"3. ä¸Šä¼  data/test2.txt æ–‡ä»¶\")\n    print(\"4. è¿è¡Œ python test_storage_fixes.py æ£€æŸ¥ç»“æžœ\")",
        "detail": "clear_and_test",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "clear_and_test",
        "description": "clear_and_test",
        "peekOfCode": "def main():\n    \"\"\"ä¸»å‡½æ•°\"\"\"\n    print(\"ðŸ§ª æ¸…ç†æ•°æ®åº“å¹¶æµ‹è¯•ä¿®å¤\")\n    print(\"=\" * 80)\n    if clear_all_data():\n        create_test_files()\n        test_upload_and_check()\n    else:\n        print(\"\\nè¯·å…ˆæ¸…ç†æ•°æ®ï¼Œç„¶åŽé‡æ–°è¿è¡Œè„šæœ¬\")\nif __name__ == \"__main__\":",
        "detail": "clear_and_test",
        "documentation": {}
    },
    {
        "label": "debug_chunking_process",
        "kind": 2,
        "importPath": "debug_chunking",
        "description": "debug_chunking",
        "peekOfCode": "def debug_chunking_process():\n    \"\"\"è°ƒè¯•æ–‡æ¡£åˆ†å—è¿‡ç¨‹\"\"\"\n    print(\"ðŸ” è°ƒè¯•æ–‡æ¡£åˆ†å—è¿‡ç¨‹\")\n    print(\"=\" * 60)\n    # 1. è¯»å–åŽŸå§‹æ–‡æ¡£\n    data_dir = os.environ.get(\"DATA_DIR\", \"data\")\n    reader = SimpleDirectoryReader(data_dir, recursive=True)\n    documents = reader.load_data()\n    print(f\"ðŸ“„ è¯»å–åˆ° {len(documents)} ä¸ªæ–‡æ¡£:\")\n    for i, doc in enumerate(documents):",
        "detail": "debug_chunking",
        "documentation": {}
    },
    {
        "label": "check_database_vs_actual",
        "kind": 2,
        "importPath": "debug_chunking",
        "description": "debug_chunking",
        "peekOfCode": "def check_database_vs_actual():\n    \"\"\"å¯¹æ¯”æ•°æ®åº“ä¸­çš„æ•°æ®å’Œå®žé™…åˆ†å—ç»“æžœ\"\"\"\n    print(\"ðŸ” å¯¹æ¯”æ•°æ®åº“æ•°æ®å’Œå®žé™…åˆ†å—\")\n    print(\"=\" * 60)\n    # æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ•°æ®\n    with sqlite3.connect('storage/docstore.db') as conn:\n        cursor = conn.execute('''\n            SELECT file_name, COUNT(*) as db_chunks,\n                   COUNT(DISTINCT LENGTH(data)) as unique_lengths,\n                   MIN(LENGTH(data)) as min_length,",
        "detail": "debug_chunking",
        "documentation": {}
    },
    {
        "label": "analyze_chunk_index_logic",
        "kind": 2,
        "importPath": "debug_chunking",
        "description": "debug_chunking",
        "peekOfCode": "def analyze_chunk_index_logic():\n    \"\"\"åˆ†æž chunk_index é€»è¾‘\"\"\"\n    print(f\"\\nðŸ” åˆ†æž chunk_index é€»è¾‘é—®é¢˜\")\n    print(\"=\" * 60)\n    print(\"ðŸ“‹ generate.py ä¸­çš„é€»è¾‘:\")\n    print(\"1. è¯»å–æ‰€æœ‰æ–‡æ¡£\")\n    print(\"2. ä½¿ç”¨ SentenceSplitter åˆ†å—\")\n    print(\"3. ä¸ºæ¯ä¸ªæ–‡æ¡£æ‰¾åˆ°å¯¹åº”çš„èŠ‚ç‚¹: doc_nodes = [node for node in nodes if node.ref_doc_id == document.doc_id]\")\n    print(\"4. ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ†é… chunk_index: 'chunk_index': i\")\n    print(f\"\\nðŸ¤” å¯èƒ½çš„é—®é¢˜:\")",
        "detail": "debug_chunking",
        "documentation": {}
    },
    {
        "label": "suggest_solution",
        "kind": 2,
        "importPath": "debug_chunking",
        "description": "debug_chunking",
        "peekOfCode": "def suggest_solution():\n    \"\"\"å»ºè®®è§£å†³æ–¹æ¡ˆ\"\"\"\n    print(f\"\\nðŸ’¡ è§£å†³æ–¹æ¡ˆ\")\n    print(\"=\" * 60)\n    print(\"ðŸ”§ ç«‹å³è§£å†³:\")\n    print(\"1. é‡ç½®æ•°æ®åº“æ¸…é™¤é‡å¤æ•°æ®\")\n    print(\"2. æ£€æŸ¥æ–‡æ¡£åˆ†å—è®¾ç½®\")\n    print(\"3. ç¡®ä¿ generate åªè¿è¡Œä¸€æ¬¡\")\n    print(f\"\\nðŸ”§ é•¿æœŸæ”¹è¿›:\")\n    print(\"1. æ·»åŠ åŽ»é‡é€»è¾‘ï¼Œé¿å…é‡å¤å¤„ç†ç›¸åŒæ–‡æ¡£\")",
        "detail": "debug_chunking",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "debug_chunking",
        "description": "debug_chunking",
        "peekOfCode": "def main():\n    \"\"\"ä¸»è°ƒè¯•æµç¨‹\"\"\"\n    debug_chunking_process()\n    check_database_vs_actual()\n    analyze_chunk_index_logic()\n    suggest_solution()\nif __name__ == \"__main__\":\n    main()",
        "detail": "debug_chunking",
        "documentation": {}
    },
    {
        "label": "debug_chunk_index",
        "kind": 2,
        "importPath": "debug_chunk_index",
        "description": "debug_chunk_index",
        "peekOfCode": "def debug_chunk_index():\n    \"\"\"è°ƒè¯• chunk_index é—®é¢˜\"\"\"\n    print(\"ðŸ” è°ƒè¯• chunk_index é—®é¢˜\")\n    print(\"=\" * 60)\n    conn = sqlite3.connect('storage/docstore.db')\n    # èŽ·å–ä¸€ä¸ªæ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯\n    cursor = conn.execute(\"\"\"\n        SELECT doc_id, data, chunk_index, file_name \n        FROM documents \n        LIMIT 3",
        "detail": "debug_chunk_index",
        "documentation": {}
    },
    {
        "label": "check_node_creation",
        "kind": 2,
        "importPath": "debug_chunk_index",
        "description": "debug_chunk_index",
        "peekOfCode": "def check_node_creation():\n    \"\"\"æ£€æŸ¥èŠ‚ç‚¹åˆ›å»ºè¿‡ç¨‹\"\"\"\n    print(\"\\nðŸ”§ æ¨¡æ‹ŸèŠ‚ç‚¹åˆ›å»ºè¿‡ç¨‹\")\n    print(\"=\" * 60)\n    # æ¨¡æ‹Ÿ main.py ä¸­çš„é€»è¾‘\n    print(\"æ¨¡æ‹Ÿä»£ç :\")\n    print(\"\"\"\n    for chunk_index, node in enumerate(nodes):\n        node.metadata.update({\n            'file_id': file_id,",
        "detail": "debug_chunk_index",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "debug_chunk_index",
        "description": "debug_chunk_index",
        "peekOfCode": "def main():\n    debug_chunk_index()\n    check_node_creation()\nif __name__ == \"__main__\":\n    main()",
        "detail": "debug_chunk_index",
        "documentation": {}
    },
    {
        "label": "test_step_by_step",
        "kind": 2,
        "importPath": "debug_index_loading",
        "description": "debug_index_loading",
        "peekOfCode": "def test_step_by_step():\n    \"\"\"é€æ­¥æµ‹è¯•ç´¢å¼•åŠ è½½è¿‡ç¨‹\"\"\"\n    print(\"ðŸ” é€æ­¥è°ƒè¯•ç´¢å¼•åŠ è½½\")\n    print(\"=\" * 60)\n    # 1. åŠ è½½çŽ¯å¢ƒå˜é‡\n    print(\"1. åŠ è½½çŽ¯å¢ƒå˜é‡...\")\n    load_dotenv()\n    print(\"âœ… çŽ¯å¢ƒå˜é‡åŠ è½½å®Œæˆ\")\n    # 2. åˆå§‹åŒ–è®¾ç½®\n    print(\"\\n2. åˆå§‹åŒ–è®¾ç½®...\")",
        "detail": "debug_index_loading",
        "documentation": {}
    },
    {
        "label": "test_get_index_function",
        "kind": 2,
        "importPath": "debug_index_loading",
        "description": "debug_index_loading",
        "peekOfCode": "def test_get_index_function():\n    \"\"\"æµ‹è¯• get_index å‡½æ•°\"\"\"\n    print(\"\\nðŸŽ¯ æµ‹è¯• get_index å‡½æ•°\")\n    print(\"=\" * 60)\n    try:\n        from app.index import get_index\n        print(\"è°ƒç”¨ get_index()...\")\n        index = get_index()\n        if index:\n            print(\"âœ… get_index() æˆåŠŸ\")",
        "detail": "debug_index_loading",
        "documentation": {}
    },
    {
        "label": "test_workflow_creation",
        "kind": 2,
        "importPath": "debug_index_loading",
        "description": "debug_index_loading",
        "peekOfCode": "def test_workflow_creation():\n    \"\"\"æµ‹è¯•å·¥ä½œæµåˆ›å»º\"\"\"\n    print(\"\\nâš™ï¸ æµ‹è¯•å·¥ä½œæµåˆ›å»º\")\n    print(\"=\" * 60)\n    try:\n        from app.workflow import create_workflow\n        print(\"è°ƒç”¨ create_workflow()...\")\n        workflow = create_workflow()\n        if workflow:\n            print(\"âœ… å·¥ä½œæµåˆ›å»ºæˆåŠŸ\")",
        "detail": "debug_index_loading",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "debug_index_loading",
        "description": "debug_index_loading",
        "peekOfCode": "def main():\n    \"\"\"ä¸»å‡½æ•°\"\"\"\n    print(\"ðŸ”§ ç´¢å¼•åŠ è½½é—®é¢˜è°ƒè¯•\")\n    print(\"=\" * 80)\n    # é€æ­¥æµ‹è¯•\n    step_ok = test_step_by_step()\n    if step_ok:\n        print(\"\\n\" + \"=\" * 80)\n        # æµ‹è¯• get_index å‡½æ•°\n        index_ok = test_get_index_function()",
        "detail": "debug_index_loading",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "debug_index_loading",
        "description": "debug_index_loading",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef test_step_by_step():\n    \"\"\"é€æ­¥æµ‹è¯•ç´¢å¼•åŠ è½½è¿‡ç¨‹\"\"\"\n    print(\"ðŸ” é€æ­¥è°ƒè¯•ç´¢å¼•åŠ è½½\")\n    print(\"=\" * 60)\n    # 1. åŠ è½½çŽ¯å¢ƒå˜é‡\n    print(\"1. åŠ è½½çŽ¯å¢ƒå˜é‡...\")\n    load_dotenv()\n    print(\"âœ… çŽ¯å¢ƒå˜é‡åŠ è½½å®Œæˆ\")\n    # 2. åˆå§‹åŒ–è®¾ç½®",
        "detail": "debug_index_loading",
        "documentation": {}
    },
    {
        "label": "check_docstore_data",
        "kind": 2,
        "importPath": "diagnose_embedding_issue",
        "description": "diagnose_embedding_issue",
        "peekOfCode": "def check_docstore_data():\n    \"\"\"æ£€æŸ¥ docstore ä¸­çš„æ•°æ®\"\"\"\n    print(\"ðŸ“„ æ£€æŸ¥ Docstore æ•°æ®\")\n    print(\"=\" * 60)\n    if not os.path.exists('storage/docstore.db'):\n        print(\"âŒ docstore.db ä¸å­˜åœ¨\")\n        return []\n    conn = sqlite3.connect('storage/docstore.db')\n    # æ£€æŸ¥ documents è¡¨\n    cursor = conn.execute(\"\"\"",
        "detail": "diagnose_embedding_issue",
        "documentation": {}
    },
    {
        "label": "check_chroma_data",
        "kind": 2,
        "importPath": "diagnose_embedding_issue",
        "description": "diagnose_embedding_issue",
        "peekOfCode": "def check_chroma_data():\n    \"\"\"æ£€æŸ¥ ChromaDB ä¸­çš„æ•°æ®\"\"\"\n    print(\"\\nðŸ§  æ£€æŸ¥ ChromaDB æ•°æ®\")\n    print(\"=\" * 60)\n    try:\n        chroma_path = 'storage/chroma_db_new'\n        client = chromadb.PersistentClient(\n            path=chroma_path,\n            settings=ChromaSettings(\n                anonymized_telemetry=False,",
        "detail": "diagnose_embedding_issue",
        "documentation": {}
    },
    {
        "label": "check_chroma_sqlite",
        "kind": 2,
        "importPath": "diagnose_embedding_issue",
        "description": "diagnose_embedding_issue",
        "peekOfCode": "def check_chroma_sqlite():\n    \"\"\"æ£€æŸ¥ ChromaDB çš„ SQLite æ–‡ä»¶\"\"\"\n    print(\"\\nðŸ—„ï¸ æ£€æŸ¥ ChromaDB SQLite æ–‡ä»¶\")\n    print(\"=\" * 60)\n    chroma_db_path = \"storage/chroma_db_new\"\n    chroma_db_file = None\n    for file in os.listdir(chroma_db_path):\n        if file.endswith('.sqlite3'):\n            chroma_db_file = os.path.join(chroma_db_path, file)\n            break",
        "detail": "diagnose_embedding_issue",
        "documentation": {}
    },
    {
        "label": "compare_data_consistency",
        "kind": 2,
        "importPath": "diagnose_embedding_issue",
        "description": "diagnose_embedding_issue",
        "peekOfCode": "def compare_data_consistency():\n    \"\"\"æ¯”è¾ƒæ•°æ®ä¸€è‡´æ€§\"\"\"\n    print(\"\\nðŸ”— æ¯”è¾ƒæ•°æ®ä¸€è‡´æ€§\")\n    print(\"=\" * 60)\n    docstore_ids = check_docstore_data()\n    chroma_ids = check_chroma_data()\n    if docstore_ids and chroma_ids:\n        docstore_set = set(docstore_ids)\n        chroma_set = set(chroma_ids)\n        common = docstore_set & chroma_set",
        "detail": "diagnose_embedding_issue",
        "documentation": {}
    },
    {
        "label": "suggest_fixes",
        "kind": 2,
        "importPath": "diagnose_embedding_issue",
        "description": "diagnose_embedding_issue",
        "peekOfCode": "def suggest_fixes():\n    \"\"\"å»ºè®®ä¿®å¤æ–¹æ¡ˆ\"\"\"\n    print(\"\\nðŸ’¡ ä¿®å¤å»ºè®®\")\n    print(\"=\" * 60)\n    print(\"åŸºäºŽè¯Šæ–­ç»“æžœï¼Œå¯èƒ½çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ:\")\n    print()\n    print(\"1. **å‘é‡ç´¢å¼•åˆ›å»ºå¤±è´¥**\")\n    print(\"   - åŽŸå› : VectorStoreIndex åˆ›å»ºè¿‡ç¨‹ä¸­å‡ºçŽ°å¼‚å¸¸\")\n    print(\"   - è§£å†³: æ£€æŸ¥ OpenAI API è¿žæŽ¥å’Œé…é¢\")\n    print()",
        "detail": "diagnose_embedding_issue",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "diagnose_embedding_issue",
        "description": "diagnose_embedding_issue",
        "peekOfCode": "def main():\n    \"\"\"ä¸»å‡½æ•°\"\"\"\n    print(\"ðŸ” Embedding Metadata æ•°æ®ä¸å®Œæ•´è¯Šæ–­\")\n    print(\"=\" * 80)\n    check_docstore_data()\n    check_chroma_data()\n    check_chroma_sqlite()\n    compare_data_consistency()\n    suggest_fixes()\nif __name__ == \"__main__\":",
        "detail": "diagnose_embedding_issue",
        "documentation": {}
    },
    {
        "label": "backup_chroma_db",
        "kind": 2,
        "importPath": "fix_chromadb",
        "description": "fix_chromadb",
        "peekOfCode": "def backup_chroma_db(chroma_db_path):\n    \"\"\"å¤‡ä»½ ChromaDB ç›®å½•\"\"\"\n    if not os.path.exists(chroma_db_path):\n        logger.info(\"ChromaDB ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€å¤‡ä»½\")\n        return None\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    backup_path = f\"{chroma_db_path}_backup_{timestamp}\"\n    try:\n        shutil.copytree(chroma_db_path, backup_path)\n        logger.info(f\"âœ… å·²å¤‡ä»½ ChromaDB åˆ°: {backup_path}\")",
        "detail": "fix_chromadb",
        "documentation": {}
    },
    {
        "label": "completely_remove_chromadb",
        "kind": 2,
        "importPath": "fix_chromadb",
        "description": "fix_chromadb",
        "peekOfCode": "def completely_remove_chromadb(chroma_db_path):\n    \"\"\"å®Œå…¨åˆ é™¤ ChromaDB ç›®å½•\"\"\"\n    if not os.path.exists(chroma_db_path):\n        logger.info(\"ChromaDB ç›®å½•ä¸å­˜åœ¨\")\n        return True\n    try:\n        shutil.rmtree(chroma_db_path)\n        logger.info(f\"âœ… å·²å®Œå…¨åˆ é™¤ ChromaDB ç›®å½•: {chroma_db_path}\")\n        return True\n    except Exception as e:",
        "detail": "fix_chromadb",
        "documentation": {}
    },
    {
        "label": "create_fresh_chromadb",
        "kind": 2,
        "importPath": "fix_chromadb",
        "description": "fix_chromadb",
        "peekOfCode": "def create_fresh_chromadb(chroma_db_path):\n    \"\"\"åˆ›å»ºå…¨æ–°çš„ ChromaDB\"\"\"\n    try:\n        import chromadb\n        from chromadb.config import Settings as ChromaSettings\n        logger.info(\"ðŸ”§ åˆ›å»ºå…¨æ–°çš„ ChromaDB...\")\n        # ç¡®ä¿ç›®å½•å­˜åœ¨\n        os.makedirs(chroma_db_path, exist_ok=True)\n        # åˆ›å»ºæ–°çš„ ChromaDB å®¢æˆ·ç«¯\n        chroma_client = chromadb.PersistentClient(",
        "detail": "fix_chromadb",
        "documentation": {}
    },
    {
        "label": "verify_chromadb",
        "kind": 2,
        "importPath": "fix_chromadb",
        "description": "fix_chromadb",
        "peekOfCode": "def verify_chromadb(chroma_db_path):\n    \"\"\"éªŒè¯ ChromaDB æ˜¯å¦æ­£å¸¸å·¥ä½œ\"\"\"\n    try:\n        import chromadb\n        from chromadb.config import Settings as ChromaSettings\n        logger.info(\"ðŸ” éªŒè¯ ChromaDB...\")\n        # è¿žæŽ¥åˆ° ChromaDB\n        chroma_client = chromadb.PersistentClient(\n            path=chroma_db_path,\n            settings=ChromaSettings(",
        "detail": "fix_chromadb",
        "documentation": {}
    },
    {
        "label": "test_storage_context",
        "kind": 2,
        "importPath": "fix_chromadb",
        "description": "fix_chromadb",
        "peekOfCode": "def test_storage_context():\n    \"\"\"æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»º\"\"\"\n    try:\n        from app.storage_config import get_storage_context\n        logger.info(\"ðŸ”§ æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»º...\")\n        storage_context = get_storage_context(\"storage\")\n        logger.info(\"âœ… å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ\")\n        return True\n    except Exception as e:\n        logger.error(f\"âŒ å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»ºå¤±è´¥: {e}\")",
        "detail": "fix_chromadb",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "fix_chromadb",
        "description": "fix_chromadb",
        "peekOfCode": "def main():\n    \"\"\"ä¸»ä¿®å¤æµç¨‹\"\"\"\n    storage_dir = \"storage\"\n    chroma_db_path = os.path.join(storage_dir, \"chroma_db\")\n    print(\"ðŸ”§ å¼€å§‹ä¿®å¤ ChromaDB\")\n    print(\"=\" * 50)\n    # 1. å¤‡ä»½çŽ°æœ‰çš„ ChromaDB\n    backup_path = backup_chroma_db(chroma_db_path)\n    # 2. å®Œå…¨åˆ é™¤çŽ°æœ‰çš„ ChromaDB\n    if not completely_remove_chromadb(chroma_db_path):",
        "detail": "fix_chromadb",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "fix_chromadb",
        "description": "fix_chromadb",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef backup_chroma_db(chroma_db_path):\n    \"\"\"å¤‡ä»½ ChromaDB ç›®å½•\"\"\"\n    if not os.path.exists(chroma_db_path):\n        logger.info(\"ChromaDB ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€å¤‡ä»½\")\n        return None\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    backup_path = f\"{chroma_db_path}_backup_{timestamp}\"\n    try:\n        shutil.copytree(chroma_db_path, backup_path)",
        "detail": "fix_chromadb",
        "documentation": {}
    },
    {
        "label": "analyze_current_state",
        "kind": 2,
        "importPath": "fix_chromadb_id_mapping",
        "description": "fix_chromadb_id_mapping",
        "peekOfCode": "def analyze_current_state():\n    \"\"\"åˆ†æžå½“å‰çŠ¶æ€\"\"\"\n    print(\"ðŸ” åˆ†æžå½“å‰ ChromaDB ID æ˜ å°„çŠ¶æ€\")\n    print(\"=\" * 60)\n    # 1. æ£€æŸ¥ DocStore æ•°æ®\n    docstore_path = 'storage/docstore.db'\n    docstore_nodes = {}\n    if os.path.exists(docstore_path):\n        conn = sqlite3.connect(docstore_path)\n        cursor = conn.execute(\"SELECT doc_id, data FROM documents\")",
        "detail": "fix_chromadb_id_mapping",
        "documentation": {}
    },
    {
        "label": "fix_chromadb_metadata",
        "kind": 2,
        "importPath": "fix_chromadb_id_mapping",
        "description": "fix_chromadb_id_mapping",
        "peekOfCode": "def fix_chromadb_metadata():\n    \"\"\"ä¿®å¤ ChromaDB å…ƒæ•°æ®\"\"\"\n    print(\"\\nðŸ”§ å¼€å§‹ä¿®å¤ ChromaDB ID æ˜ å°„\")\n    print(\"=\" * 60)\n    # 1. åˆ†æžå½“å‰çŠ¶æ€\n    docstore_nodes, chroma_nodes = analyze_current_state()\n    if not docstore_nodes or not chroma_nodes:\n        print(\"âŒ æ— æ³•èŽ·å–å¿…è¦çš„æ•°æ®ï¼Œä¿®å¤å¤±è´¥\")\n        return False\n    # 2. å»ºç«‹æ˜ å°„å…³ç³»",
        "detail": "fix_chromadb_id_mapping",
        "documentation": {}
    },
    {
        "label": "verify_fix",
        "kind": 2,
        "importPath": "fix_chromadb_id_mapping",
        "description": "fix_chromadb_id_mapping",
        "peekOfCode": "def verify_fix():\n    \"\"\"éªŒè¯ä¿®å¤ç»“æžœ\"\"\"\n    print(\"\\nðŸ” éªŒè¯ä¿®å¤ç»“æžœ\")\n    print(\"=\" * 60)\n    chroma_path = 'storage/chroma_db_new/chroma.sqlite3'\n    if not os.path.exists(chroma_path):\n        print(\"âŒ ChromaDB æ–‡ä»¶ä¸å­˜åœ¨\")\n        return False\n    conn = sqlite3.connect(chroma_path)\n    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ç›¸åŒçš„ document_id, doc_id, ref_doc_id",
        "detail": "fix_chromadb_id_mapping",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "fix_chromadb_id_mapping",
        "description": "fix_chromadb_id_mapping",
        "peekOfCode": "def main():\n    \"\"\"ä¸»å‡½æ•°\"\"\"\n    print(\"ðŸš€ ChromaDB ID æ˜ å°„ä¿®å¤å·¥å…·\")\n    print(\"=\" * 60)\n    # 1. åˆ†æžå½“å‰çŠ¶æ€\n    analyze_current_state()\n    # 2. æ‰§è¡Œä¿®å¤\n    if fix_chromadb_metadata():\n        # 3. éªŒè¯ä¿®å¤ç»“æžœ\n        verify_fix()",
        "detail": "fix_chromadb_id_mapping",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "fix_chromadb_id_mapping",
        "description": "fix_chromadb_id_mapping",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef analyze_current_state():\n    \"\"\"åˆ†æžå½“å‰çŠ¶æ€\"\"\"\n    print(\"ðŸ” åˆ†æžå½“å‰ ChromaDB ID æ˜ å°„çŠ¶æ€\")\n    print(\"=\" * 60)\n    # 1. æ£€æŸ¥ DocStore æ•°æ®\n    docstore_path = 'storage/docstore.db'\n    docstore_nodes = {}\n    if os.path.exists(docstore_path):\n        conn = sqlite3.connect(docstore_path)",
        "detail": "fix_chromadb_id_mapping",
        "documentation": {}
    },
    {
        "label": "fix_chromadb_migrations",
        "kind": 2,
        "importPath": "fix_chromadb_migrations",
        "description": "fix_chromadb_migrations",
        "peekOfCode": "def fix_chromadb_migrations():\n    \"\"\"ä¿®å¤ ChromaDB è¿ç§»çŠ¶æ€\"\"\"\n    chroma_sqlite_path = os.path.join(\"storage\", \"chroma_db\", \"chroma.sqlite3\")\n    if not os.path.exists(chroma_sqlite_path):\n        logger.info(\"chroma.sqlite3 ä¸å­˜åœ¨\")\n        return True\n    try:\n        with sqlite3.connect(chroma_sqlite_path) as conn:\n            logger.info(\"ðŸ”§ ä¿®å¤ ChromaDB è¿ç§»çŠ¶æ€...\")\n            # æ£€æŸ¥ migrations è¡¨",
        "detail": "fix_chromadb_migrations",
        "documentation": {}
    },
    {
        "label": "test_chromadb_after_fix",
        "kind": 2,
        "importPath": "fix_chromadb_migrations",
        "description": "fix_chromadb_migrations",
        "peekOfCode": "def test_chromadb_after_fix():\n    \"\"\"ä¿®å¤åŽæµ‹è¯• ChromaDB\"\"\"\n    try:\n        import chromadb\n        from chromadb.config import Settings as ChromaSettings\n        logger.info(\"ðŸ” æµ‹è¯•ä¿®å¤åŽçš„ ChromaDB...\")\n        chroma_db_path = os.path.join(\"storage\", \"chroma_db\")\n        # åˆ›å»ºå®¢æˆ·ç«¯\n        chroma_client = chromadb.PersistentClient(\n            path=chroma_db_path,",
        "detail": "fix_chromadb_migrations",
        "documentation": {}
    },
    {
        "label": "test_full_workflow",
        "kind": 2,
        "importPath": "fix_chromadb_migrations",
        "description": "fix_chromadb_migrations",
        "peekOfCode": "def test_full_workflow():\n    \"\"\"æµ‹è¯•å®Œæ•´å·¥ä½œæµ\"\"\"\n    try:\n        from app.storage_config import get_storage_context\n        logger.info(\"ðŸ”§ æµ‹è¯•å®Œæ•´å·¥ä½œæµ...\")\n        # æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡\n        storage_context = get_storage_context(\"storage\")\n        logger.info(\"âœ… å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ\")\n        return True\n    except Exception as e:",
        "detail": "fix_chromadb_migrations",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "fix_chromadb_migrations",
        "description": "fix_chromadb_migrations",
        "peekOfCode": "def main():\n    \"\"\"ä¸»ä¿®å¤æµç¨‹\"\"\"\n    print(\"ðŸ”§ å¼€å§‹ä¿®å¤ ChromaDB è¿ç§»çŠ¶æ€\")\n    print(\"=\" * 50)\n    # 1. ä¿®å¤ ChromaDB è¿ç§»çŠ¶æ€\n    if not fix_chromadb_migrations():\n        print(\"âŒ ä¿®å¤å¤±è´¥ï¼šæ— æ³•ä¿®å¤ ChromaDB è¿ç§»çŠ¶æ€\")\n        return\n    # 2. æµ‹è¯•ä¿®å¤åŽçš„ ChromaDB\n    if not test_chromadb_after_fix():",
        "detail": "fix_chromadb_migrations",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "fix_chromadb_migrations",
        "description": "fix_chromadb_migrations",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef fix_chromadb_migrations():\n    \"\"\"ä¿®å¤ ChromaDB è¿ç§»çŠ¶æ€\"\"\"\n    chroma_sqlite_path = os.path.join(\"storage\", \"chroma_db\", \"chroma.sqlite3\")\n    if not os.path.exists(chroma_sqlite_path):\n        logger.info(\"chroma.sqlite3 ä¸å­˜åœ¨\")\n        return True\n    try:\n        with sqlite3.connect(chroma_sqlite_path) as conn:\n            logger.info(\"ðŸ”§ ä¿®å¤ ChromaDB è¿ç§»çŠ¶æ€...\")",
        "detail": "fix_chromadb_migrations",
        "documentation": {}
    },
    {
        "label": "fix_chroma_fulltext_search",
        "kind": 2,
        "importPath": "fix_chroma_fulltext",
        "description": "fix_chroma_fulltext",
        "peekOfCode": "def fix_chroma_fulltext_search():\n    \"\"\"ä¿®å¤ ChromaDB çš„ embedding_fulltext_search è¡¨ç»“æž„\"\"\"\n    print(\"ðŸ”§ ä¿®å¤ ChromaDB embedding_fulltext_search è¡¨ç»“æž„\")\n    print(\"=\" * 60)\n    chroma_db_path = \"storage/chroma_db_new\"\n    if not os.path.exists(chroma_db_path):\n        print(\"âŒ ChromaDB ç›®å½•ä¸å­˜åœ¨\")\n        return False\n    # æŸ¥æ‰¾ ChromaDB çš„ SQLite æ•°æ®åº“æ–‡ä»¶\n    chroma_db_file = None",
        "detail": "fix_chroma_fulltext",
        "documentation": {}
    },
    {
        "label": "test_chroma_after_fix",
        "kind": 2,
        "importPath": "fix_chroma_fulltext",
        "description": "fix_chroma_fulltext",
        "peekOfCode": "def test_chroma_after_fix():\n    \"\"\"æµ‹è¯•ä¿®å¤åŽçš„ ChromaDB\"\"\"\n    print(\"\\nðŸ§ª æµ‹è¯•ä¿®å¤åŽçš„ ChromaDB\")\n    print(\"=\" * 60)\n    try:\n        import chromadb\n        from chromadb.config import Settings as ChromaSettings\n        chroma_path = 'storage/chroma_db_new'\n        client = chromadb.PersistentClient(\n            path=chroma_path,",
        "detail": "fix_chroma_fulltext",
        "documentation": {}
    },
    {
        "label": "clean_duplicate_data",
        "kind": 2,
        "importPath": "fix_chroma_fulltext",
        "description": "fix_chroma_fulltext",
        "peekOfCode": "def clean_duplicate_data():\n    \"\"\"æ¸…ç†é‡å¤æ•°æ®\"\"\"\n    print(\"\\nðŸ§¹ æ¸…ç†é‡å¤æ•°æ®\")\n    print(\"=\" * 60)\n    try:\n        import sqlite3\n        # æ¸…ç† docstore ä¸­çš„é‡å¤æ•°æ®\n        docstore_path = \"storage/docstore.db\"\n        if os.path.exists(docstore_path):\n            conn = sqlite3.connect(docstore_path)",
        "detail": "fix_chroma_fulltext",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "fix_chroma_fulltext",
        "description": "fix_chroma_fulltext",
        "peekOfCode": "def main():\n    \"\"\"ä¸»å‡½æ•°\"\"\"\n    print(\"ðŸ”§ ChromaDB embedding_fulltext_search ä¿®å¤å·¥å…·\")\n    print(\"=\" * 80)\n    # 1. ä¿®å¤ embedding_fulltext_search è¡¨\n    if fix_chroma_fulltext_search():\n        print(\"\\nâœ… embedding_fulltext_search è¡¨ä¿®å¤æˆåŠŸ\")\n        # 2. æµ‹è¯•ä¿®å¤ç»“æžœ\n        if test_chroma_after_fix():\n            print(\"\\nâœ… ChromaDB ä¿®å¤éªŒè¯é€šè¿‡\")",
        "detail": "fix_chroma_fulltext",
        "documentation": {}
    },
    {
        "label": "force_cleanup_old_chroma",
        "kind": 2,
        "importPath": "force_cleanup",
        "description": "force_cleanup",
        "peekOfCode": "def force_cleanup_old_chroma():\n    \"\"\"å¼ºåˆ¶æ¸…ç†æ—§çš„ chroma_db ç›®å½•\"\"\"\n    storage_dir = \"storage\"\n    old_chroma_path = os.path.join(storage_dir, \"chroma_db\")\n    if not os.path.exists(old_chroma_path):\n        print(\"âœ… æ—§çš„ chroma_db ç›®å½•å·²ç»ä¸å­˜åœ¨\")\n        return True\n    print(\"ðŸ”§ å°è¯•å¼ºåˆ¶æ¸…ç†æ—§çš„ chroma_db ç›®å½•...\")\n    # æ–¹æ³•1: å°è¯•é‡å‘½åç›®å½•\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")",
        "detail": "force_cleanup",
        "documentation": {}
    },
    {
        "label": "check_final_state",
        "kind": 2,
        "importPath": "force_cleanup",
        "description": "force_cleanup",
        "peekOfCode": "def check_final_state():\n    \"\"\"æ£€æŸ¥æœ€ç»ˆçš„å­˜å‚¨çŠ¶æ€\"\"\"\n    print(\"\\nðŸ” æ£€æŸ¥æœ€ç»ˆå­˜å‚¨çŠ¶æ€\")\n    print(\"=\" * 50)\n    storage_dir = \"storage\"\n    if not os.path.exists(storage_dir):\n        print(\"âŒ å­˜å‚¨ç›®å½•ä¸å­˜åœ¨\")\n        return\n    print(\"ðŸ“ å½“å‰å­˜å‚¨ç›®å½•å†…å®¹:\")\n    for item in sorted(os.listdir(storage_dir)):",
        "detail": "force_cleanup",
        "documentation": {}
    },
    {
        "label": "format_size",
        "kind": 2,
        "importPath": "force_cleanup",
        "description": "force_cleanup",
        "peekOfCode": "def format_size(size_bytes):\n    \"\"\"æ ¼å¼åŒ–æ–‡ä»¶å¤§å°\"\"\"\n    if size_bytes == 0:\n        return \"0 B\"\n    for unit in ['B', 'KB', 'MB', 'GB']:\n        if size_bytes < 1024.0:\n            return f\"{size_bytes:.1f} {unit}\"\n        size_bytes /= 1024.0\n    return f\"{size_bytes:.1f} TB\"\ndef main():",
        "detail": "force_cleanup",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "force_cleanup",
        "description": "force_cleanup",
        "peekOfCode": "def main():\n    \"\"\"ä¸»æ¸…ç†æµç¨‹\"\"\"\n    print(\"ðŸ§¹ å¼ºåˆ¶æ¸…ç†å·¥å…·\")\n    print(\"=\" * 50)\n    # å¼ºåˆ¶æ¸…ç†æ—§çš„ ChromaDB\n    success = force_cleanup_old_chroma()\n    if success:\n        print(\"âœ… æ¸…ç†å®Œæˆ\")\n    else:\n        print(\"âš ï¸  éƒ¨åˆ†æ¸…ç†å®Œæˆï¼Œå¯èƒ½æœ‰æ–‡ä»¶ä»è¢«å ç”¨\")",
        "detail": "force_cleanup",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "force_cleanup",
        "description": "force_cleanup",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef force_cleanup_old_chroma():\n    \"\"\"å¼ºåˆ¶æ¸…ç†æ—§çš„ chroma_db ç›®å½•\"\"\"\n    storage_dir = \"storage\"\n    old_chroma_path = os.path.join(storage_dir, \"chroma_db\")\n    if not os.path.exists(old_chroma_path):\n        print(\"âœ… æ—§çš„ chroma_db ç›®å½•å·²ç»ä¸å­˜åœ¨\")\n        return True\n    print(\"ðŸ”§ å°è¯•å¼ºåˆ¶æ¸…ç†æ—§çš„ chroma_db ç›®å½•...\")\n    # æ–¹æ³•1: å°è¯•é‡å‘½åç›®å½•",
        "detail": "force_cleanup",
        "documentation": {}
    },
    {
        "label": "force_move_chromadb",
        "kind": 2,
        "importPath": "force_fix_chromadb",
        "description": "force_fix_chromadb",
        "peekOfCode": "def force_move_chromadb(chroma_db_path):\n    \"\"\"å¼ºåˆ¶ç§»åŠ¨ ChromaDB ç›®å½•ï¼ˆç»•è¿‡æ–‡ä»¶å ç”¨ï¼‰\"\"\"\n    if not os.path.exists(chroma_db_path):\n        logger.info(\"ChromaDB ç›®å½•ä¸å­˜åœ¨\")\n        return True\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    temp_path = f\"{chroma_db_path}_old_{timestamp}\"\n    try:\n        # å°è¯•é‡å‘½åç›®å½•\n        os.rename(chroma_db_path, temp_path)",
        "detail": "force_fix_chromadb",
        "documentation": {}
    },
    {
        "label": "create_fresh_chromadb_simple",
        "kind": 2,
        "importPath": "force_fix_chromadb",
        "description": "force_fix_chromadb",
        "peekOfCode": "def create_fresh_chromadb_simple(chroma_db_path):\n    \"\"\"åˆ›å»ºå…¨æ–°çš„ ChromaDBï¼ˆç®€åŒ–ç‰ˆï¼‰\"\"\"\n    try:\n        logger.info(\"ðŸ”§ åˆ›å»ºå…¨æ–°çš„ ChromaDB...\")\n        # ç¡®ä¿ç›®å½•å­˜åœ¨\n        os.makedirs(chroma_db_path, exist_ok=True)\n        # åˆ›å»ºä¸€ä¸ªç®€å•çš„ SQLite æ•°æ®åº“æ–‡ä»¶\n        chroma_sqlite_path = os.path.join(chroma_db_path, \"chroma.sqlite3\")\n        # åˆ›å»ºåŸºæœ¬çš„æ•°æ®åº“ç»“æž„\n        with sqlite3.connect(chroma_sqlite_path) as conn:",
        "detail": "force_fix_chromadb",
        "documentation": {}
    },
    {
        "label": "test_chromadb_with_retry",
        "kind": 2,
        "importPath": "force_fix_chromadb",
        "description": "force_fix_chromadb",
        "peekOfCode": "def test_chromadb_with_retry(chroma_db_path):\n    \"\"\"æµ‹è¯• ChromaDB è¿žæŽ¥ï¼ˆå¸¦é‡è¯•ï¼‰\"\"\"\n    try:\n        import chromadb\n        from chromadb.config import Settings as ChromaSettings\n        logger.info(\"ðŸ” æµ‹è¯• ChromaDB è¿žæŽ¥...\")\n        # åˆ›å»º ChromaDB å®¢æˆ·ç«¯\n        chroma_client = chromadb.PersistentClient(\n            path=chroma_db_path,\n            settings=ChromaSettings(",
        "detail": "force_fix_chromadb",
        "documentation": {}
    },
    {
        "label": "test_full_storage_context",
        "kind": 2,
        "importPath": "force_fix_chromadb",
        "description": "force_fix_chromadb",
        "peekOfCode": "def test_full_storage_context():\n    \"\"\"æµ‹è¯•å®Œæ•´çš„å­˜å‚¨ä¸Šä¸‹æ–‡\"\"\"\n    try:\n        from app.storage_config import get_storage_context\n        logger.info(\"ðŸ”§ æµ‹è¯•å®Œæ•´å­˜å‚¨ä¸Šä¸‹æ–‡...\")\n        storage_context = get_storage_context(\"storage\")\n        logger.info(\"âœ… å®Œæ•´å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ\")\n        return True\n    except Exception as e:\n        logger.error(f\"âŒ å®Œæ•´å­˜å‚¨ä¸Šä¸‹æ–‡æµ‹è¯•å¤±è´¥: {e}\")",
        "detail": "force_fix_chromadb",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "force_fix_chromadb",
        "description": "force_fix_chromadb",
        "peekOfCode": "def main():\n    \"\"\"ä¸»ä¿®å¤æµç¨‹\"\"\"\n    storage_dir = \"storage\"\n    chroma_db_path = os.path.join(storage_dir, \"chroma_db\")\n    print(\"ðŸ”§ å¼€å§‹å¼ºåˆ¶ä¿®å¤ ChromaDB\")\n    print(\"=\" * 50)\n    # 1. å¼ºåˆ¶ç§»åŠ¨çŽ°æœ‰çš„ ChromaDB ç›®å½•\n    if not force_move_chromadb(chroma_db_path):\n        print(\"âŒ ä¿®å¤å¤±è´¥ï¼šæ— æ³•ç§»åŠ¨çŽ°æœ‰ ChromaDB\")\n        return",
        "detail": "force_fix_chromadb",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "force_fix_chromadb",
        "description": "force_fix_chromadb",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef force_move_chromadb(chroma_db_path):\n    \"\"\"å¼ºåˆ¶ç§»åŠ¨ ChromaDB ç›®å½•ï¼ˆç»•è¿‡æ–‡ä»¶å ç”¨ï¼‰\"\"\"\n    if not os.path.exists(chroma_db_path):\n        logger.info(\"ChromaDB ç›®å½•ä¸å­˜åœ¨\")\n        return True\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    temp_path = f\"{chroma_db_path}_old_{timestamp}\"\n    try:\n        # å°è¯•é‡å‘½åç›®å½•",
        "detail": "force_fix_chromadb",
        "documentation": {}
    },
    {
        "label": "force_remove_file",
        "kind": 2,
        "importPath": "force_reset_database",
        "description": "force_reset_database",
        "peekOfCode": "def force_remove_file(file_path, max_attempts=5):\n    \"\"\"å¼ºåˆ¶åˆ é™¤æ–‡ä»¶ï¼Œå¤„ç†è¢«å ç”¨çš„æƒ…å†µ\"\"\"\n    for attempt in range(max_attempts):\n        try:\n            if os.path.isfile(file_path):\n                os.remove(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n            logger.info(f\"âœ… å·²åˆ é™¤: {file_path}\")\n            return True",
        "detail": "force_reset_database",
        "documentation": {}
    },
    {
        "label": "force_reset_storage",
        "kind": 2,
        "importPath": "force_reset_database",
        "description": "force_reset_database",
        "peekOfCode": "def force_reset_storage(storage_dir=\"storage\"):\n    \"\"\"å¼ºåˆ¶é‡ç½®å­˜å‚¨ç›®å½•\"\"\"\n    logger.info(\"ðŸ”„ å¼€å§‹å¼ºåˆ¶é‡ç½®å­˜å‚¨...\")\n    if not os.path.exists(storage_dir):\n        logger.info(f\"å­˜å‚¨ç›®å½• {storage_dir} ä¸å­˜åœ¨\")\n        return True\n    # é€ä¸ªåˆ é™¤æ–‡ä»¶\n    files_to_remove = []\n    for root, dirs, files in os.walk(storage_dir):\n        for file in files:",
        "detail": "force_reset_database",
        "documentation": {}
    },
    {
        "label": "create_clean_storage_context",
        "kind": 2,
        "importPath": "force_reset_database",
        "description": "force_reset_database",
        "peekOfCode": "def create_clean_storage_context():\n    \"\"\"åˆ›å»ºå¹²å‡€çš„å­˜å‚¨ä¸Šä¸‹æ–‡\"\"\"\n    try:\n        from app.storage_config import get_storage_context\n        logger.info(\"ðŸ”§ åˆ›å»ºå¹²å‡€çš„å­˜å‚¨ä¸Šä¸‹æ–‡...\")\n        storage_context = get_storage_context(\"storage\")\n        # éªŒè¯åˆ›å»ºç»“æžœ\n        storage_files = [\n            \"storage/docstore.db\",\n            \"storage/index_store.db\",",
        "detail": "force_reset_database",
        "documentation": {}
    },
    {
        "label": "verify_clean_state",
        "kind": 2,
        "importPath": "force_reset_database",
        "description": "force_reset_database",
        "peekOfCode": "def verify_clean_state():\n    \"\"\"éªŒè¯æ¸…ç†çŠ¶æ€\"\"\"\n    logger.info(\"ðŸ” éªŒè¯æ¸…ç†çŠ¶æ€...\")\n    # æ£€æŸ¥ä¸åº”è¯¥å­˜åœ¨çš„æ–‡ä»¶\n    unwanted_files = [\n        \"storage/graph_store.json\",\n        \"storage/image__vector_store.json\"\n    ]\n    clean = True\n    for file_path in unwanted_files:",
        "detail": "force_reset_database",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "force_reset_database",
        "description": "force_reset_database",
        "peekOfCode": "def main():\n    \"\"\"ä¸»é‡ç½®æµç¨‹\"\"\"\n    print(\"ðŸ”„ å¼ºåˆ¶æ•°æ®åº“é‡ç½®\")\n    print(\"=\" * 40)\n    # 1. å¼ºåˆ¶åˆ é™¤å­˜å‚¨ç›®å½•\n    if not force_reset_storage(\"storage\"):\n        print(\"âŒ å¼ºåˆ¶é‡ç½®å¤±è´¥\")\n        return\n    # 2. åˆ›å»ºå¹²å‡€çš„å­˜å‚¨\n    if not create_clean_storage_context():",
        "detail": "force_reset_database",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "force_reset_database",
        "description": "force_reset_database",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef force_remove_file(file_path, max_attempts=5):\n    \"\"\"å¼ºåˆ¶åˆ é™¤æ–‡ä»¶ï¼Œå¤„ç†è¢«å ç”¨çš„æƒ…å†µ\"\"\"\n    for attempt in range(max_attempts):\n        try:\n            if os.path.isfile(file_path):\n                os.remove(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n            logger.info(f\"âœ… å·²åˆ é™¤: {file_path}\")",
        "detail": "force_reset_database",
        "documentation": {}
    },
    {
        "label": "clear_all_storage_data",
        "kind": 2,
        "importPath": "generate",
        "description": "generate",
        "peekOfCode": "def clear_all_storage_data(storage_context):\n    \"\"\"æ¸…ç†æ‰€æœ‰å­˜å‚¨æ•°æ®\"\"\"\n    logger.info(\"ðŸ§¹ æ¸…ç†æ‰€æœ‰å­˜å‚¨æ•°æ®...\")\n    try:\n        # 1. æ¸…ç† docstore æ•°æ®\n        import sqlite3\n        docstore_path = storage_context.docstore.db_path\n        with sqlite3.connect(docstore_path) as conn:\n            conn.execute(\"DELETE FROM documents\")\n            conn.execute(\"DELETE FROM files\") ",
        "detail": "generate",
        "documentation": {}
    },
    {
        "label": "generate_index",
        "kind": 2,
        "importPath": "generate",
        "description": "generate",
        "peekOfCode": "def generate_index():\n    \"\"\"\n    é‡æ–°ç”Ÿæˆç´¢å¼•ï¼šæ¸…ç†æ‰€æœ‰æ•°æ®ï¼Œç„¶åŽé‡æ–°ç´¢å¼•æ–‡æ¡£\n    \"\"\"\n    from app.index import STORAGE_DIR\n    from app.settings import init_settings\n    from app.storage_config import get_storage_context\n    from llama_index.core.indices import VectorStoreIndex\n    from llama_index.core.readers import SimpleDirectoryReader\n    from llama_index.core.node_parser import SentenceSplitter",
        "detail": "generate",
        "documentation": {}
    },
    {
        "label": "generate_ui_for_workflow",
        "kind": 2,
        "importPath": "generate",
        "description": "generate",
        "peekOfCode": "def generate_ui_for_workflow():\n    \"\"\"\n    Generate UI for UIEventData event in app/workflow.py\n    \"\"\"\n    import asyncio\n    from main import COMPONENT_DIR\n    # To generate UI components for additional event types,\n    # import the corresponding data model (e.g., MyCustomEventData)\n    # and run the generate_ui_for_workflow function with the imported model.\n    # Make sure the output filename of the generated UI component matches the event type (here `ui_event`)",
        "detail": "generate",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "generate",
        "description": "generate",
        "peekOfCode": "logger = logging.getLogger()\ndef clear_all_storage_data(storage_context):\n    \"\"\"æ¸…ç†æ‰€æœ‰å­˜å‚¨æ•°æ®\"\"\"\n    logger.info(\"ðŸ§¹ æ¸…ç†æ‰€æœ‰å­˜å‚¨æ•°æ®...\")\n    try:\n        # 1. æ¸…ç† docstore æ•°æ®\n        import sqlite3\n        docstore_path = storage_context.docstore.db_path\n        with sqlite3.connect(docstore_path) as conn:\n            conn.execute(\"DELETE FROM documents\")",
        "detail": "generate",
        "documentation": {}
    },
    {
        "label": "generate_index_improved",
        "kind": 2,
        "importPath": "generate_improved",
        "description": "generate_improved",
        "peekOfCode": "def generate_index_improved():\n    \"\"\"\n    æ”¹è¿›çš„ç´¢å¼•ç”Ÿæˆå‡½æ•°ï¼Œè§£å†³ chunk_index å’Œé‡å¤æ•°æ®é—®é¢˜\n    \"\"\"\n    from app.index import STORAGE_DIR\n    from app.settings import init_settings\n    from app.storage_config import get_storage_context\n    from llama_index.core.indices import VectorStoreIndex\n    from llama_index.core.readers import SimpleDirectoryReader\n    from llama_index.core.node_parser import SentenceSplitter",
        "detail": "generate_improved",
        "documentation": {}
    },
    {
        "label": "verify_chunk_index_in_db",
        "kind": 2,
        "importPath": "generate_improved",
        "description": "generate_improved",
        "peekOfCode": "def verify_chunk_index_in_db():\n    \"\"\"éªŒè¯æ•°æ®åº“ä¸­çš„ chunk_index\"\"\"\n    import sqlite3\n    try:\n        with sqlite3.connect('storage/docstore.db') as conn:\n            cursor = conn.execute('''\n                SELECT file_name, chunk_index, COUNT(*) as count\n                FROM documents \n                GROUP BY file_name, chunk_index \n                ORDER BY file_name, chunk_index",
        "detail": "generate_improved",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "generate_improved",
        "description": "generate_improved",
        "peekOfCode": "logger = logging.getLogger()\ndef generate_index_improved():\n    \"\"\"\n    æ”¹è¿›çš„ç´¢å¼•ç”Ÿæˆå‡½æ•°ï¼Œè§£å†³ chunk_index å’Œé‡å¤æ•°æ®é—®é¢˜\n    \"\"\"\n    from app.index import STORAGE_DIR\n    from app.settings import init_settings\n    from app.storage_config import get_storage_context\n    from llama_index.core.indices import VectorStoreIndex\n    from llama_index.core.readers import SimpleDirectoryReader",
        "detail": "generate_improved",
        "documentation": {}
    },
    {
        "label": "create_app",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def create_app():\n    app = LlamaIndexServer(\n        workflow_factory=create_workflow,  # A factory function that creates a new workflow for each request\n        ui_config=UIConfig(\n            enabled=False,  # ç¦ç”¨é»˜è®¤UI\n        ),\n        logger=logger,\n        env=\"dev\",\n    )\n    # æ·»åŠ è‡ªå®šä¹‰ä¸­é—´ä»¶æ¥å¤„ç†sourcesæ³¨è§£",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "logger = logging.getLogger(\"uvicorn\")\n# A path to a directory where the customized UI code is stored\nCOMPONENT_DIR = \"components\"\ndef create_app():\n    app = LlamaIndexServer(\n        workflow_factory=create_workflow,  # A factory function that creates a new workflow for each request\n        ui_config=UIConfig(\n            enabled=False,  # ç¦ç”¨é»˜è®¤UI\n        ),\n        logger=logger,",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "COMPONENT_DIR",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "COMPONENT_DIR = \"components\"\ndef create_app():\n    app = LlamaIndexServer(\n        workflow_factory=create_workflow,  # A factory function that creates a new workflow for each request\n        ui_config=UIConfig(\n            enabled=False,  # ç¦ç”¨é»˜è®¤UI\n        ),\n        logger=logger,\n        env=\"dev\",\n    )",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "app = create_app()",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "calculate_file_hash",
        "kind": 2,
        "importPath": "migrate_to_file_management",
        "description": "migrate_to_file_management",
        "peekOfCode": "def calculate_file_hash(file_path: str) -> str:\n    \"\"\"è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼\"\"\"\n    hash_md5 = hashlib.md5()\n    try:\n        with open(file_path, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                hash_md5.update(chunk)\n        return hash_md5.hexdigest()\n    except Exception as e:\n        logger.warning(f\"Failed to calculate hash for {file_path}: {e}\")",
        "detail": "migrate_to_file_management",
        "documentation": {}
    },
    {
        "label": "migrate_database",
        "kind": 2,
        "importPath": "migrate_to_file_management",
        "description": "migrate_to_file_management",
        "peekOfCode": "def migrate_database():\n    \"\"\"æ‰§è¡Œæ•°æ®åº“è¿ç§»\"\"\"\n    db_path = \"storage/docstore.db\"\n    data_dir = \"data\"\n    if not os.path.exists(db_path):\n        logger.error(f\"Database file not found: {db_path}\")\n        return False\n    if not os.path.exists(data_dir):\n        logger.error(f\"Data directory not found: {data_dir}\")\n        return False",
        "detail": "migrate_to_file_management",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "migrate_to_file_management",
        "description": "migrate_to_file_management",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef calculate_file_hash(file_path: str) -> str:\n    \"\"\"è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼\"\"\"\n    hash_md5 = hashlib.md5()\n    try:\n        with open(file_path, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                hash_md5.update(chunk)\n        return hash_md5.hexdigest()\n    except Exception as e:",
        "detail": "migrate_to_file_management",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "quick_check",
        "description": "quick_check",
        "peekOfCode": "def main():\n    print(\"=== DOCSTORE æ•°æ® ===\")\n    conn = sqlite3.connect('storage/docstore.db')\n    cursor = conn.execute('SELECT doc_id, file_name, chunk_index FROM documents ORDER BY file_name, chunk_index')\n    docs = cursor.fetchall()\n    print(f'Documents è¡¨è®°å½•æ•°: {len(docs)}')\n    for doc_id, file_name, chunk_index in docs:\n        print(f'  {doc_id[:8]}... | {file_name} | chunk_{chunk_index}')\n    conn.close()\n    print('\\n=== CHROMADB æ•°æ® ===')",
        "detail": "quick_check",
        "documentation": {}
    },
    {
        "label": "reset_chromadb_database",
        "kind": 2,
        "importPath": "reinit_chromadb",
        "description": "reinit_chromadb",
        "peekOfCode": "def reset_chromadb_database():\n    \"\"\"é‡ç½® ChromaDB æ•°æ®åº“å†…å®¹\"\"\"\n    chroma_sqlite_path = os.path.join(\"storage\", \"chroma_db\", \"chroma.sqlite3\")\n    if not os.path.exists(chroma_sqlite_path):\n        logger.info(\"chroma.sqlite3 ä¸å­˜åœ¨\")\n        return True\n    try:\n        with sqlite3.connect(chroma_sqlite_path) as conn:\n            logger.info(\"ðŸ”§ é‡ç½® ChromaDB æ•°æ®åº“...\")\n            # åˆ é™¤æ‰€æœ‰æ•°æ®ä½†ä¿ç•™è¡¨ç»“æž„",
        "detail": "reinit_chromadb",
        "documentation": {}
    },
    {
        "label": "test_chromadb_connection",
        "kind": 2,
        "importPath": "reinit_chromadb",
        "description": "reinit_chromadb",
        "peekOfCode": "def test_chromadb_connection():\n    \"\"\"æµ‹è¯• ChromaDB è¿žæŽ¥\"\"\"\n    try:\n        import chromadb\n        from chromadb.config import Settings as ChromaSettings\n        logger.info(\"ðŸ” æµ‹è¯• ChromaDB è¿žæŽ¥...\")\n        chroma_db_path = os.path.join(\"storage\", \"chroma_db\")\n        # åˆ›å»ºå®¢æˆ·ç«¯\n        chroma_client = chromadb.PersistentClient(\n            path=chroma_db_path,",
        "detail": "reinit_chromadb",
        "documentation": {}
    },
    {
        "label": "test_storage_context",
        "kind": 2,
        "importPath": "reinit_chromadb",
        "description": "reinit_chromadb",
        "peekOfCode": "def test_storage_context():\n    \"\"\"æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡\"\"\"\n    try:\n        from app.storage_config import get_storage_context\n        logger.info(\"ðŸ”§ æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡...\")\n        storage_context = get_storage_context(\"storage\")\n        logger.info(\"âœ… å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ\")\n        return True\n    except Exception as e:\n        logger.error(f\"âŒ å­˜å‚¨ä¸Šä¸‹æ–‡æµ‹è¯•å¤±è´¥: {e}\")",
        "detail": "reinit_chromadb",
        "documentation": {}
    },
    {
        "label": "test_generate_command",
        "kind": 2,
        "importPath": "reinit_chromadb",
        "description": "reinit_chromadb",
        "peekOfCode": "def test_generate_command():\n    \"\"\"æµ‹è¯• generate å‘½ä»¤çš„æ ¸å¿ƒé€»è¾‘\"\"\"\n    try:\n        from app.index import STORAGE_DIR\n        from app.settings import init_settings\n        from app.storage_config import get_storage_context\n        from llama_index.core.readers import SimpleDirectoryReader\n        from dotenv import load_dotenv\n        logger.info(\"ðŸ”§ æµ‹è¯• generate å‘½ä»¤æ ¸å¿ƒé€»è¾‘...\")\n        load_dotenv()",
        "detail": "reinit_chromadb",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "reinit_chromadb",
        "description": "reinit_chromadb",
        "peekOfCode": "def main():\n    \"\"\"ä¸»ä¿®å¤æµç¨‹\"\"\"\n    print(\"ðŸ”§ å¼€å§‹é‡æ–°åˆå§‹åŒ– ChromaDB\")\n    print(\"=\" * 50)\n    # 1. é‡ç½® ChromaDB æ•°æ®åº“\n    if not reset_chromadb_database():\n        print(\"âŒ ä¿®å¤å¤±è´¥ï¼šæ— æ³•é‡ç½® ChromaDB æ•°æ®åº“\")\n        return\n    # 2. æµ‹è¯• ChromaDB è¿žæŽ¥\n    if not test_chromadb_connection():",
        "detail": "reinit_chromadb",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reinit_chromadb",
        "description": "reinit_chromadb",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef reset_chromadb_database():\n    \"\"\"é‡ç½® ChromaDB æ•°æ®åº“å†…å®¹\"\"\"\n    chroma_sqlite_path = os.path.join(\"storage\", \"chroma_db\", \"chroma.sqlite3\")\n    if not os.path.exists(chroma_sqlite_path):\n        logger.info(\"chroma.sqlite3 ä¸å­˜åœ¨\")\n        return True\n    try:\n        with sqlite3.connect(chroma_sqlite_path) as conn:\n            logger.info(\"ðŸ”§ é‡ç½® ChromaDB æ•°æ®åº“...\")",
        "detail": "reinit_chromadb",
        "documentation": {}
    },
    {
        "label": "backup_storage",
        "kind": 2,
        "importPath": "reset_database",
        "description": "reset_database",
        "peekOfCode": "def backup_storage(storage_dir=\"storage\"):\n    \"\"\"å¤‡ä»½å½“å‰å­˜å‚¨ç›®å½•\"\"\"\n    if not os.path.exists(storage_dir):\n        logger.info(f\"å­˜å‚¨ç›®å½• {storage_dir} ä¸å­˜åœ¨ï¼Œæ— éœ€å¤‡ä»½\")\n        return None\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    backup_dir = f\"{storage_dir}_backup_{timestamp}\"\n    try:\n        shutil.copytree(storage_dir, backup_dir)\n        logger.info(f\"âœ… å·²å¤‡ä»½å­˜å‚¨ç›®å½•åˆ°: {backup_dir}\")",
        "detail": "reset_database",
        "documentation": {}
    },
    {
        "label": "analyze_current_storage",
        "kind": 2,
        "importPath": "reset_database",
        "description": "reset_database",
        "peekOfCode": "def analyze_current_storage(storage_dir=\"storage\"):\n    \"\"\"åˆ†æžå½“å‰å­˜å‚¨çŠ¶æ€\"\"\"\n    logger.info(\"ðŸ“Š åˆ†æžå½“å‰å­˜å‚¨çŠ¶æ€...\")\n    if not os.path.exists(storage_dir):\n        logger.info(\"å­˜å‚¨ç›®å½•ä¸å­˜åœ¨\")\n        return\n    # ç»Ÿè®¡æ–‡ä»¶å¤§å°\n    total_size = 0\n    file_count = 0\n    for root, dirs, files in os.walk(storage_dir):",
        "detail": "reset_database",
        "documentation": {}
    },
    {
        "label": "remove_storage_directory",
        "kind": 2,
        "importPath": "reset_database",
        "description": "reset_database",
        "peekOfCode": "def remove_storage_directory(storage_dir=\"storage\"):\n    \"\"\"åˆ é™¤å­˜å‚¨ç›®å½•\"\"\"\n    if not os.path.exists(storage_dir):\n        logger.info(f\"å­˜å‚¨ç›®å½• {storage_dir} ä¸å­˜åœ¨\")\n        return True\n    try:\n        shutil.rmtree(storage_dir)\n        logger.info(f\"ðŸ—‘ï¸  å·²åˆ é™¤å­˜å‚¨ç›®å½•: {storage_dir}\")\n        return True\n    except Exception as e:",
        "detail": "reset_database",
        "documentation": {}
    },
    {
        "label": "create_clean_storage",
        "kind": 2,
        "importPath": "reset_database",
        "description": "reset_database",
        "peekOfCode": "def create_clean_storage(storage_dir=\"storage\"):\n    \"\"\"åˆ›å»ºå¹²å‡€çš„å­˜å‚¨ç›®å½•ç»“æž„\"\"\"\n    try:\n        os.makedirs(storage_dir, exist_ok=True)\n        logger.info(f\"ðŸ“ å·²åˆ›å»ºå¹²å‡€çš„å­˜å‚¨ç›®å½•: {storage_dir}\")\n        return True\n    except Exception as e:\n        logger.error(f\"âŒ åˆ›å»ºå­˜å‚¨ç›®å½•å¤±è´¥: {e}\")\n        return False\ndef initialize_clean_databases(storage_dir=\"storage\"):",
        "detail": "reset_database",
        "documentation": {}
    },
    {
        "label": "initialize_clean_databases",
        "kind": 2,
        "importPath": "reset_database",
        "description": "reset_database",
        "peekOfCode": "def initialize_clean_databases(storage_dir=\"storage\"):\n    \"\"\"åˆå§‹åŒ–å¹²å‡€çš„æ•°æ®åº“\"\"\"\n    try:\n        from app.storage_config import get_storage_context\n        logger.info(\"ðŸ”§ åˆå§‹åŒ–å¹²å‡€çš„å­˜å‚¨ä¸Šä¸‹æ–‡...\")\n        storage_context = get_storage_context(storage_dir)\n        # éªŒè¯æ•°æ®åº“æ˜¯å¦æ­£ç¡®åˆ›å»º\n        docstore_path = os.path.join(storage_dir, \"docstore.db\")\n        index_store_path = os.path.join(storage_dir, \"index_store.db\")\n        chroma_path = os.path.join(storage_dir, \"chroma_db\")",
        "detail": "reset_database",
        "documentation": {}
    },
    {
        "label": "verify_reset",
        "kind": 2,
        "importPath": "reset_database",
        "description": "reset_database",
        "peekOfCode": "def verify_reset(storage_dir=\"storage\"):\n    \"\"\"éªŒè¯é‡ç½®ç»“æžœ\"\"\"\n    logger.info(\"ðŸ” éªŒè¯é‡ç½®ç»“æžœ...\")\n    if not os.path.exists(storage_dir):\n        logger.error(\"âŒ å­˜å‚¨ç›®å½•ä¸å­˜åœ¨\")\n        return False\n    # æ£€æŸ¥å¿…éœ€çš„æ–‡ä»¶\n    required_files = [\n        \"docstore.db\",\n        \"index_store.db\",",
        "detail": "reset_database",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "reset_database",
        "description": "reset_database",
        "peekOfCode": "def main():\n    \"\"\"ä¸»é‡ç½®æµç¨‹\"\"\"\n    storage_dir = \"storage\"\n    print(\"ðŸ”„ å¼€å§‹æ•°æ®åº“é‡ç½®æµç¨‹\")\n    print(\"=\" * 50)\n    # 1. åˆ†æžå½“å‰çŠ¶æ€\n    analyze_current_storage(storage_dir)\n    print()\n    # 2. ç¡®è®¤é‡ç½®\n    response = input(\"âš ï¸  ç¡®å®šè¦é‡ç½®æ•°æ®åº“å—ï¼Ÿè¿™å°†åˆ é™¤æ‰€æœ‰çŽ°æœ‰æ•°æ®ï¼(y/N): \")",
        "detail": "reset_database",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reset_database",
        "description": "reset_database",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef backup_storage(storage_dir=\"storage\"):\n    \"\"\"å¤‡ä»½å½“å‰å­˜å‚¨ç›®å½•\"\"\"\n    if not os.path.exists(storage_dir):\n        logger.info(f\"å­˜å‚¨ç›®å½• {storage_dir} ä¸å­˜åœ¨ï¼Œæ— éœ€å¤‡ä»½\")\n        return None\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    backup_dir = f\"{storage_dir}_backup_{timestamp}\"\n    try:\n        shutil.copytree(storage_dir, backup_dir)",
        "detail": "reset_database",
        "documentation": {}
    },
    {
        "label": "clear_chroma_sqlite",
        "kind": 2,
        "importPath": "reset_data_complete",
        "description": "reset_data_complete",
        "peekOfCode": "def clear_chroma_sqlite(chroma_db_path):\n    \"\"\"ç›´æŽ¥æ¸…ç† chroma.sqlite3 æ•°æ®åº“\"\"\"\n    chroma_sqlite_path = os.path.join(chroma_db_path, \"chroma.sqlite3\")\n    if not os.path.exists(chroma_sqlite_path):\n        logger.info(\"chroma.sqlite3 ä¸å­˜åœ¨ï¼Œè·³è¿‡æ¸…ç†\")\n        return True\n    try:\n        with sqlite3.connect(chroma_sqlite_path) as conn:\n            # èŽ·å–æ‰€æœ‰è¡¨å\n            cursor = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")",
        "detail": "reset_data_complete",
        "documentation": {}
    },
    {
        "label": "clear_chroma_vector_files",
        "kind": 2,
        "importPath": "reset_data_complete",
        "description": "reset_data_complete",
        "peekOfCode": "def clear_chroma_vector_files(chroma_db_path):\n    \"\"\"æ¸…ç† ChromaDB å‘é‡æ–‡ä»¶\"\"\"\n    try:\n        deleted_dirs = []\n        for item in os.listdir(chroma_db_path):\n            item_path = os.path.join(chroma_db_path, item)\n            if os.path.isdir(item_path) and item != \"__pycache__\":\n                try:\n                    shutil.rmtree(item_path)\n                    deleted_dirs.append(item)",
        "detail": "reset_data_complete",
        "documentation": {}
    },
    {
        "label": "clear_document_data_complete",
        "kind": 2,
        "importPath": "reset_data_complete",
        "description": "reset_data_complete",
        "peekOfCode": "def clear_document_data_complete(storage_dir=\"storage\"):\n    \"\"\"å®Œæ•´æ¸…ç©ºæ–‡æ¡£æ•°æ®ï¼ŒåŒ…æ‹¬ç›´æŽ¥æ“ä½œ chroma.sqlite3\"\"\"\n    logger.info(\"ðŸ”„ å¼€å§‹å®Œæ•´æ¸…ç©ºæ–‡æ¡£æ•°æ®...\")\n    # 1. æ¸…ç©ºSQLiteæ•°æ®åº“ä¸­çš„æ–‡æ¡£æ•°æ®\n    docstore_path = os.path.join(storage_dir, \"docstore.db\")\n    if os.path.exists(docstore_path):\n        try:\n            with sqlite3.connect(docstore_path) as conn:\n                cursor = conn.execute(\"DELETE FROM documents\")\n                docs_deleted = cursor.rowcount",
        "detail": "reset_data_complete",
        "documentation": {}
    },
    {
        "label": "verify_complete_reset",
        "kind": 2,
        "importPath": "reset_data_complete",
        "description": "reset_data_complete",
        "peekOfCode": "def verify_complete_reset(storage_dir=\"storage\"):\n    \"\"\"éªŒè¯å®Œæ•´é‡ç½®ç»“æžœ\"\"\"\n    logger.info(\"ðŸ” éªŒè¯å®Œæ•´é‡ç½®ç»“æžœ...\")\n    # æ£€æŸ¥docstore.db\n    docstore_path = os.path.join(storage_dir, \"docstore.db\")\n    if os.path.exists(docstore_path):\n        with sqlite3.connect(docstore_path) as conn:\n            cursor = conn.execute(\"SELECT COUNT(*) FROM documents\")\n            doc_count = cursor.fetchone()[0]\n            cursor = conn.execute(\"SELECT COUNT(*) FROM files\")",
        "detail": "reset_data_complete",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "reset_data_complete",
        "description": "reset_data_complete",
        "peekOfCode": "def main():\n    \"\"\"ä¸»é‡ç½®æµç¨‹\"\"\"\n    storage_dir = \"storage\"\n    print(\"ðŸ”„ å¼€å§‹å®Œæ•´æ•°æ®é‡ç½®ï¼ˆåŒ…æ‹¬ chroma.sqlite3ï¼‰\")\n    print(\"=\" * 60)\n    # 1. å®Œæ•´æ¸…ç©ºæ–‡æ¡£æ•°æ®\n    if not clear_document_data_complete(storage_dir):\n        print(\"âŒ å®Œæ•´æ•°æ®é‡ç½®å¤±è´¥\")\n        return\n    # 2. éªŒè¯é‡ç½®ç»“æžœ",
        "detail": "reset_data_complete",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reset_data_complete",
        "description": "reset_data_complete",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef clear_chroma_sqlite(chroma_db_path):\n    \"\"\"ç›´æŽ¥æ¸…ç† chroma.sqlite3 æ•°æ®åº“\"\"\"\n    chroma_sqlite_path = os.path.join(chroma_db_path, \"chroma.sqlite3\")\n    if not os.path.exists(chroma_sqlite_path):\n        logger.info(\"chroma.sqlite3 ä¸å­˜åœ¨ï¼Œè·³è¿‡æ¸…ç†\")\n        return True\n    try:\n        with sqlite3.connect(chroma_sqlite_path) as conn:\n            # èŽ·å–æ‰€æœ‰è¡¨å",
        "detail": "reset_data_complete",
        "documentation": {}
    },
    {
        "label": "clear_document_data",
        "kind": 2,
        "importPath": "reset_data_only",
        "description": "reset_data_only",
        "peekOfCode": "def clear_document_data(storage_dir=\"storage\"):\n    \"\"\"æ¸…ç©ºæ–‡æ¡£æ•°æ®ï¼Œä¿ç•™é…ç½®å’Œè¡¨ç»“æž„\"\"\"\n    logger.info(\"ðŸ”„ å¼€å§‹æ¸…ç©ºæ–‡æ¡£æ•°æ®...\")\n    # 1. æ¸…ç©ºSQLiteæ•°æ®åº“ä¸­çš„æ–‡æ¡£æ•°æ®\n    docstore_path = os.path.join(storage_dir, \"docstore.db\")\n    if os.path.exists(docstore_path):\n        try:\n            with sqlite3.connect(docstore_path) as conn:\n                # æ¸…ç©ºæ–‡æ¡£ç›¸å…³è¡¨çš„æ•°æ®ï¼Œä½†ä¿ç•™è¡¨ç»“æž„\n                cursor = conn.execute(\"DELETE FROM documents\")",
        "detail": "reset_data_only",
        "documentation": {}
    },
    {
        "label": "verify_data_reset",
        "kind": 2,
        "importPath": "reset_data_only",
        "description": "reset_data_only",
        "peekOfCode": "def verify_data_reset(storage_dir=\"storage\"):\n    \"\"\"éªŒè¯æ•°æ®é‡ç½®ç»“æžœ\"\"\"\n    logger.info(\"ðŸ” éªŒè¯æ•°æ®é‡ç½®ç»“æžœ...\")\n    # æ£€æŸ¥docstore.db\n    docstore_path = os.path.join(storage_dir, \"docstore.db\")\n    if os.path.exists(docstore_path):\n        with sqlite3.connect(docstore_path) as conn:\n            cursor = conn.execute(\"SELECT COUNT(*) FROM documents\")\n            doc_count = cursor.fetchone()[0]\n            cursor = conn.execute(\"SELECT COUNT(*) FROM files\")",
        "detail": "reset_data_only",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "reset_data_only",
        "description": "reset_data_only",
        "peekOfCode": "def main():\n    \"\"\"ä¸»é‡ç½®æµç¨‹\"\"\"\n    storage_dir = \"storage\"\n    print(\"ðŸ”„ å¼€å§‹æ•°æ®é‡ç½®ï¼ˆä¿ç•™é…ç½®ï¼‰\")\n    print(\"=\" * 50)\n    # 1. æ¸…ç©ºæ–‡æ¡£æ•°æ®\n    if not clear_document_data(storage_dir):\n        print(\"âŒ æ•°æ®é‡ç½®å¤±è´¥\")\n        return\n    # 2. éªŒè¯é‡ç½®ç»“æžœ",
        "detail": "reset_data_only",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reset_data_only",
        "description": "reset_data_only",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef clear_document_data(storage_dir=\"storage\"):\n    \"\"\"æ¸…ç©ºæ–‡æ¡£æ•°æ®ï¼Œä¿ç•™é…ç½®å’Œè¡¨ç»“æž„\"\"\"\n    logger.info(\"ðŸ”„ å¼€å§‹æ¸…ç©ºæ–‡æ¡£æ•°æ®...\")\n    # 1. æ¸…ç©ºSQLiteæ•°æ®åº“ä¸­çš„æ–‡æ¡£æ•°æ®\n    docstore_path = os.path.join(storage_dir, \"docstore.db\")\n    if os.path.exists(docstore_path):\n        try:\n            with sqlite3.connect(docstore_path) as conn:\n                # æ¸…ç©ºæ–‡æ¡£ç›¸å…³è¡¨çš„æ•°æ®ï¼Œä½†ä¿ç•™è¡¨ç»“æž„",
        "detail": "reset_data_only",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "simple_fix_chromadb",
        "description": "simple_fix_chromadb",
        "peekOfCode": "def main():\n    print(\"ðŸš€ å¼€å§‹ä¿®å¤ ChromaDB ID æ˜ å°„é—®é¢˜\")\n    # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n    docstore_path = 'storage/docstore.db'\n    chroma_path = 'storage/chroma_db_new/chroma.sqlite3'\n    if not os.path.exists(docstore_path):\n        print(f\"âŒ DocStore æ–‡ä»¶ä¸å­˜åœ¨: {docstore_path}\")\n        return\n    if not os.path.exists(chroma_path):\n        print(f\"âŒ ChromaDB æ–‡ä»¶ä¸å­˜åœ¨: {chroma_path}\")",
        "detail": "simple_fix_chromadb",
        "documentation": {}
    },
    {
        "label": "simple_reset",
        "kind": 2,
        "importPath": "simple_reset",
        "description": "simple_reset",
        "peekOfCode": "def simple_reset():\n    \"\"\"ç®€å•é‡ç½®æ•°æ®åº“\"\"\"\n    storage_dir = \"storage\"\n    print(\"ðŸ”„ å¼€å§‹ç®€å•æ•°æ®åº“é‡ç½®\")\n    print(\"=\" * 40)\n    # 1. å°è¯•åˆ é™¤å­˜å‚¨ç›®å½•\n    if os.path.exists(storage_dir):\n        try:\n            shutil.rmtree(storage_dir)\n            logger.info(f\"âœ… å·²åˆ é™¤å­˜å‚¨ç›®å½•: {storage_dir}\")",
        "detail": "simple_reset",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "simple_reset",
        "description": "simple_reset",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef simple_reset():\n    \"\"\"ç®€å•é‡ç½®æ•°æ®åº“\"\"\"\n    storage_dir = \"storage\"\n    print(\"ðŸ”„ å¼€å§‹ç®€å•æ•°æ®åº“é‡ç½®\")\n    print(\"=\" * 40)\n    # 1. å°è¯•åˆ é™¤å­˜å‚¨ç›®å½•\n    if os.path.exists(storage_dir):\n        try:\n            shutil.rmtree(storage_dir)",
        "detail": "simple_reset",
        "documentation": {}
    },
    {
        "label": "test_chat_api",
        "kind": 2,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "def test_chat_api():\n    \"\"\"æµ‹è¯•chat APIå¹¶æ£€æŸ¥å“åº”æ ¼å¼\"\"\"\n    url = \"http://localhost:8000/api/chat\"\n    payload = {\n        \"id\": \"test-123\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"å‘ç¥¨ä¸¢å¤±äº†æ€Žä¹ˆåŠžï¼Ÿ\"\n            }",
        "detail": "test_api",
        "documentation": {}
    },
    {
        "label": "test_health_api",
        "kind": 2,
        "importPath": "test_api",
        "description": "test_api",
        "peekOfCode": "def test_health_api():\n    \"\"\"æµ‹è¯•å¥åº·æ£€æŸ¥API\"\"\"\n    url = \"http://localhost:8000/api/health\"\n    try:\n        response = requests.get(url, timeout=10)\n        print(f\"å¥åº·æ£€æŸ¥çŠ¶æ€ç : {response.status_code}\")\n        print(f\"å¥åº·æ£€æŸ¥å“åº”: {response.json()}\")\n        return response.status_code == 200\n    except Exception as e:\n        print(f\"å¥åº·æ£€æŸ¥å¤±è´¥: {e}\")",
        "detail": "test_api",
        "documentation": {}
    },
    {
        "label": "test_health",
        "kind": 2,
        "importPath": "test_api_simple",
        "description": "test_api_simple",
        "peekOfCode": "def test_health():\n    \"\"\"æµ‹è¯•å¥åº·æ£€æŸ¥\"\"\"\n    try:\n        response = requests.get('http://localhost:8000/api/health', timeout=5)\n        print(f\"å¥åº·æ£€æŸ¥: {response.status_code} - {response.text}\")\n        return response.status_code == 200\n    except Exception as e:\n        print(f\"å¥åº·æ£€æŸ¥å¤±è´¥: {e}\")\n        return False\ndef test_chat():",
        "detail": "test_api_simple",
        "documentation": {}
    },
    {
        "label": "test_chat",
        "kind": 2,
        "importPath": "test_api_simple",
        "description": "test_api_simple",
        "peekOfCode": "def test_chat():\n    \"\"\"æµ‹è¯•èŠå¤© API\"\"\"\n    url = 'http://localhost:8000/api/chat'\n    payload = {\n        \"id\": \"test-session\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"AIæ™ºèƒ½åŠ©æ‰‹æœ‰ä»€ä¹ˆåŠŸèƒ½ï¼Ÿ\"\n            }",
        "detail": "test_api_simple",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "test_api_simple",
        "description": "test_api_simple",
        "peekOfCode": "def main():\n    print(\"ðŸ§ª ç®€å• API æµ‹è¯•\")\n    print(\"=\" * 50)\n    # æµ‹è¯•å¥åº·æ£€æŸ¥\n    if test_health():\n        print(\"\\nâœ… å¥åº·æ£€æŸ¥é€šè¿‡ï¼Œæµ‹è¯•èŠå¤© API...\")\n        test_chat()\n    else:\n        print(\"\\nâŒ å¥åº·æ£€æŸ¥å¤±è´¥ï¼ŒæœåŠ¡å™¨å¯èƒ½æœªè¿è¡Œ\")\nif __name__ == \"__main__\":",
        "detail": "test_api_simple",
        "documentation": {}
    },
    {
        "label": "test_node_creation",
        "kind": 2,
        "importPath": "test_chunk_index_fix",
        "description": "test_chunk_index_fix",
        "peekOfCode": "def test_node_creation():\n    \"\"\"æµ‹è¯•èŠ‚ç‚¹åˆ›å»ºè¿‡ç¨‹\"\"\"\n    print(\"ðŸ§ª æµ‹è¯•èŠ‚ç‚¹åˆ›å»ºè¿‡ç¨‹\")\n    print(\"=\" * 60)\n    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶\n    test_content = \"\"\"æµ‹è¯•æ–‡æ¡£\nè¿™æ˜¯ç¬¬ä¸€æ®µå†…å®¹ï¼Œç”¨äºŽæµ‹è¯•æ–‡æ¡£åˆ†å—åŠŸèƒ½ã€‚\nè¿™æ˜¯ç¬¬äºŒæ®µå†…å®¹ï¼Œåº”è¯¥è¢«åˆ†æˆä¸åŒçš„å—ã€‚\nè¿™æ˜¯ç¬¬ä¸‰æ®µå†…å®¹ï¼Œæ¯ä¸ªå—éƒ½åº”è¯¥æœ‰æ­£ç¡®çš„ç´¢å¼•ã€‚\"\"\"\n    test_file = \"test_chunk_index.txt\"",
        "detail": "test_chunk_index_fix",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "test_chunk_index_fix",
        "description": "test_chunk_index_fix",
        "peekOfCode": "def main():\n    \"\"\"ä¸»å‡½æ•°\"\"\"\n    print(\"ðŸ§ª Chunk Index ä¿®å¤æµ‹è¯•\")\n    print(\"=\" * 80)\n    test_node_creation()\n    print(\"\\nðŸ’¡ æµ‹è¯•ç»“è®º:\")\n    print(\"å¦‚æžœåºåˆ—åŒ–åŽçš„ chunk_index æ­£ç¡®ï¼Œä½†æ•°æ®åº“ä¸­ä»ç„¶æ˜¯0ï¼Œ\")\n    print(\"é‚£ä¹ˆé—®é¢˜å¯èƒ½åœ¨äºŽ:\")\n    print(\"1. æ•°æ®åº“ä¸­çš„æ•°æ®æ˜¯æ—§çš„\")\n    print(\"2. add_documents æ–¹æ³•ä¸­çš„é€»è¾‘æœ‰é—®é¢˜\")",
        "detail": "test_chunk_index_fix",
        "documentation": {}
    },
    {
        "label": "create_test_files_with_duplicate_content",
        "kind": 2,
        "importPath": "test_duplicate_content",
        "description": "test_duplicate_content",
        "peekOfCode": "def create_test_files_with_duplicate_content():\n    \"\"\"åˆ›å»ºåŒ…å«é‡å¤å†…å®¹çš„æµ‹è¯•æ–‡ä»¶\"\"\"\n    test_dir = \"test_duplicate_data\"\n    os.makedirs(test_dir, exist_ok=True)\n    # åˆ›å»ºä¸¤ä¸ªæ–‡ä»¶ï¼ŒåŒ…å«ç›¸åŒçš„æ–‡æœ¬å—\n    duplicate_content = \"\"\"\nè¿™æ˜¯ä¸€æ®µé‡å¤çš„æ–‡æœ¬å†…å®¹ã€‚\nè¿™æ®µå†…å®¹ä¼šå‡ºçŽ°åœ¨å¤šä¸ªæ–‡ä»¶ä¸­ã€‚\nç”¨äºŽæµ‹è¯•ç³»ç»Ÿå¦‚ä½•å¤„ç†é‡å¤çš„æ–‡æœ¬å—ã€‚\n\"\"\"",
        "detail": "test_duplicate_content",
        "documentation": {}
    },
    {
        "label": "analyze_node_generation",
        "kind": 2,
        "importPath": "test_duplicate_content",
        "description": "test_duplicate_content",
        "peekOfCode": "def analyze_node_generation():\n    \"\"\"åˆ†æžèŠ‚ç‚¹ç”Ÿæˆè¿‡ç¨‹\"\"\"\n    logger.info(\"ðŸ” åˆ†æžèŠ‚ç‚¹ç”Ÿæˆè¿‡ç¨‹...\")\n    test_dir = create_test_files_with_duplicate_content()\n    try:\n        # è¯»å–æ–‡æ¡£\n        reader = SimpleDirectoryReader(test_dir)\n        documents = reader.load_data()\n        logger.info(f\"ðŸ“„ è¯»å–äº† {len(documents)} ä¸ªæ–‡æ¡£\")\n        # è§£æžä¸ºèŠ‚ç‚¹",
        "detail": "test_duplicate_content",
        "documentation": {}
    },
    {
        "label": "test_storage_behavior",
        "kind": 2,
        "importPath": "test_duplicate_content",
        "description": "test_duplicate_content",
        "peekOfCode": "def test_storage_behavior():\n    \"\"\"æµ‹è¯•å­˜å‚¨è¡Œä¸º\"\"\"\n    logger.info(\"ðŸ” æµ‹è¯•å­˜å‚¨è¡Œä¸º...\")\n    # åˆ›å»ºæµ‹è¯•å­˜å‚¨\n    test_storage_dir = \"test_storage\"\n    storage_context = get_storage_context(test_storage_dir)\n    try:\n        # åˆ›å»ºä¸¤ä¸ªå†…å®¹ç›¸åŒä½†IDä¸åŒçš„èŠ‚ç‚¹\n        node1 = TextNode(\n            text=\"è¿™æ˜¯é‡å¤çš„æµ‹è¯•å†…å®¹\",",
        "detail": "test_duplicate_content",
        "documentation": {}
    },
    {
        "label": "analyze_vector_store_behavior",
        "kind": 2,
        "importPath": "test_duplicate_content",
        "description": "test_duplicate_content",
        "peekOfCode": "def analyze_vector_store_behavior():\n    \"\"\"åˆ†æžå‘é‡å­˜å‚¨è¡Œä¸º\"\"\"\n    logger.info(\"ðŸ” åˆ†æžå‘é‡å­˜å‚¨è¡Œä¸º...\")\n    # å¯¹äºŽç›¸åŒå†…å®¹çš„æ–‡æœ¬å—ï¼š\n    # 1. LlamaIndexä¼šä¸ºæ¯ä¸ªèŠ‚ç‚¹ç”Ÿæˆå”¯ä¸€çš„node_id\n    # 2. å³ä½¿å†…å®¹ç›¸åŒï¼Œä¹Ÿä¼šè¢«è§†ä¸ºä¸åŒçš„èŠ‚ç‚¹\n    # 3. ä¼šç”Ÿæˆç›¸åŒæˆ–éžå¸¸ç›¸ä¼¼çš„å‘é‡è¡¨ç¤º\n    # 4. åœ¨æ£€ç´¢æ—¶å¯èƒ½ä¼šè¿”å›žå¤šä¸ªç›¸ä¼¼çš„ç»“æžœ\n    logger.info(\"ðŸ“Š å‘é‡å­˜å‚¨è¡Œä¸ºåˆ†æž:\")\n    logger.info(\"1. âœ… æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰å”¯ä¸€çš„node_idï¼Œå³ä½¿å†…å®¹ç›¸åŒ\")",
        "detail": "test_duplicate_content",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "test_duplicate_content",
        "description": "test_duplicate_content",
        "peekOfCode": "def main():\n    \"\"\"ä¸»æµ‹è¯•æµç¨‹\"\"\"\n    print(\"ðŸ§ª é‡å¤æ–‡æœ¬å—å¤„ç†æœºåˆ¶æµ‹è¯•\")\n    print(\"=\" * 50)\n    # 1. åˆ†æžèŠ‚ç‚¹ç”Ÿæˆ\n    nodes, content_analysis = analyze_node_generation()\n    print()\n    # 2. æµ‹è¯•å­˜å‚¨è¡Œä¸º\n    test_storage_behavior()\n    print()",
        "detail": "test_duplicate_content",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "test_duplicate_content",
        "description": "test_duplicate_content",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef create_test_files_with_duplicate_content():\n    \"\"\"åˆ›å»ºåŒ…å«é‡å¤å†…å®¹çš„æµ‹è¯•æ–‡ä»¶\"\"\"\n    test_dir = \"test_duplicate_data\"\n    os.makedirs(test_dir, exist_ok=True)\n    # åˆ›å»ºä¸¤ä¸ªæ–‡ä»¶ï¼ŒåŒ…å«ç›¸åŒçš„æ–‡æœ¬å—\n    duplicate_content = \"\"\"\nè¿™æ˜¯ä¸€æ®µé‡å¤çš„æ–‡æœ¬å†…å®¹ã€‚\nè¿™æ®µå†…å®¹ä¼šå‡ºçŽ°åœ¨å¤šä¸ªæ–‡ä»¶ä¸­ã€‚\nç”¨äºŽæµ‹è¯•ç³»ç»Ÿå¦‚ä½•å¤„ç†é‡å¤çš„æ–‡æœ¬å—ã€‚",
        "detail": "test_duplicate_content",
        "documentation": {}
    },
    {
        "label": "test_query_engine",
        "kind": 2,
        "importPath": "test_query_system",
        "description": "test_query_system",
        "peekOfCode": "def test_query_engine():\n    \"\"\"æµ‹è¯•æŸ¥è¯¢å¼•æ“Ž\"\"\"\n    print(\"ðŸ” æµ‹è¯•æŸ¥è¯¢å¼•æ“Ž\")\n    print(\"=\" * 60)\n    try:\n        # åŠ è½½çŽ¯å¢ƒå˜é‡\n        load_dotenv()\n        # åˆå§‹åŒ–è®¾ç½®\n        from app.settings import init_settings\n        init_settings()",
        "detail": "test_query_system",
        "documentation": {}
    },
    {
        "label": "test_api_endpoint",
        "kind": 2,
        "importPath": "test_query_system",
        "description": "test_query_system",
        "peekOfCode": "def test_api_endpoint():\n    \"\"\"æµ‹è¯• API ç«¯ç‚¹\"\"\"\n    print(\"\\nðŸŒ æµ‹è¯• API ç«¯ç‚¹\")\n    print(\"=\" * 60)\n    try:\n        import requests\n        import json\n        # æµ‹è¯•æŸ¥è¯¢ API\n        url = \"http://localhost:8000/api/chat\"\n        payload = {",
        "detail": "test_query_system",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "test_query_system",
        "description": "test_query_system",
        "peekOfCode": "def main():\n    \"\"\"ä¸»å‡½æ•°\"\"\"\n    print(\"ðŸ§ª æŸ¥è¯¢ç³»ç»Ÿå®Œæ•´æ€§æµ‹è¯•\")\n    print(\"=\" * 80)\n    # æµ‹è¯•æŸ¥è¯¢å¼•æ“Ž\n    engine_ok = test_query_engine()\n    # æµ‹è¯• API ç«¯ç‚¹\n    api_ok = test_api_endpoint()\n    print(\"\\nðŸ“Š æµ‹è¯•ç»“æžœæ€»ç»“\")\n    print(\"=\" * 60)",
        "detail": "test_query_system",
        "documentation": {}
    },
    {
        "label": "test_storage_creation",
        "kind": 2,
        "importPath": "test_storage",
        "description": "test_storage",
        "peekOfCode": "def test_storage_creation():\n    \"\"\"æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»º\"\"\"\n    try:\n        logger.info(\"å¼€å§‹æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»º...\")\n        # å¯¼å…¥å¿…è¦çš„æ¨¡å—\n        from app.storage_config import get_storage_context\n        from app.index import STORAGE_DIR\n        logger.info(f\"å­˜å‚¨ç›®å½•: {STORAGE_DIR}\")\n        # å°è¯•åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡\n        storage_context = get_storage_context(STORAGE_DIR)",
        "detail": "test_storage",
        "documentation": {}
    },
    {
        "label": "test_chromadb_directly",
        "kind": 2,
        "importPath": "test_storage",
        "description": "test_storage",
        "peekOfCode": "def test_chromadb_directly():\n    \"\"\"ç›´æŽ¥æµ‹è¯• ChromaDB\"\"\"\n    try:\n        logger.info(\"å¼€å§‹ç›´æŽ¥æµ‹è¯• ChromaDB...\")\n        import chromadb\n        from chromadb.config import Settings as ChromaSettings\n        chroma_db_path = os.path.join(\"storage\", \"chroma_db\")\n        logger.info(f\"ChromaDB è·¯å¾„: {chroma_db_path}\")\n        # åˆ›å»º ChromaDB å®¢æˆ·ç«¯\n        chroma_client = chromadb.PersistentClient(",
        "detail": "test_storage",
        "documentation": {}
    },
    {
        "label": "test_sqlite_stores",
        "kind": 2,
        "importPath": "test_storage",
        "description": "test_storage",
        "peekOfCode": "def test_sqlite_stores():\n    \"\"\"æµ‹è¯• SQLite å­˜å‚¨\"\"\"\n    try:\n        logger.info(\"å¼€å§‹æµ‹è¯• SQLite å­˜å‚¨...\")\n        from app.sqlite_stores import SQLiteDocumentStore, SQLiteIndexStore\n        # æµ‹è¯•æ–‡æ¡£å­˜å‚¨\n        docstore_path = os.path.join(\"storage\", \"docstore.db\")\n        docstore = SQLiteDocumentStore(docstore_path)\n        logger.info(\"âœ… SQLite æ–‡æ¡£å­˜å‚¨åˆ›å»ºæˆåŠŸï¼\")\n        # æµ‹è¯•ç´¢å¼•å­˜å‚¨",
        "detail": "test_storage",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "test_storage",
        "description": "test_storage",
        "peekOfCode": "def main():\n    \"\"\"ä¸»æµ‹è¯•æµç¨‹\"\"\"\n    print(\"ðŸ” å¼€å§‹å­˜å‚¨ç³»ç»Ÿè¯Šæ–­\")\n    print(\"=\" * 50)\n    # 1. æµ‹è¯• SQLite å­˜å‚¨\n    if test_sqlite_stores():\n        print(\"âœ… SQLite å­˜å‚¨æµ‹è¯•é€šè¿‡\")\n    else:\n        print(\"âŒ SQLite å­˜å‚¨æµ‹è¯•å¤±è´¥\")\n    print()",
        "detail": "test_storage",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "test_storage",
        "description": "test_storage",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef test_storage_creation():\n    \"\"\"æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»º\"\"\"\n    try:\n        logger.info(\"å¼€å§‹æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»º...\")\n        # å¯¼å…¥å¿…è¦çš„æ¨¡å—\n        from app.storage_config import get_storage_context\n        from app.index import STORAGE_DIR\n        logger.info(f\"å­˜å‚¨ç›®å½•: {STORAGE_DIR}\")\n        # å°è¯•åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡",
        "detail": "test_storage",
        "documentation": {}
    },
    {
        "label": "test_storage_creation",
        "kind": 2,
        "importPath": "test_storage_config",
        "description": "test_storage_config",
        "peekOfCode": "def test_storage_creation():\n    \"\"\"æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»º\"\"\"\n    logger.info(\"ðŸ§ª æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»º...\")\n    try:\n        storage_context = get_storage_context(\"storage\")\n        # æ£€æŸ¥ç»„ä»¶\n        logger.info(f\"âœ… Vector Store: {type(storage_context.vector_store).__name__}\")\n        logger.info(f\"âœ… Document Store: {type(storage_context.docstore).__name__}\")\n        logger.info(f\"âœ… Index Store: {type(storage_context.index_store).__name__}\")\n        # æ£€æŸ¥æ˜¯å¦ç¦ç”¨äº†ä¸éœ€è¦çš„ç»„ä»¶",
        "detail": "test_storage_config",
        "documentation": {}
    },
    {
        "label": "test_storage_loading",
        "kind": 2,
        "importPath": "test_storage_config",
        "description": "test_storage_config",
        "peekOfCode": "def test_storage_loading():\n    \"\"\"æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡åŠ è½½\"\"\"\n    logger.info(\"ðŸ§ª æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡åŠ è½½...\")\n    try:\n        storage_context = load_storage_context(\"storage\")\n        if storage_context:\n            logger.info(\"âœ… å­˜å‚¨ä¸Šä¸‹æ–‡åŠ è½½æˆåŠŸ\")\n            return True\n        else:\n            logger.warning(\"âš ï¸  å­˜å‚¨ä¸Šä¸‹æ–‡åŠ è½½è¿”å›žNone\")",
        "detail": "test_storage_config",
        "documentation": {}
    },
    {
        "label": "check_storage_files",
        "kind": 2,
        "importPath": "test_storage_config",
        "description": "test_storage_config",
        "peekOfCode": "def check_storage_files():\n    \"\"\"æ£€æŸ¥å­˜å‚¨æ–‡ä»¶çŠ¶æ€\"\"\"\n    logger.info(\"ðŸ§ª æ£€æŸ¥å­˜å‚¨æ–‡ä»¶...\")\n    storage_dir = \"storage\"\n    required_files = [\n        \"docstore.db\",\n        \"index_store.db\",\n        \"chroma_db\"\n    ]\n    unwanted_files = [",
        "detail": "test_storage_config",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "test_storage_config",
        "description": "test_storage_config",
        "peekOfCode": "def main():\n    \"\"\"ä¸»æµ‹è¯•æµç¨‹\"\"\"\n    print(\"ðŸ§ª å­˜å‚¨é…ç½®æµ‹è¯•\")\n    print(\"=\" * 40)\n    tests = [\n        (\"å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»º\", test_storage_creation),\n        (\"å­˜å‚¨ä¸Šä¸‹æ–‡åŠ è½½\", test_storage_loading),\n        (\"å­˜å‚¨æ–‡ä»¶æ£€æŸ¥\", check_storage_files)\n    ]\n    passed = 0",
        "detail": "test_storage_config",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "test_storage_config",
        "description": "test_storage_config",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef test_storage_creation():\n    \"\"\"æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»º\"\"\"\n    logger.info(\"ðŸ§ª æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»º...\")\n    try:\n        storage_context = get_storage_context(\"storage\")\n        # æ£€æŸ¥ç»„ä»¶\n        logger.info(f\"âœ… Vector Store: {type(storage_context.vector_store).__name__}\")\n        logger.info(f\"âœ… Document Store: {type(storage_context.docstore).__name__}\")\n        logger.info(f\"âœ… Index Store: {type(storage_context.index_store).__name__}\")",
        "detail": "test_storage_config",
        "documentation": {}
    },
    {
        "label": "test_chunk_index_fix",
        "kind": 2,
        "importPath": "test_storage_fixes",
        "description": "test_storage_fixes",
        "peekOfCode": "def test_chunk_index_fix():\n    \"\"\"æµ‹è¯• chunk_index ä¿®å¤\"\"\"\n    print(\"ðŸ”§ æµ‹è¯• chunk_index ä¿®å¤\")\n    print(\"=\" * 60)\n    if not os.path.exists('storage/docstore.db'):\n        print(\"âŒ docstore.db ä¸å­˜åœ¨ï¼Œè¯·å…ˆä¸Šä¼ ä¸€äº›æ–‡æ¡£\")\n        return\n    conn = sqlite3.connect('storage/docstore.db')\n    # æ£€æŸ¥ chunk_index åˆ†å¸ƒ\n    cursor = conn.execute(\"\"\"",
        "detail": "test_storage_fixes",
        "documentation": {}
    },
    {
        "label": "test_same_file_handling",
        "kind": 2,
        "importPath": "test_storage_fixes",
        "description": "test_storage_fixes",
        "peekOfCode": "def test_same_file_handling():\n    \"\"\"æµ‹è¯•åŒåæ–‡ä»¶å¤„ç†\"\"\"\n    print(\"\\nðŸ”„ æµ‹è¯•åŒåæ–‡ä»¶å¤„ç†\")\n    print(\"=\" * 60)\n    if not os.path.exists('storage/docstore.db'):\n        print(\"âŒ docstore.db ä¸å­˜åœ¨\")\n        return\n    conn = sqlite3.connect('storage/docstore.db')\n    # æ£€æŸ¥æ–‡ä»¶é‡å¤æƒ…å†µ\n    cursor = conn.execute(\"\"\"",
        "detail": "test_storage_fixes",
        "documentation": {}
    },
    {
        "label": "test_chroma_consistency",
        "kind": 2,
        "importPath": "test_storage_fixes",
        "description": "test_storage_fixes",
        "peekOfCode": "def test_chroma_consistency():\n    \"\"\"æµ‹è¯• ChromaDB ä¸€è‡´æ€§\"\"\"\n    print(\"\\nðŸ§  æµ‹è¯• ChromaDB ä¸€è‡´æ€§\")\n    print(\"=\" * 60)\n    # èŽ·å– docstore ä¸­çš„ doc_id\n    docstore_ids = set()\n    if os.path.exists('storage/docstore.db'):\n        conn = sqlite3.connect('storage/docstore.db')\n        cursor = conn.execute(\"SELECT doc_id FROM documents\")\n        docstore_ids = {row[0] for row in cursor.fetchall()}",
        "detail": "test_storage_fixes",
        "documentation": {}
    },
    {
        "label": "test_data_field_structure",
        "kind": 2,
        "importPath": "test_storage_fixes",
        "description": "test_storage_fixes",
        "peekOfCode": "def test_data_field_structure():\n    \"\"\"æµ‹è¯• data å­—æ®µç»“æž„\"\"\"\n    print(\"\\nðŸ“‹ æµ‹è¯• data å­—æ®µç»“æž„\")\n    print(\"=\" * 60)\n    if not os.path.exists('storage/docstore.db'):\n        print(\"âŒ docstore.db ä¸å­˜åœ¨\")\n        return\n    conn = sqlite3.connect('storage/docstore.db')\n    cursor = conn.execute('SELECT doc_id, data FROM documents LIMIT 1')\n    row = cursor.fetchone()",
        "detail": "test_storage_fixes",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "test_storage_fixes",
        "description": "test_storage_fixes",
        "peekOfCode": "def main():\n    \"\"\"ä¸»å‡½æ•°\"\"\"\n    print(\"ðŸ§ª å­˜å‚¨é€»è¾‘ä¿®å¤æµ‹è¯•\")\n    print(\"=\" * 80)\n    test_chunk_index_fix()\n    test_same_file_handling()\n    test_chroma_consistency()\n    test_data_field_structure()\n    print(\"\\nðŸ’¡ æµ‹è¯•å®Œæˆ\")\n    print(\"å¦‚æžœå‘çŽ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä¿®å¤æ˜¯å¦æ­£ç¡®åº”ç”¨\")",
        "detail": "test_storage_fixes",
        "documentation": {}
    },
    {
        "label": "test_upload",
        "kind": 2,
        "importPath": "test_upload_api",
        "description": "test_upload_api",
        "peekOfCode": "def test_upload():\n    # æµ‹è¯•æ–‡ä»¶è·¯å¾„\n    file_path = \"test_upload.txt\"\n    if not os.path.exists(file_path):\n        print(f\"æµ‹è¯•æ–‡ä»¶ {file_path} ä¸å­˜åœ¨\")\n        return\n    # APIç«¯ç‚¹\n    url = \"http://127.0.0.1:8000/api/documents/upload\"\n    # å‡†å¤‡æ–‡ä»¶\n    with open(file_path, 'rb') as f:",
        "detail": "test_upload_api",
        "documentation": {}
    },
    {
        "label": "check_documents",
        "kind": 2,
        "importPath": "test_upload_api",
        "description": "test_upload_api",
        "peekOfCode": "def check_documents():\n    \"\"\"æ£€æŸ¥å½“å‰æ–‡æ¡£åˆ—è¡¨\"\"\"\n    url = \"http://127.0.0.1:8000/api/documents\"\n    try:\n        response = requests.get(url)\n        if response.status_code == 200:\n            result = response.json()\n            print(\"\\nðŸ“‹ å½“å‰æ–‡æ¡£åˆ—è¡¨:\")\n            if 'documents' in result:\n                for doc in result['documents']:",
        "detail": "test_upload_api",
        "documentation": {}
    },
    {
        "label": "trace_mysterious_id",
        "kind": 2,
        "importPath": "trace_mysterious_id",
        "description": "trace_mysterious_id",
        "peekOfCode": "def trace_mysterious_id():\n    \"\"\"è¿½è¸ªç¥žç§˜IDçš„æ¥æº\"\"\"\n    target_id = \"10316370-d5dc-4a54-ac60-7e853b805328\"\n    print(f\"ðŸ” è¿½è¸ªç¥žç§˜ID: {target_id}\")\n    print(\"=\" * 80)\n    # 1. æ£€æŸ¥ DocStore ä¸­æ˜¯å¦æœ‰è¿™ä¸ªID\n    print(\"\\n=== 1. æ£€æŸ¥ DocStore ===\")\n    docstore_path = 'storage/docstore.db'\n    if os.path.exists(docstore_path):\n        conn = sqlite3.connect(docstore_path)",
        "detail": "trace_mysterious_id",
        "documentation": {}
    },
    {
        "label": "update_file_metadata",
        "kind": 2,
        "importPath": "update_file_metadata",
        "description": "update_file_metadata",
        "peekOfCode": "def update_file_metadata():\n    \"\"\"Update existing documents with file metadata.\"\"\"\n    db_path = \"storage/docstore.db\"\n    data_dir = \"data\"\n    if not os.path.exists(db_path):\n        logger.error(f\"Database file not found: {db_path}\")\n        return False\n    if not os.path.exists(data_dir):\n        logger.error(f\"Data directory not found: {data_dir}\")\n        return False",
        "detail": "update_file_metadata",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "update_file_metadata",
        "description": "update_file_metadata",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef update_file_metadata():\n    \"\"\"Update existing documents with file metadata.\"\"\"\n    db_path = \"storage/docstore.db\"\n    data_dir = \"data\"\n    if not os.path.exists(db_path):\n        logger.error(f\"Database file not found: {db_path}\")\n        return False\n    if not os.path.exists(data_dir):\n        logger.error(f\"Data directory not found: {data_dir}\")",
        "detail": "update_file_metadata",
        "documentation": {}
    }
]