[
    {
        "label": "runpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "runpy",
        "description": "runpy",
        "detail": "runpy",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "site",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "site",
        "description": "site",
        "detail": "site",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices",
        "description": "llama_index.core.indices",
        "isExtraImport": true,
        "detail": "llama_index.core.indices",
        "documentation": {}
    },
    {
        "label": "ChatRequest",
        "importPath": "llama_index.server.api.models",
        "description": "llama_index.server.api.models",
        "isExtraImport": true,
        "detail": "llama_index.server.api.models",
        "documentation": {}
    },
    {
        "label": "ChatRequest",
        "importPath": "llama_index.server.api.models",
        "description": "llama_index.server.api.models",
        "isExtraImport": true,
        "detail": "llama_index.server.api.models",
        "documentation": {}
    },
    {
        "label": "load_storage_context",
        "importPath": "app.storage_config",
        "description": "app.storage_config",
        "isExtraImport": true,
        "detail": "app.storage_config",
        "documentation": {}
    },
    {
        "label": "get_storage_context",
        "importPath": "app.storage_config",
        "description": "app.storage_config",
        "isExtraImport": true,
        "detail": "app.storage_config",
        "documentation": {}
    },
    {
        "label": "get_storage_context",
        "importPath": "app.storage_config",
        "description": "app.storage_config",
        "isExtraImport": true,
        "detail": "app.storage_config",
        "documentation": {}
    },
    {
        "label": "load_storage_context",
        "importPath": "app.storage_config",
        "description": "app.storage_config",
        "isExtraImport": true,
        "detail": "app.storage_config",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbedding",
        "importPath": "llama_index.embeddings.openai",
        "description": "llama_index.embeddings.openai",
        "isExtraImport": true,
        "detail": "llama_index.embeddings.openai",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "llama_index.llms.openai",
        "description": "llama_index.llms.openai",
        "isExtraImport": true,
        "detail": "llama_index.llms.openai",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "llama_index.llms.openai",
        "description": "llama_index.llms.openai",
        "isExtraImport": true,
        "detail": "llama_index.llms.openai",
        "documentation": {}
    },
    {
        "label": "sqlite3",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sqlite3",
        "description": "sqlite3",
        "detail": "sqlite3",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "BaseDocumentStore",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "BaseIndexStore",
        "importPath": "llama_index.core.storage.index_store.types",
        "description": "llama_index.core.storage.index_store.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.index_store.types",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexStruct",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "ChromaVectorStore",
        "importPath": "llama_index.vector_stores.chroma",
        "description": "llama_index.vector_stores.chroma",
        "isExtraImport": true,
        "detail": "llama_index.vector_stores.chroma",
        "documentation": {}
    },
    {
        "label": "chromadb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "chromadb",
        "description": "chromadb",
        "detail": "chromadb",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "chromadb.config",
        "description": "chromadb.config",
        "isExtraImport": true,
        "detail": "chromadb.config",
        "documentation": {}
    },
    {
        "label": "SQLiteDocumentStore",
        "importPath": "app.sqlite_stores",
        "description": "app.sqlite_stores",
        "isExtraImport": true,
        "detail": "app.sqlite_stores",
        "documentation": {}
    },
    {
        "label": "SQLiteIndexStore",
        "importPath": "app.sqlite_stores",
        "description": "app.sqlite_stores",
        "isExtraImport": true,
        "detail": "app.sqlite_stores",
        "documentation": {}
    },
    {
        "label": "get_index",
        "importPath": "app.index",
        "description": "app.index",
        "isExtraImport": true,
        "detail": "app.index",
        "documentation": {}
    },
    {
        "label": "get_index",
        "importPath": "app.index",
        "description": "app.index",
        "isExtraImport": true,
        "detail": "app.index",
        "documentation": {}
    },
    {
        "label": "AgentWorkflow",
        "importPath": "llama_index.core.agent.workflow",
        "description": "llama_index.core.agent.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "get_query_engine_tool",
        "importPath": "llama_index.server.tools.index",
        "description": "llama_index.server.tools.index",
        "isExtraImport": true,
        "detail": "llama_index.server.tools.index",
        "documentation": {}
    },
    {
        "label": "CITATION_SYSTEM_PROMPT",
        "importPath": "llama_index.server.tools.index.citation",
        "description": "llama_index.server.tools.index.citation",
        "isExtraImport": true,
        "detail": "llama_index.server.tools.index.citation",
        "documentation": {}
    },
    {
        "label": "enable_citation",
        "importPath": "llama_index.server.tools.index.citation",
        "description": "llama_index.server.tools.index.citation",
        "isExtraImport": true,
        "detail": "llama_index.server.tools.index.citation",
        "documentation": {}
    },
    {
        "label": "TTFont",
        "importPath": "fontTools.ttLib",
        "description": "fontTools.ttLib",
        "isExtraImport": true,
        "detail": "fontTools.ttLib",
        "documentation": {}
    },
    {
        "label": "sfnt",
        "importPath": "fontTools.ttLib",
        "description": "fontTools.ttLib",
        "isExtraImport": true,
        "detail": "fontTools.ttLib",
        "documentation": {}
    },
    {
        "label": "TTFont",
        "importPath": "fontTools.ttLib",
        "description": "fontTools.ttLib",
        "isExtraImport": true,
        "detail": "fontTools.ttLib",
        "documentation": {}
    },
    {
        "label": "timestampNow",
        "importPath": "fontTools.misc.timeTools",
        "description": "fontTools.misc.timeTools",
        "isExtraImport": true,
        "detail": "fontTools.misc.timeTools",
        "documentation": {}
    },
    {
        "label": "collections",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections",
        "description": "collections",
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "parse_tfm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "parse_tfm",
        "description": "parse_tfm",
        "detail": "parse_tfm",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "init_settings",
        "importPath": "app.settings",
        "description": "app.settings",
        "isExtraImport": true,
        "detail": "app.settings",
        "documentation": {}
    },
    {
        "label": "create_workflow",
        "importPath": "app.workflow",
        "description": "app.workflow",
        "isExtraImport": true,
        "detail": "app.workflow",
        "documentation": {}
    },
    {
        "label": "LlamaIndexServer",
        "importPath": "llama_index.server",
        "description": "llama_index.server",
        "isExtraImport": true,
        "detail": "llama_index.server",
        "documentation": {}
    },
    {
        "label": "UIConfig",
        "importPath": "llama_index.server",
        "description": "llama_index.server",
        "isExtraImport": true,
        "detail": "llama_index.server",
        "documentation": {}
    },
    {
        "label": "JSONResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "StaticFiles",
        "importPath": "fastapi.staticfiles",
        "description": "fastapi.staticfiles",
        "isExtraImport": true,
        "detail": "fastapi.staticfiles",
        "documentation": {}
    },
    {
        "label": "CORSMiddleware",
        "importPath": "fastapi.middleware.cors",
        "description": "fastapi.middleware.cors",
        "isExtraImport": true,
        "detail": "fastapi.middleware.cors",
        "documentation": {}
    },
    {
        "label": "UploadFile",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "File",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "SentenceSplitter",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "SimpleDirectoryReader",
        "importPath": "llama_index.core.readers",
        "description": "llama_index.core.readers",
        "isExtraImport": true,
        "detail": "llama_index.core.readers",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "bin_dir",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "bin_dir = os.path.dirname(abs_file)\nbase = bin_dir[: -len(\"Scripts\") - 1]  # strip away the bin part from the __file__, plus the path separator\n# prepend bin to PATH (this file is inside the bin directory)\nos.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"app\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "base",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "base = bin_dir[: -len(\"Scripts\") - 1]  # strip away the bin part from the __file__, plus the path separator\n# prepend bin to PATH (this file is inside the bin directory)\nos.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"app\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PATH\"]",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"app\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"VIRTUAL_ENV\"]",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"app\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"VIRTUAL_ENV_PROMPT\"]",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"VIRTUAL_ENV_PROMPT\"] = \"app\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "prev_length",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "prev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.path[:]",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "sys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.real_prefix",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "sys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.prefix",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "sys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "get_index",
        "kind": 2,
        "importPath": "app.index",
        "description": "app.index",
        "peekOfCode": "def get_index(chat_request: Optional[ChatRequest] = None):\n    # check if storage already exists\n    if not os.path.exists(STORAGE_DIR):\n        return None\n    # load the existing storage context with SQLite and ChromaDB\n    logger.info(f\"Loading index from {STORAGE_DIR} using SQLite and ChromaDB...\")\n    storage_context = load_storage_context(STORAGE_DIR)\n    if storage_context is None:\n        logger.warning(f\"Could not load storage context from {STORAGE_DIR}\")\n        return None",
        "detail": "app.index",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.index",
        "description": "app.index",
        "peekOfCode": "logger = logging.getLogger(\"uvicorn\")\nSTORAGE_DIR = \"storage\"\ndef get_index(chat_request: Optional[ChatRequest] = None):\n    # check if storage already exists\n    if not os.path.exists(STORAGE_DIR):\n        return None\n    # load the existing storage context with SQLite and ChromaDB\n    logger.info(f\"Loading index from {STORAGE_DIR} using SQLite and ChromaDB...\")\n    storage_context = load_storage_context(STORAGE_DIR)\n    if storage_context is None:",
        "detail": "app.index",
        "documentation": {}
    },
    {
        "label": "STORAGE_DIR",
        "kind": 5,
        "importPath": "app.index",
        "description": "app.index",
        "peekOfCode": "STORAGE_DIR = \"storage\"\ndef get_index(chat_request: Optional[ChatRequest] = None):\n    # check if storage already exists\n    if not os.path.exists(STORAGE_DIR):\n        return None\n    # load the existing storage context with SQLite and ChromaDB\n    logger.info(f\"Loading index from {STORAGE_DIR} using SQLite and ChromaDB...\")\n    storage_context = load_storage_context(STORAGE_DIR)\n    if storage_context is None:\n        logger.warning(f\"Could not load storage context from {STORAGE_DIR}\")",
        "detail": "app.index",
        "documentation": {}
    },
    {
        "label": "init_settings",
        "kind": 2,
        "importPath": "app.settings",
        "description": "app.settings",
        "peekOfCode": "def init_settings():\n    if os.getenv(\"OPENAI_API_KEY\") is None:\n        raise RuntimeError(\"OPENAI_API_KEY is missing in environment variables\")\n    Settings.llm = OpenAI(model=\"gpt-4o-mini\")\n    Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")",
        "detail": "app.settings",
        "documentation": {}
    },
    {
        "label": "SQLiteDocumentStore",
        "kind": 6,
        "importPath": "app.sqlite_stores",
        "description": "app.sqlite_stores",
        "peekOfCode": "class SQLiteDocumentStore(BaseDocumentStore):\n    \"\"\"SQLite-based document store for better performance and concurrency.\"\"\"\n    def __init__(self, db_path: str):\n        \"\"\"Initialize SQLite document store.\n        Args:\n            db_path: Path to SQLite database file\n        \"\"\"\n        self.db_path = db_path\n        logger.info(f\"ðŸ”¥ Initializing SQLiteDocumentStore at {db_path}\")\n        self._init_db()",
        "detail": "app.sqlite_stores",
        "documentation": {}
    },
    {
        "label": "SQLiteIndexStore",
        "kind": 6,
        "importPath": "app.sqlite_stores",
        "description": "app.sqlite_stores",
        "peekOfCode": "class SQLiteIndexStore(BaseIndexStore):\n    \"\"\"SQLite-based index store for better performance and concurrency.\"\"\"\n    def __init__(self, db_path: str):\n        \"\"\"Initialize SQLite index store.\n        Args:\n            db_path: Path to SQLite database file\n        \"\"\"\n        self.db_path = db_path\n        self._init_db()\n    def _init_db(self):",
        "detail": "app.sqlite_stores",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.sqlite_stores",
        "description": "app.sqlite_stores",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass SQLiteDocumentStore(BaseDocumentStore):\n    \"\"\"SQLite-based document store for better performance and concurrency.\"\"\"\n    def __init__(self, db_path: str):\n        \"\"\"Initialize SQLite document store.\n        Args:\n            db_path: Path to SQLite database file\n        \"\"\"\n        self.db_path = db_path\n        logger.info(f\"ðŸ”¥ Initializing SQLiteDocumentStore at {db_path}\")",
        "detail": "app.sqlite_stores",
        "documentation": {}
    },
    {
        "label": "get_storage_context",
        "kind": 2,
        "importPath": "app.storage_config",
        "description": "app.storage_config",
        "peekOfCode": "def get_storage_context(storage_dir: str = \"storage\") -> StorageContext:\n    \"\"\"\n    Create a storage context using SQLite for docstore/index store and ChromaDB for vector store.\n    Optimized for text-only processing - no graph store or image vector store.\n    Args:\n        storage_dir: Directory to store the databases\n    Returns:\n        StorageContext configured with SQLite and ChromaDB backends\n    \"\"\"\n    # Ensure storage directory exists",
        "detail": "app.storage_config",
        "documentation": {}
    },
    {
        "label": "load_storage_context",
        "kind": 2,
        "importPath": "app.storage_config",
        "description": "app.storage_config",
        "peekOfCode": "def load_storage_context(storage_dir: str = \"storage\") -> Optional[StorageContext]:\n    \"\"\"\n    Load existing storage context from ChromaDB and SQLite stores.\n    Args:\n        storage_dir: Directory containing the databases\n    Returns:\n        StorageContext if databases exist, None otherwise\n    \"\"\"\n    # Check if storage directory exists\n    if not os.path.exists(storage_dir):",
        "detail": "app.storage_config",
        "documentation": {}
    },
    {
        "label": "migrate_json_to_sqlite",
        "kind": 2,
        "importPath": "app.storage_config",
        "description": "app.storage_config",
        "peekOfCode": "def migrate_json_to_sqlite(storage_dir: str = \"storage\") -> bool:\n    \"\"\"\n    Migrate existing JSON storage to SQLite.\n    Args:\n        storage_dir: Directory containing the storage files\n    Returns:\n        True if migration was successful, False otherwise\n    \"\"\"\n    import json\n    import sqlite3",
        "detail": "app.storage_config",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.storage_config",
        "description": "app.storage_config",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef get_storage_context(storage_dir: str = \"storage\") -> StorageContext:\n    \"\"\"\n    Create a storage context using SQLite for docstore/index store and ChromaDB for vector store.\n    Optimized for text-only processing - no graph store or image vector store.\n    Args:\n        storage_dir: Directory to store the databases\n    Returns:\n        StorageContext configured with SQLite and ChromaDB backends\n    \"\"\"",
        "detail": "app.storage_config",
        "documentation": {}
    },
    {
        "label": "create_workflow",
        "kind": 2,
        "importPath": "app.workflow",
        "description": "app.workflow",
        "peekOfCode": "def create_workflow(chat_request: Optional[ChatRequest] = None) -> AgentWorkflow:\n    index = get_index(chat_request=chat_request)\n    if index is None:\n        raise RuntimeError(\n            \"Index not found! Please run `uv run generate` to index the data first.\"\n        )\n    # Create a query tool with citations enabled\n    query_tool = enable_citation(get_query_engine_tool(index=index))\n    # Define the system prompt for the agent\n    # Append the citation system prompt to the system prompt",
        "detail": "app.workflow",
        "documentation": {}
    },
    {
        "label": "_Known",
        "kind": 6,
        "importPath": "components.node_modules.flatted.python.flatted",
        "description": "components.node_modules.flatted.python.flatted",
        "peekOfCode": "class _Known:\n    def __init__(self):\n        self.key = []\n        self.value = []\nclass _String:\n    def __init__(self, value):\n        self.value = value\ndef _array_keys(value):\n    keys = []\n    i = 0",
        "detail": "components.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "_String",
        "kind": 6,
        "importPath": "components.node_modules.flatted.python.flatted",
        "description": "components.node_modules.flatted.python.flatted",
        "peekOfCode": "class _String:\n    def __init__(self, value):\n        self.value = value\ndef _array_keys(value):\n    keys = []\n    i = 0\n    for _ in value:\n        keys.append(i)\n        i += 1\n    return keys",
        "detail": "components.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "parse",
        "kind": 2,
        "importPath": "components.node_modules.flatted.python.flatted",
        "description": "components.node_modules.flatted.python.flatted",
        "peekOfCode": "def parse(value, *args, **kwargs):\n    json = _json.loads(value, *args, **kwargs)\n    wrapped = []\n    for value in json:\n        wrapped.append(_wrap(value))\n    input = []\n    for value in wrapped:\n        if isinstance(value, _String):\n            input.append(value.value)\n        else:",
        "detail": "components.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "stringify",
        "kind": 2,
        "importPath": "components.node_modules.flatted.python.flatted",
        "description": "components.node_modules.flatted.python.flatted",
        "peekOfCode": "def stringify(value, *args, **kwargs):\n    known = _Known()\n    input = []\n    output = []\n    i = int(_index(known, input, value))\n    while i < len(input):\n        output.append(_transform(known, input, input[i]))\n        i += 1\n    return _json.dumps(output, *args, **kwargs)",
        "detail": "components.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "sfnt.USE_ZOPFLI",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "sfnt.USE_ZOPFLI = True\nif len(sys.argv) < 2:\n    print(\"Usage: %s <font file>\" % sys.argv[0])\n    sys.exit(1)\nfont_file = sys.argv[1]\nfont_name = os.path.splitext(os.path.basename(font_file))[0]\nfont = TTFont(font_file, recalcBBoxes=False, recalcTimestamp=False)\n# fix timestamp to the epoch\nfont['head'].created = 0\nfont['head'].modified = 0",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font_file",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font_file = sys.argv[1]\nfont_name = os.path.splitext(os.path.basename(font_file))[0]\nfont = TTFont(font_file, recalcBBoxes=False, recalcTimestamp=False)\n# fix timestamp to the epoch\nfont['head'].created = 0\nfont['head'].modified = 0\n# remove fontforge timestamps\nif 'FFTM' in font:\n    del font['FFTM']\n# remove redundant GDEF table",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font_name",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font_name = os.path.splitext(os.path.basename(font_file))[0]\nfont = TTFont(font_file, recalcBBoxes=False, recalcTimestamp=False)\n# fix timestamp to the epoch\nfont['head'].created = 0\nfont['head'].modified = 0\n# remove fontforge timestamps\nif 'FFTM' in font:\n    del font['FFTM']\n# remove redundant GDEF table\nif 'GDEF' in font:",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font = TTFont(font_file, recalcBBoxes=False, recalcTimestamp=False)\n# fix timestamp to the epoch\nfont['head'].created = 0\nfont['head'].modified = 0\n# remove fontforge timestamps\nif 'FFTM' in font:\n    del font['FFTM']\n# remove redundant GDEF table\nif 'GDEF' in font:\n    del font['GDEF']",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font['head'].created",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font['head'].created = 0\nfont['head'].modified = 0\n# remove fontforge timestamps\nif 'FFTM' in font:\n    del font['FFTM']\n# remove redundant GDEF table\nif 'GDEF' in font:\n    del font['GDEF']\n# remove Macintosh table\n# https://developer.apple.com/fonts/TrueType-Reference-Manual/RM06/Chap6cmap.html",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font['head'].modified",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font['head'].modified = 0\n# remove fontforge timestamps\nif 'FFTM' in font:\n    del font['FFTM']\n# remove redundant GDEF table\nif 'GDEF' in font:\n    del font['GDEF']\n# remove Macintosh table\n# https://developer.apple.com/fonts/TrueType-Reference-Manual/RM06/Chap6cmap.html\nfont['name'].names = [record for record in font['name'].names if record.platformID != 1]",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font['name'].names",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font['name'].names = [record for record in font['name'].names if record.platformID != 1]\nfont['cmap'].tables = [table for table in font['cmap'].tables if table.platformID != 1]\n# fix OS/2 and hhea metrics\nglyf = font['glyf']\nascent = int(max(glyf[c].yMax for c in font.getGlyphOrder() if hasattr(glyf[c], \"yMax\")))\ndescent = -int(min(glyf[c].yMin for c in font.getGlyphOrder() if hasattr(glyf[c], \"yMin\")))\nfont['OS/2'].usWinAscent = ascent\nfont['OS/2'].usWinDescent = descent\nfont['hhea'].ascent = ascent\nfont['hhea'].descent = -descent",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font['cmap'].tables",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font['cmap'].tables = [table for table in font['cmap'].tables if table.platformID != 1]\n# fix OS/2 and hhea metrics\nglyf = font['glyf']\nascent = int(max(glyf[c].yMax for c in font.getGlyphOrder() if hasattr(glyf[c], \"yMax\")))\ndescent = -int(min(glyf[c].yMin for c in font.getGlyphOrder() if hasattr(glyf[c], \"yMin\")))\nfont['OS/2'].usWinAscent = ascent\nfont['OS/2'].usWinDescent = descent\nfont['hhea'].ascent = ascent\nfont['hhea'].descent = -descent\n# save TTF",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "glyf",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "glyf = font['glyf']\nascent = int(max(glyf[c].yMax for c in font.getGlyphOrder() if hasattr(glyf[c], \"yMax\")))\ndescent = -int(min(glyf[c].yMin for c in font.getGlyphOrder() if hasattr(glyf[c], \"yMin\")))\nfont['OS/2'].usWinAscent = ascent\nfont['OS/2'].usWinDescent = descent\nfont['hhea'].ascent = ascent\nfont['hhea'].descent = -descent\n# save TTF\nfont.save(font_file, reorderTables=None)\n# save WOFF",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "ascent",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "ascent = int(max(glyf[c].yMax for c in font.getGlyphOrder() if hasattr(glyf[c], \"yMax\")))\ndescent = -int(min(glyf[c].yMin for c in font.getGlyphOrder() if hasattr(glyf[c], \"yMin\")))\nfont['OS/2'].usWinAscent = ascent\nfont['OS/2'].usWinDescent = descent\nfont['hhea'].ascent = ascent\nfont['hhea'].descent = -descent\n# save TTF\nfont.save(font_file, reorderTables=None)\n# save WOFF\nfont.flavor = 'woff'",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "descent",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "descent = -int(min(glyf[c].yMin for c in font.getGlyphOrder() if hasattr(glyf[c], \"yMin\")))\nfont['OS/2'].usWinAscent = ascent\nfont['OS/2'].usWinDescent = descent\nfont['hhea'].ascent = ascent\nfont['hhea'].descent = -descent\n# save TTF\nfont.save(font_file, reorderTables=None)\n# save WOFF\nfont.flavor = 'woff'\nfont.save(os.path.join('woff', font_name + '.woff'), reorderTables=None)",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font['OS/2'].usWinAscent",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font['OS/2'].usWinAscent = ascent\nfont['OS/2'].usWinDescent = descent\nfont['hhea'].ascent = ascent\nfont['hhea'].descent = -descent\n# save TTF\nfont.save(font_file, reorderTables=None)\n# save WOFF\nfont.flavor = 'woff'\nfont.save(os.path.join('woff', font_name + '.woff'), reorderTables=None)\n# save WOFF2",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font['OS/2'].usWinDescent",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font['OS/2'].usWinDescent = descent\nfont['hhea'].ascent = ascent\nfont['hhea'].descent = -descent\n# save TTF\nfont.save(font_file, reorderTables=None)\n# save WOFF\nfont.flavor = 'woff'\nfont.save(os.path.join('woff', font_name + '.woff'), reorderTables=None)\n# save WOFF2\nfont.flavor = 'woff2'",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font['hhea'].ascent",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font['hhea'].ascent = ascent\nfont['hhea'].descent = -descent\n# save TTF\nfont.save(font_file, reorderTables=None)\n# save WOFF\nfont.flavor = 'woff'\nfont.save(os.path.join('woff', font_name + '.woff'), reorderTables=None)\n# save WOFF2\nfont.flavor = 'woff2'\nfont.save(os.path.join('woff2', font_name + '.woff2'), reorderTables=None)",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font['hhea'].descent",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font['hhea'].descent = -descent\n# save TTF\nfont.save(font_file, reorderTables=None)\n# save WOFF\nfont.flavor = 'woff'\nfont.save(os.path.join('woff', font_name + '.woff'), reorderTables=None)\n# save WOFF2\nfont.flavor = 'woff2'\nfont.save(os.path.join('woff2', font_name + '.woff2'), reorderTables=None)",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font.flavor",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font.flavor = 'woff'\nfont.save(os.path.join('woff', font_name + '.woff'), reorderTables=None)\n# save WOFF2\nfont.flavor = 'woff2'\nfont.save(os.path.join('woff2', font_name + '.woff2'), reorderTables=None)",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "font.flavor",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.fonts.generate_fonts",
        "description": "components.node_modules.katex.src.fonts.generate_fonts",
        "peekOfCode": "font.flavor = 'woff2'\nfont.save(os.path.join('woff2', font_name + '.woff2'), reorderTables=None)",
        "detail": "components.node_modules.katex.src.fonts.generate_fonts",
        "documentation": {}
    },
    {
        "label": "find_font_path",
        "kind": 2,
        "importPath": "components.node_modules.katex.src.metrics.extract_tfms",
        "description": "components.node_modules.katex.src.metrics.extract_tfms",
        "peekOfCode": "def find_font_path(font_name):\n    try:\n        font_path = subprocess.check_output(['kpsewhich', font_name])\n    except OSError:\n        raise RuntimeError(\"Couldn't find kpsewhich program, make sure you\" +\n                           \" have TeX installed\")\n    except subprocess.CalledProcessError:\n        raise RuntimeError(\"Couldn't find font metrics: '%s'\" % font_name)\n    return font_path.strip()\ndef main():",
        "detail": "components.node_modules.katex.src.metrics.extract_tfms",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "components.node_modules.katex.src.metrics.extract_tfms",
        "description": "components.node_modules.katex.src.metrics.extract_tfms",
        "peekOfCode": "def main():\n    mapping = json.load(sys.stdin)\n    fonts = [\n        'cmbsy10.tfm',\n        'cmbx10.tfm',\n        'cmbxti10.tfm',\n        'cmex10.tfm',\n        'cmmi10.tfm',\n        'cmmib10.tfm',\n        'cmr10.tfm',",
        "detail": "components.node_modules.katex.src.metrics.extract_tfms",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "components.node_modules.katex.src.metrics.extract_ttfs",
        "description": "components.node_modules.katex.src.metrics.extract_ttfs",
        "peekOfCode": "def main():\n    start_json = json.load(sys.stdin)\n    for font in start_json:\n        fontInfo = TTFont(\"../../fonts/KaTeX_\" + font + \".ttf\")\n        glyf = fontInfo[\"glyf\"]\n        widths = fontInfo.getGlyphSet()\n        unitsPerEm = float(fontInfo[\"head\"].unitsPerEm)\n        # We keep ALL Unicode cmaps, not just fontInfo[\"cmap\"].getcmap(3, 1).\n        # This is playing it extra safe, since it reports inconsistencies.\n        # Platform 0 is Unicode, platform 3 is Windows. For platform 3,",
        "detail": "components.node_modules.katex.src.metrics.extract_ttfs",
        "documentation": {}
    },
    {
        "label": "metrics_to_extract",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.metrics.extract_ttfs",
        "description": "components.node_modules.katex.src.metrics.extract_ttfs",
        "peekOfCode": "metrics_to_extract = {\n    # Font name\n    \"AMS-Regular\": {\n        u\"\\u21e2\": None,  # \\dashrightarrow\n        u\"\\u21e0\": None,  # \\dashleftarrow\n    },\n    \"Main-Regular\": {\n        # Skew and italic metrics can't be easily parsed from the TTF. Instead,\n        # we map each character to a \"base character\", which is a character\n        # from the same font with correct italic and skew metrics. A character",
        "detail": "components.node_modules.katex.src.metrics.extract_ttfs",
        "documentation": {}
    },
    {
        "label": "props",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.metrics.format_json",
        "description": "components.node_modules.katex.src.metrics.format_json",
        "peekOfCode": "props = ['depth', 'height', 'italic', 'skew']\nif len(sys.argv) > 1:\n    if sys.argv[1] == '--width':\n        props.append('width')\ndata = json.load(sys.stdin)\nsys.stdout.write(\n  \"// This file is GENERATED by buildMetrics.sh. DO NOT MODIFY.\\n\")\nsep = \"export default {\\n    \"\nfor font in sorted(data):\n    sys.stdout.write(sep + json.dumps(font))",
        "detail": "components.node_modules.katex.src.metrics.format_json",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.metrics.format_json",
        "description": "components.node_modules.katex.src.metrics.format_json",
        "peekOfCode": "data = json.load(sys.stdin)\nsys.stdout.write(\n  \"// This file is GENERATED by buildMetrics.sh. DO NOT MODIFY.\\n\")\nsep = \"export default {\\n    \"\nfor font in sorted(data):\n    sys.stdout.write(sep + json.dumps(font))\n    sep = \": {\\n        \"\n    for glyph in sorted(data[font], key=int):\n        sys.stdout.write(sep + json.dumps(glyph) + \": \")\n        values = [value if value != 0.0 else 0 for value in",
        "detail": "components.node_modules.katex.src.metrics.format_json",
        "documentation": {}
    },
    {
        "label": "sep",
        "kind": 5,
        "importPath": "components.node_modules.katex.src.metrics.format_json",
        "description": "components.node_modules.katex.src.metrics.format_json",
        "peekOfCode": "sep = \"export default {\\n    \"\nfor font in sorted(data):\n    sys.stdout.write(sep + json.dumps(font))\n    sep = \": {\\n        \"\n    for glyph in sorted(data[font], key=int):\n        sys.stdout.write(sep + json.dumps(glyph) + \": \")\n        values = [value if value != 0.0 else 0 for value in\n                  [data[font][glyph][key] for key in props]]\n        sys.stdout.write(json.dumps(values))\n        sep = \",\\n        \"",
        "detail": "components.node_modules.katex.src.metrics.format_json",
        "documentation": {}
    },
    {
        "label": "CharInfoWord",
        "kind": 6,
        "importPath": "components.node_modules.katex.src.metrics.parse_tfm",
        "description": "components.node_modules.katex.src.metrics.parse_tfm",
        "peekOfCode": "class CharInfoWord(object):\n    def __init__(self, word):\n        b1, b2, b3, b4 = (word >> 24,\n                          (word & 0xff0000) >> 16,\n                          (word & 0xff00) >> 8,\n                          word & 0xff)\n        self.width_index = b1\n        self.height_index = b2 >> 4\n        self.depth_index = b2 & 0x0f\n        self.italic_index = (b3 & 0b11111100) >> 2",
        "detail": "components.node_modules.katex.src.metrics.parse_tfm",
        "documentation": {}
    },
    {
        "label": "LigKernProgram",
        "kind": 6,
        "importPath": "components.node_modules.katex.src.metrics.parse_tfm",
        "description": "components.node_modules.katex.src.metrics.parse_tfm",
        "peekOfCode": "class LigKernProgram(object):\n    def __init__(self, program):\n        self.program = program\n    def execute(self, start, next_char):\n        curr_instruction = start\n        while True:\n            instruction = self.program[curr_instruction]\n            (skip, inst_next_char, op, remainder) = instruction\n            if inst_next_char == next_char:\n                if op < 128:",
        "detail": "components.node_modules.katex.src.metrics.parse_tfm",
        "documentation": {}
    },
    {
        "label": "TfmCharMetrics",
        "kind": 6,
        "importPath": "components.node_modules.katex.src.metrics.parse_tfm",
        "description": "components.node_modules.katex.src.metrics.parse_tfm",
        "peekOfCode": "class TfmCharMetrics(object):\n    def __init__(self, width, height, depth, italic, kern_table):\n        self.width = width\n        self.height = height\n        self.depth = depth\n        self.italic_correction = italic\n        self.kern_table = kern_table\nclass TfmFile(object):\n    def __init__(self, start_char, end_char, char_info, width_table,\n                 height_table, depth_table, italic_table, ligkern_table,",
        "detail": "components.node_modules.katex.src.metrics.parse_tfm",
        "documentation": {}
    },
    {
        "label": "TfmFile",
        "kind": 6,
        "importPath": "components.node_modules.katex.src.metrics.parse_tfm",
        "description": "components.node_modules.katex.src.metrics.parse_tfm",
        "peekOfCode": "class TfmFile(object):\n    def __init__(self, start_char, end_char, char_info, width_table,\n                 height_table, depth_table, italic_table, ligkern_table,\n                 kern_table):\n        self.start_char = start_char\n        self.end_char = end_char\n        self.char_info = char_info\n        self.width_table = width_table\n        self.height_table = height_table\n        self.depth_table = depth_table",
        "detail": "components.node_modules.katex.src.metrics.parse_tfm",
        "documentation": {}
    },
    {
        "label": "TfmReader",
        "kind": 6,
        "importPath": "components.node_modules.katex.src.metrics.parse_tfm",
        "description": "components.node_modules.katex.src.metrics.parse_tfm",
        "peekOfCode": "class TfmReader(object):\n    def __init__(self, f):\n        self.f = f\n    def read_byte(self):\n        return ord(self.f.read(1))\n    def read_halfword(self):\n        b1 = self.read_byte()\n        b2 = self.read_byte()\n        return (b1 << 8) | b2\n    def read_word(self):",
        "detail": "components.node_modules.katex.src.metrics.parse_tfm",
        "documentation": {}
    },
    {
        "label": "read_tfm_file",
        "kind": 2,
        "importPath": "components.node_modules.katex.src.metrics.parse_tfm",
        "description": "components.node_modules.katex.src.metrics.parse_tfm",
        "peekOfCode": "def read_tfm_file(file_name):\n    with open(file_name, 'rb') as f:\n        reader = TfmReader(f)\n        # file_size\n        reader.read_halfword()\n        header_size = reader.read_halfword()\n        start_char = reader.read_halfword()\n        end_char = reader.read_halfword()\n        width_table_size = reader.read_halfword()\n        height_table_size = reader.read_halfword()",
        "detail": "components.node_modules.katex.src.metrics.parse_tfm",
        "documentation": {}
    },
    {
        "label": "analyze_storage_directory",
        "kind": 2,
        "importPath": "analyze_storage_files",
        "description": "analyze_storage_files",
        "peekOfCode": "def analyze_storage_directory():\n    \"\"\"åˆ†æžå­˜å‚¨ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶\"\"\"\n    storage_dir = Path(\"storage\")\n    print(\"ðŸ“ å­˜å‚¨ç›®å½•åˆ†æžæŠ¥å‘Š\")\n    print(\"=\" * 60)\n    # 1. åˆ†æžæ‰€æœ‰æ–‡ä»¶\n    print(\"ðŸ“‹ æ–‡ä»¶æ¸…å•:\")\n    for file_path in storage_dir.rglob(\"*\"):\n        if file_path.is_file():\n            size = file_path.stat().st_size",
        "detail": "analyze_storage_files",
        "documentation": {}
    },
    {
        "label": "analyze_sqlite_db",
        "kind": 2,
        "importPath": "analyze_storage_files",
        "description": "analyze_storage_files",
        "peekOfCode": "def analyze_sqlite_db(db_path):\n    \"\"\"åˆ†æžSQLiteæ•°æ®åº“\"\"\"\n    try:\n        with sqlite3.connect(db_path) as conn:\n            cursor = conn.cursor()\n            # èŽ·å–è¡¨åˆ—è¡¨\n            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n            tables = [row[0] for row in cursor.fetchall()]\n            print(f\"    è¡¨æ•°é‡: {len(tables)}\")\n            for table in tables:",
        "detail": "analyze_storage_files",
        "documentation": {}
    },
    {
        "label": "analyze_json_file",
        "kind": 2,
        "importPath": "analyze_storage_files",
        "description": "analyze_storage_files",
        "peekOfCode": "def analyze_json_file(json_path):\n    \"\"\"åˆ†æžJSONæ–‡ä»¶\"\"\"\n    try:\n        with open(json_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        print(f\"    æ–‡ä»¶å¤§å°: {json_path.stat().st_size:,} bytes\")\n        print(f\"    JSONç»“æž„:\")\n        def analyze_json_structure(obj, indent=6):\n            if isinstance(obj, dict):\n                print(f\"{' ' * indent}å­—å…¸ - {len(obj)} ä¸ªé”®:\")",
        "detail": "analyze_storage_files",
        "documentation": {}
    },
    {
        "label": "check_file_usage",
        "kind": 2,
        "importPath": "analyze_storage_files",
        "description": "analyze_storage_files",
        "peekOfCode": "def check_file_usage():\n    \"\"\"æ£€æŸ¥æ–‡ä»¶çš„å®žé™…ä½¿ç”¨æƒ…å†µ\"\"\"\n    print(\"ðŸ” æ–‡ä»¶ä½¿ç”¨æƒ…å†µåˆ†æž:\")\n    print(\"=\" * 60)\n    # æ£€æŸ¥ä»£ç ä¸­å¯¹è¿™äº›æ–‡ä»¶çš„å¼•ç”¨\n    storage_files = [\n        \"docstore.db\",\n        \"index_store.db\", \n        \"chroma.sqlite3\",\n        \"graph_store.json\",",
        "detail": "analyze_storage_files",
        "documentation": {}
    },
    {
        "label": "check_database_status",
        "kind": 2,
        "importPath": "check_db_duplicates",
        "description": "check_db_duplicates",
        "peekOfCode": "def check_database_status():\n    \"\"\"æ£€æŸ¥æ•°æ®åº“çŠ¶æ€å’Œé‡å¤æ•°æ®\"\"\"\n    db_path = \"storage/docstore.db\"\n    if not os.path.exists(db_path):\n        print(f\"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}\")\n        return\n    print(f\"ðŸ“Š æ£€æŸ¥æ•°æ®åº“: {db_path}\")\n    print(\"=\" * 50)\n    with sqlite3.connect(db_path) as conn:\n        cursor = conn.cursor()",
        "detail": "check_db_duplicates",
        "documentation": {}
    },
    {
        "label": "suggest_cleanup_actions",
        "kind": 2,
        "importPath": "check_db_duplicates",
        "description": "check_db_duplicates",
        "peekOfCode": "def suggest_cleanup_actions():\n    \"\"\"å»ºè®®æ¸…ç†æ“ä½œ\"\"\"\n    print(\"ðŸ› ï¸  æ•°æ®åº“æ¸…ç†å»ºè®®:\")\n    print(\"=\" * 50)\n    print(\"1. ðŸ”„ å®Œå…¨é‡ç½®æ•°æ®åº“ (æŽ¨è)\")\n    print(\"   - åˆ é™¤çŽ°æœ‰æ•°æ®åº“æ–‡ä»¶\")\n    print(\"   - é‡æ–°åˆå§‹åŒ–å¹²å‡€çš„æ•°æ®åº“\")\n    print(\"   - é‡æ–°ç´¢å¼•æ‰€æœ‰æ–‡æ¡£\")\n    print()\n    print(\"2. ðŸ§¹ æ¸…ç†é‡å¤æ•°æ®\")",
        "detail": "check_db_duplicates",
        "documentation": {}
    },
    {
        "label": "force_remove_file",
        "kind": 2,
        "importPath": "force_reset_database",
        "description": "force_reset_database",
        "peekOfCode": "def force_remove_file(file_path, max_attempts=5):\n    \"\"\"å¼ºåˆ¶åˆ é™¤æ–‡ä»¶ï¼Œå¤„ç†è¢«å ç”¨çš„æƒ…å†µ\"\"\"\n    for attempt in range(max_attempts):\n        try:\n            if os.path.isfile(file_path):\n                os.remove(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n            logger.info(f\"âœ… å·²åˆ é™¤: {file_path}\")\n            return True",
        "detail": "force_reset_database",
        "documentation": {}
    },
    {
        "label": "force_reset_storage",
        "kind": 2,
        "importPath": "force_reset_database",
        "description": "force_reset_database",
        "peekOfCode": "def force_reset_storage(storage_dir=\"storage\"):\n    \"\"\"å¼ºåˆ¶é‡ç½®å­˜å‚¨ç›®å½•\"\"\"\n    logger.info(\"ðŸ”„ å¼€å§‹å¼ºåˆ¶é‡ç½®å­˜å‚¨...\")\n    if not os.path.exists(storage_dir):\n        logger.info(f\"å­˜å‚¨ç›®å½• {storage_dir} ä¸å­˜åœ¨\")\n        return True\n    # é€ä¸ªåˆ é™¤æ–‡ä»¶\n    files_to_remove = []\n    for root, dirs, files in os.walk(storage_dir):\n        for file in files:",
        "detail": "force_reset_database",
        "documentation": {}
    },
    {
        "label": "create_clean_storage_context",
        "kind": 2,
        "importPath": "force_reset_database",
        "description": "force_reset_database",
        "peekOfCode": "def create_clean_storage_context():\n    \"\"\"åˆ›å»ºå¹²å‡€çš„å­˜å‚¨ä¸Šä¸‹æ–‡\"\"\"\n    try:\n        from app.storage_config import get_storage_context\n        logger.info(\"ðŸ”§ åˆ›å»ºå¹²å‡€çš„å­˜å‚¨ä¸Šä¸‹æ–‡...\")\n        storage_context = get_storage_context(\"storage\")\n        # éªŒè¯åˆ›å»ºç»“æžœ\n        storage_files = [\n            \"storage/docstore.db\",\n            \"storage/index_store.db\",",
        "detail": "force_reset_database",
        "documentation": {}
    },
    {
        "label": "verify_clean_state",
        "kind": 2,
        "importPath": "force_reset_database",
        "description": "force_reset_database",
        "peekOfCode": "def verify_clean_state():\n    \"\"\"éªŒè¯æ¸…ç†çŠ¶æ€\"\"\"\n    logger.info(\"ðŸ” éªŒè¯æ¸…ç†çŠ¶æ€...\")\n    # æ£€æŸ¥ä¸åº”è¯¥å­˜åœ¨çš„æ–‡ä»¶\n    unwanted_files = [\n        \"storage/graph_store.json\",\n        \"storage/image__vector_store.json\"\n    ]\n    clean = True\n    for file_path in unwanted_files:",
        "detail": "force_reset_database",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "force_reset_database",
        "description": "force_reset_database",
        "peekOfCode": "def main():\n    \"\"\"ä¸»é‡ç½®æµç¨‹\"\"\"\n    print(\"ðŸ”„ å¼ºåˆ¶æ•°æ®åº“é‡ç½®\")\n    print(\"=\" * 40)\n    # 1. å¼ºåˆ¶åˆ é™¤å­˜å‚¨ç›®å½•\n    if not force_reset_storage(\"storage\"):\n        print(\"âŒ å¼ºåˆ¶é‡ç½®å¤±è´¥\")\n        return\n    # 2. åˆ›å»ºå¹²å‡€çš„å­˜å‚¨\n    if not create_clean_storage_context():",
        "detail": "force_reset_database",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "force_reset_database",
        "description": "force_reset_database",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef force_remove_file(file_path, max_attempts=5):\n    \"\"\"å¼ºåˆ¶åˆ é™¤æ–‡ä»¶ï¼Œå¤„ç†è¢«å ç”¨çš„æƒ…å†µ\"\"\"\n    for attempt in range(max_attempts):\n        try:\n            if os.path.isfile(file_path):\n                os.remove(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n            logger.info(f\"âœ… å·²åˆ é™¤: {file_path}\")",
        "detail": "force_reset_database",
        "documentation": {}
    },
    {
        "label": "generate_index",
        "kind": 2,
        "importPath": "generate",
        "description": "generate",
        "peekOfCode": "def generate_index():\n    \"\"\"\n    Index the documents in the data directory using SQLite and ChromaDB.\n    \"\"\"\n    from app.index import STORAGE_DIR\n    from app.settings import init_settings\n    from app.storage_config import get_storage_context\n    from llama_index.core.indices import (\n        VectorStoreIndex,\n    )",
        "detail": "generate",
        "documentation": {}
    },
    {
        "label": "generate_ui_for_workflow",
        "kind": 2,
        "importPath": "generate",
        "description": "generate",
        "peekOfCode": "def generate_ui_for_workflow():\n    \"\"\"\n    Generate UI for UIEventData event in app/workflow.py\n    \"\"\"\n    import asyncio\n    from main import COMPONENT_DIR\n    # To generate UI components for additional event types,\n    # import the corresponding data model (e.g., MyCustomEventData)\n    # and run the generate_ui_for_workflow function with the imported model.\n    # Make sure the output filename of the generated UI component matches the event type (here `ui_event`)",
        "detail": "generate",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "generate",
        "description": "generate",
        "peekOfCode": "logger = logging.getLogger()\ndef generate_index():\n    \"\"\"\n    Index the documents in the data directory using SQLite and ChromaDB.\n    \"\"\"\n    from app.index import STORAGE_DIR\n    from app.settings import init_settings\n    from app.storage_config import get_storage_context\n    from llama_index.core.indices import (\n        VectorStoreIndex,",
        "detail": "generate",
        "documentation": {}
    },
    {
        "label": "create_app",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def create_app():\n    app = LlamaIndexServer(\n        workflow_factory=create_workflow,  # A factory function that creates a new workflow for each request\n        ui_config=UIConfig(\n            enabled=False,  # ç¦ç”¨é»˜è®¤UI\n        ),\n        logger=logger,\n        env=\"dev\",\n    )\n    # æ·»åŠ CORSä¸­é—´ä»¶",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "logger = logging.getLogger(\"uvicorn\")\n# A path to a directory where the customized UI code is stored\nCOMPONENT_DIR = \"components\"\ndef create_app():\n    app = LlamaIndexServer(\n        workflow_factory=create_workflow,  # A factory function that creates a new workflow for each request\n        ui_config=UIConfig(\n            enabled=False,  # ç¦ç”¨é»˜è®¤UI\n        ),\n        logger=logger,",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "COMPONENT_DIR",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "COMPONENT_DIR = \"components\"\ndef create_app():\n    app = LlamaIndexServer(\n        workflow_factory=create_workflow,  # A factory function that creates a new workflow for each request\n        ui_config=UIConfig(\n            enabled=False,  # ç¦ç”¨é»˜è®¤UI\n        ),\n        logger=logger,\n        env=\"dev\",\n    )",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "app = create_app()",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "calculate_file_hash",
        "kind": 2,
        "importPath": "migrate_to_file_management",
        "description": "migrate_to_file_management",
        "peekOfCode": "def calculate_file_hash(file_path: str) -> str:\n    \"\"\"è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼\"\"\"\n    hash_md5 = hashlib.md5()\n    try:\n        with open(file_path, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                hash_md5.update(chunk)\n        return hash_md5.hexdigest()\n    except Exception as e:\n        logger.warning(f\"Failed to calculate hash for {file_path}: {e}\")",
        "detail": "migrate_to_file_management",
        "documentation": {}
    },
    {
        "label": "migrate_database",
        "kind": 2,
        "importPath": "migrate_to_file_management",
        "description": "migrate_to_file_management",
        "peekOfCode": "def migrate_database():\n    \"\"\"æ‰§è¡Œæ•°æ®åº“è¿ç§»\"\"\"\n    db_path = \"storage/docstore.db\"\n    data_dir = \"data\"\n    if not os.path.exists(db_path):\n        logger.error(f\"Database file not found: {db_path}\")\n        return False\n    if not os.path.exists(data_dir):\n        logger.error(f\"Data directory not found: {data_dir}\")\n        return False",
        "detail": "migrate_to_file_management",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "migrate_to_file_management",
        "description": "migrate_to_file_management",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef calculate_file_hash(file_path: str) -> str:\n    \"\"\"è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼\"\"\"\n    hash_md5 = hashlib.md5()\n    try:\n        with open(file_path, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                hash_md5.update(chunk)\n        return hash_md5.hexdigest()\n    except Exception as e:",
        "detail": "migrate_to_file_management",
        "documentation": {}
    },
    {
        "label": "backup_storage",
        "kind": 2,
        "importPath": "reset_database",
        "description": "reset_database",
        "peekOfCode": "def backup_storage(storage_dir=\"storage\"):\n    \"\"\"å¤‡ä»½å½“å‰å­˜å‚¨ç›®å½•\"\"\"\n    if not os.path.exists(storage_dir):\n        logger.info(f\"å­˜å‚¨ç›®å½• {storage_dir} ä¸å­˜åœ¨ï¼Œæ— éœ€å¤‡ä»½\")\n        return None\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    backup_dir = f\"{storage_dir}_backup_{timestamp}\"\n    try:\n        shutil.copytree(storage_dir, backup_dir)\n        logger.info(f\"âœ… å·²å¤‡ä»½å­˜å‚¨ç›®å½•åˆ°: {backup_dir}\")",
        "detail": "reset_database",
        "documentation": {}
    },
    {
        "label": "analyze_current_storage",
        "kind": 2,
        "importPath": "reset_database",
        "description": "reset_database",
        "peekOfCode": "def analyze_current_storage(storage_dir=\"storage\"):\n    \"\"\"åˆ†æžå½“å‰å­˜å‚¨çŠ¶æ€\"\"\"\n    logger.info(\"ðŸ“Š åˆ†æžå½“å‰å­˜å‚¨çŠ¶æ€...\")\n    if not os.path.exists(storage_dir):\n        logger.info(\"å­˜å‚¨ç›®å½•ä¸å­˜åœ¨\")\n        return\n    # ç»Ÿè®¡æ–‡ä»¶å¤§å°\n    total_size = 0\n    file_count = 0\n    for root, dirs, files in os.walk(storage_dir):",
        "detail": "reset_database",
        "documentation": {}
    },
    {
        "label": "remove_storage_directory",
        "kind": 2,
        "importPath": "reset_database",
        "description": "reset_database",
        "peekOfCode": "def remove_storage_directory(storage_dir=\"storage\"):\n    \"\"\"åˆ é™¤å­˜å‚¨ç›®å½•\"\"\"\n    if not os.path.exists(storage_dir):\n        logger.info(f\"å­˜å‚¨ç›®å½• {storage_dir} ä¸å­˜åœ¨\")\n        return True\n    try:\n        shutil.rmtree(storage_dir)\n        logger.info(f\"ðŸ—‘ï¸  å·²åˆ é™¤å­˜å‚¨ç›®å½•: {storage_dir}\")\n        return True\n    except Exception as e:",
        "detail": "reset_database",
        "documentation": {}
    },
    {
        "label": "create_clean_storage",
        "kind": 2,
        "importPath": "reset_database",
        "description": "reset_database",
        "peekOfCode": "def create_clean_storage(storage_dir=\"storage\"):\n    \"\"\"åˆ›å»ºå¹²å‡€çš„å­˜å‚¨ç›®å½•ç»“æž„\"\"\"\n    try:\n        os.makedirs(storage_dir, exist_ok=True)\n        logger.info(f\"ðŸ“ å·²åˆ›å»ºå¹²å‡€çš„å­˜å‚¨ç›®å½•: {storage_dir}\")\n        return True\n    except Exception as e:\n        logger.error(f\"âŒ åˆ›å»ºå­˜å‚¨ç›®å½•å¤±è´¥: {e}\")\n        return False\ndef initialize_clean_databases(storage_dir=\"storage\"):",
        "detail": "reset_database",
        "documentation": {}
    },
    {
        "label": "initialize_clean_databases",
        "kind": 2,
        "importPath": "reset_database",
        "description": "reset_database",
        "peekOfCode": "def initialize_clean_databases(storage_dir=\"storage\"):\n    \"\"\"åˆå§‹åŒ–å¹²å‡€çš„æ•°æ®åº“\"\"\"\n    try:\n        from app.storage_config import get_storage_context\n        logger.info(\"ðŸ”§ åˆå§‹åŒ–å¹²å‡€çš„å­˜å‚¨ä¸Šä¸‹æ–‡...\")\n        storage_context = get_storage_context(storage_dir)\n        # éªŒè¯æ•°æ®åº“æ˜¯å¦æ­£ç¡®åˆ›å»º\n        docstore_path = os.path.join(storage_dir, \"docstore.db\")\n        index_store_path = os.path.join(storage_dir, \"index_store.db\")\n        chroma_path = os.path.join(storage_dir, \"chroma_db\")",
        "detail": "reset_database",
        "documentation": {}
    },
    {
        "label": "verify_reset",
        "kind": 2,
        "importPath": "reset_database",
        "description": "reset_database",
        "peekOfCode": "def verify_reset(storage_dir=\"storage\"):\n    \"\"\"éªŒè¯é‡ç½®ç»“æžœ\"\"\"\n    logger.info(\"ðŸ” éªŒè¯é‡ç½®ç»“æžœ...\")\n    if not os.path.exists(storage_dir):\n        logger.error(\"âŒ å­˜å‚¨ç›®å½•ä¸å­˜åœ¨\")\n        return False\n    # æ£€æŸ¥å¿…éœ€çš„æ–‡ä»¶\n    required_files = [\n        \"docstore.db\",\n        \"index_store.db\",",
        "detail": "reset_database",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "reset_database",
        "description": "reset_database",
        "peekOfCode": "def main():\n    \"\"\"ä¸»é‡ç½®æµç¨‹\"\"\"\n    storage_dir = \"storage\"\n    print(\"ðŸ”„ å¼€å§‹æ•°æ®åº“é‡ç½®æµç¨‹\")\n    print(\"=\" * 50)\n    # 1. åˆ†æžå½“å‰çŠ¶æ€\n    analyze_current_storage(storage_dir)\n    print()\n    # 2. ç¡®è®¤é‡ç½®\n    response = input(\"âš ï¸  ç¡®å®šè¦é‡ç½®æ•°æ®åº“å—ï¼Ÿè¿™å°†åˆ é™¤æ‰€æœ‰çŽ°æœ‰æ•°æ®ï¼(y/N): \")",
        "detail": "reset_database",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reset_database",
        "description": "reset_database",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef backup_storage(storage_dir=\"storage\"):\n    \"\"\"å¤‡ä»½å½“å‰å­˜å‚¨ç›®å½•\"\"\"\n    if not os.path.exists(storage_dir):\n        logger.info(f\"å­˜å‚¨ç›®å½• {storage_dir} ä¸å­˜åœ¨ï¼Œæ— éœ€å¤‡ä»½\")\n        return None\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    backup_dir = f\"{storage_dir}_backup_{timestamp}\"\n    try:\n        shutil.copytree(storage_dir, backup_dir)",
        "detail": "reset_database",
        "documentation": {}
    },
    {
        "label": "create_test_files_with_duplicate_content",
        "kind": 2,
        "importPath": "test_duplicate_content",
        "description": "test_duplicate_content",
        "peekOfCode": "def create_test_files_with_duplicate_content():\n    \"\"\"åˆ›å»ºåŒ…å«é‡å¤å†…å®¹çš„æµ‹è¯•æ–‡ä»¶\"\"\"\n    test_dir = \"test_duplicate_data\"\n    os.makedirs(test_dir, exist_ok=True)\n    # åˆ›å»ºä¸¤ä¸ªæ–‡ä»¶ï¼ŒåŒ…å«ç›¸åŒçš„æ–‡æœ¬å—\n    duplicate_content = \"\"\"\nè¿™æ˜¯ä¸€æ®µé‡å¤çš„æ–‡æœ¬å†…å®¹ã€‚\nè¿™æ®µå†…å®¹ä¼šå‡ºçŽ°åœ¨å¤šä¸ªæ–‡ä»¶ä¸­ã€‚\nç”¨äºŽæµ‹è¯•ç³»ç»Ÿå¦‚ä½•å¤„ç†é‡å¤çš„æ–‡æœ¬å—ã€‚\n\"\"\"",
        "detail": "test_duplicate_content",
        "documentation": {}
    },
    {
        "label": "analyze_node_generation",
        "kind": 2,
        "importPath": "test_duplicate_content",
        "description": "test_duplicate_content",
        "peekOfCode": "def analyze_node_generation():\n    \"\"\"åˆ†æžèŠ‚ç‚¹ç”Ÿæˆè¿‡ç¨‹\"\"\"\n    logger.info(\"ðŸ” åˆ†æžèŠ‚ç‚¹ç”Ÿæˆè¿‡ç¨‹...\")\n    test_dir = create_test_files_with_duplicate_content()\n    try:\n        # è¯»å–æ–‡æ¡£\n        reader = SimpleDirectoryReader(test_dir)\n        documents = reader.load_data()\n        logger.info(f\"ðŸ“„ è¯»å–äº† {len(documents)} ä¸ªæ–‡æ¡£\")\n        # è§£æžä¸ºèŠ‚ç‚¹",
        "detail": "test_duplicate_content",
        "documentation": {}
    },
    {
        "label": "test_storage_behavior",
        "kind": 2,
        "importPath": "test_duplicate_content",
        "description": "test_duplicate_content",
        "peekOfCode": "def test_storage_behavior():\n    \"\"\"æµ‹è¯•å­˜å‚¨è¡Œä¸º\"\"\"\n    logger.info(\"ðŸ” æµ‹è¯•å­˜å‚¨è¡Œä¸º...\")\n    # åˆ›å»ºæµ‹è¯•å­˜å‚¨\n    test_storage_dir = \"test_storage\"\n    storage_context = get_storage_context(test_storage_dir)\n    try:\n        # åˆ›å»ºä¸¤ä¸ªå†…å®¹ç›¸åŒä½†IDä¸åŒçš„èŠ‚ç‚¹\n        node1 = TextNode(\n            text=\"è¿™æ˜¯é‡å¤çš„æµ‹è¯•å†…å®¹\",",
        "detail": "test_duplicate_content",
        "documentation": {}
    },
    {
        "label": "analyze_vector_store_behavior",
        "kind": 2,
        "importPath": "test_duplicate_content",
        "description": "test_duplicate_content",
        "peekOfCode": "def analyze_vector_store_behavior():\n    \"\"\"åˆ†æžå‘é‡å­˜å‚¨è¡Œä¸º\"\"\"\n    logger.info(\"ðŸ” åˆ†æžå‘é‡å­˜å‚¨è¡Œä¸º...\")\n    # å¯¹äºŽç›¸åŒå†…å®¹çš„æ–‡æœ¬å—ï¼š\n    # 1. LlamaIndexä¼šä¸ºæ¯ä¸ªèŠ‚ç‚¹ç”Ÿæˆå”¯ä¸€çš„node_id\n    # 2. å³ä½¿å†…å®¹ç›¸åŒï¼Œä¹Ÿä¼šè¢«è§†ä¸ºä¸åŒçš„èŠ‚ç‚¹\n    # 3. ä¼šç”Ÿæˆç›¸åŒæˆ–éžå¸¸ç›¸ä¼¼çš„å‘é‡è¡¨ç¤º\n    # 4. åœ¨æ£€ç´¢æ—¶å¯èƒ½ä¼šè¿”å›žå¤šä¸ªç›¸ä¼¼çš„ç»“æžœ\n    logger.info(\"ðŸ“Š å‘é‡å­˜å‚¨è¡Œä¸ºåˆ†æž:\")\n    logger.info(\"1. âœ… æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰å”¯ä¸€çš„node_idï¼Œå³ä½¿å†…å®¹ç›¸åŒ\")",
        "detail": "test_duplicate_content",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "test_duplicate_content",
        "description": "test_duplicate_content",
        "peekOfCode": "def main():\n    \"\"\"ä¸»æµ‹è¯•æµç¨‹\"\"\"\n    print(\"ðŸ§ª é‡å¤æ–‡æœ¬å—å¤„ç†æœºåˆ¶æµ‹è¯•\")\n    print(\"=\" * 50)\n    # 1. åˆ†æžèŠ‚ç‚¹ç”Ÿæˆ\n    nodes, content_analysis = analyze_node_generation()\n    print()\n    # 2. æµ‹è¯•å­˜å‚¨è¡Œä¸º\n    test_storage_behavior()\n    print()",
        "detail": "test_duplicate_content",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "test_duplicate_content",
        "description": "test_duplicate_content",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef create_test_files_with_duplicate_content():\n    \"\"\"åˆ›å»ºåŒ…å«é‡å¤å†…å®¹çš„æµ‹è¯•æ–‡ä»¶\"\"\"\n    test_dir = \"test_duplicate_data\"\n    os.makedirs(test_dir, exist_ok=True)\n    # åˆ›å»ºä¸¤ä¸ªæ–‡ä»¶ï¼ŒåŒ…å«ç›¸åŒçš„æ–‡æœ¬å—\n    duplicate_content = \"\"\"\nè¿™æ˜¯ä¸€æ®µé‡å¤çš„æ–‡æœ¬å†…å®¹ã€‚\nè¿™æ®µå†…å®¹ä¼šå‡ºçŽ°åœ¨å¤šä¸ªæ–‡ä»¶ä¸­ã€‚\nç”¨äºŽæµ‹è¯•ç³»ç»Ÿå¦‚ä½•å¤„ç†é‡å¤çš„æ–‡æœ¬å—ã€‚",
        "detail": "test_duplicate_content",
        "documentation": {}
    },
    {
        "label": "test_storage_creation",
        "kind": 2,
        "importPath": "test_storage_config",
        "description": "test_storage_config",
        "peekOfCode": "def test_storage_creation():\n    \"\"\"æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»º\"\"\"\n    logger.info(\"ðŸ§ª æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»º...\")\n    try:\n        storage_context = get_storage_context(\"storage\")\n        # æ£€æŸ¥ç»„ä»¶\n        logger.info(f\"âœ… Vector Store: {type(storage_context.vector_store).__name__}\")\n        logger.info(f\"âœ… Document Store: {type(storage_context.docstore).__name__}\")\n        logger.info(f\"âœ… Index Store: {type(storage_context.index_store).__name__}\")\n        # æ£€æŸ¥æ˜¯å¦ç¦ç”¨äº†ä¸éœ€è¦çš„ç»„ä»¶",
        "detail": "test_storage_config",
        "documentation": {}
    },
    {
        "label": "test_storage_loading",
        "kind": 2,
        "importPath": "test_storage_config",
        "description": "test_storage_config",
        "peekOfCode": "def test_storage_loading():\n    \"\"\"æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡åŠ è½½\"\"\"\n    logger.info(\"ðŸ§ª æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡åŠ è½½...\")\n    try:\n        storage_context = load_storage_context(\"storage\")\n        if storage_context:\n            logger.info(\"âœ… å­˜å‚¨ä¸Šä¸‹æ–‡åŠ è½½æˆåŠŸ\")\n            return True\n        else:\n            logger.warning(\"âš ï¸  å­˜å‚¨ä¸Šä¸‹æ–‡åŠ è½½è¿”å›žNone\")",
        "detail": "test_storage_config",
        "documentation": {}
    },
    {
        "label": "check_storage_files",
        "kind": 2,
        "importPath": "test_storage_config",
        "description": "test_storage_config",
        "peekOfCode": "def check_storage_files():\n    \"\"\"æ£€æŸ¥å­˜å‚¨æ–‡ä»¶çŠ¶æ€\"\"\"\n    logger.info(\"ðŸ§ª æ£€æŸ¥å­˜å‚¨æ–‡ä»¶...\")\n    storage_dir = \"storage\"\n    required_files = [\n        \"docstore.db\",\n        \"index_store.db\",\n        \"chroma_db\"\n    ]\n    unwanted_files = [",
        "detail": "test_storage_config",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "test_storage_config",
        "description": "test_storage_config",
        "peekOfCode": "def main():\n    \"\"\"ä¸»æµ‹è¯•æµç¨‹\"\"\"\n    print(\"ðŸ§ª å­˜å‚¨é…ç½®æµ‹è¯•\")\n    print(\"=\" * 40)\n    tests = [\n        (\"å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»º\", test_storage_creation),\n        (\"å­˜å‚¨ä¸Šä¸‹æ–‡åŠ è½½\", test_storage_loading),\n        (\"å­˜å‚¨æ–‡ä»¶æ£€æŸ¥\", check_storage_files)\n    ]\n    passed = 0",
        "detail": "test_storage_config",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "test_storage_config",
        "description": "test_storage_config",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef test_storage_creation():\n    \"\"\"æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»º\"\"\"\n    logger.info(\"ðŸ§ª æµ‹è¯•å­˜å‚¨ä¸Šä¸‹æ–‡åˆ›å»º...\")\n    try:\n        storage_context = get_storage_context(\"storage\")\n        # æ£€æŸ¥ç»„ä»¶\n        logger.info(f\"âœ… Vector Store: {type(storage_context.vector_store).__name__}\")\n        logger.info(f\"âœ… Document Store: {type(storage_context.docstore).__name__}\")\n        logger.info(f\"âœ… Index Store: {type(storage_context.index_store).__name__}\")",
        "detail": "test_storage_config",
        "documentation": {}
    },
    {
        "label": "test_upload",
        "kind": 2,
        "importPath": "test_upload_api",
        "description": "test_upload_api",
        "peekOfCode": "def test_upload():\n    # æµ‹è¯•æ–‡ä»¶è·¯å¾„\n    file_path = \"test_upload.txt\"\n    if not os.path.exists(file_path):\n        print(f\"æµ‹è¯•æ–‡ä»¶ {file_path} ä¸å­˜åœ¨\")\n        return\n    # APIç«¯ç‚¹\n    url = \"http://127.0.0.1:8000/api/documents/upload\"\n    # å‡†å¤‡æ–‡ä»¶\n    with open(file_path, 'rb') as f:",
        "detail": "test_upload_api",
        "documentation": {}
    },
    {
        "label": "check_documents",
        "kind": 2,
        "importPath": "test_upload_api",
        "description": "test_upload_api",
        "peekOfCode": "def check_documents():\n    \"\"\"æ£€æŸ¥å½“å‰æ–‡æ¡£åˆ—è¡¨\"\"\"\n    url = \"http://127.0.0.1:8000/api/documents\"\n    try:\n        response = requests.get(url)\n        if response.status_code == 200:\n            result = response.json()\n            print(\"\\nðŸ“‹ å½“å‰æ–‡æ¡£åˆ—è¡¨:\")\n            if 'documents' in result:\n                for doc in result['documents']:",
        "detail": "test_upload_api",
        "documentation": {}
    },
    {
        "label": "update_file_metadata",
        "kind": 2,
        "importPath": "update_file_metadata",
        "description": "update_file_metadata",
        "peekOfCode": "def update_file_metadata():\n    \"\"\"Update existing documents with file metadata.\"\"\"\n    db_path = \"storage/docstore.db\"\n    data_dir = \"data\"\n    if not os.path.exists(db_path):\n        logger.error(f\"Database file not found: {db_path}\")\n        return False\n    if not os.path.exists(data_dir):\n        logger.error(f\"Data directory not found: {data_dir}\")\n        return False",
        "detail": "update_file_metadata",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "update_file_metadata",
        "description": "update_file_metadata",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef update_file_metadata():\n    \"\"\"Update existing documents with file metadata.\"\"\"\n    db_path = \"storage/docstore.db\"\n    data_dir = \"data\"\n    if not os.path.exists(db_path):\n        logger.error(f\"Database file not found: {db_path}\")\n        return False\n    if not os.path.exists(data_dir):\n        logger.error(f\"Data directory not found: {data_dir}\")",
        "detail": "update_file_metadata",
        "documentation": {}
    }
]