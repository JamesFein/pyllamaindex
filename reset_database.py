#!/usr/bin/env python3
"""
æ•°æ®åº“é‡ç½®è„šæœ¬ - å®‰å…¨åœ°é‡ç½®æ‰€æœ‰å­˜å‚¨ç»„ä»¶
"""
import os
import shutil
import sqlite3
import logging
from datetime import datetime
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def backup_storage(storage_dir="storage"):
    """å¤‡ä»½å½“å‰å­˜å‚¨ç›®å½•"""
    if not os.path.exists(storage_dir):
        logger.info(f"å­˜å‚¨ç›®å½• {storage_dir} ä¸å­˜åœ¨ï¼Œæ— éœ€å¤‡ä»½")
        return None
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_dir = f"{storage_dir}_backup_{timestamp}"
    
    try:
        shutil.copytree(storage_dir, backup_dir)
        logger.info(f"âœ… å·²å¤‡ä»½å­˜å‚¨ç›®å½•åˆ°: {backup_dir}")
        return backup_dir
    except Exception as e:
        logger.error(f"âŒ å¤‡ä»½å¤±è´¥: {e}")
        return None

def analyze_current_storage(storage_dir="storage"):
    """åˆ†æå½“å‰å­˜å‚¨çŠ¶æ€"""
    logger.info("ğŸ“Š åˆ†æå½“å‰å­˜å‚¨çŠ¶æ€...")
    
    if not os.path.exists(storage_dir):
        logger.info("å­˜å‚¨ç›®å½•ä¸å­˜åœ¨")
        return
    
    # ç»Ÿè®¡æ–‡ä»¶å¤§å°
    total_size = 0
    file_count = 0
    
    for root, dirs, files in os.walk(storage_dir):
        for file in files:
            file_path = os.path.join(root, file)
            size = os.path.getsize(file_path)
            total_size += size
            file_count += 1
            logger.info(f"  - {os.path.relpath(file_path, storage_dir)}: {size:,} bytes")
    
    logger.info(f"ğŸ“ æ€»è®¡: {file_count} ä¸ªæ–‡ä»¶, {total_size:,} bytes ({total_size/1024/1024:.1f} MB)")
    
    # æ£€æŸ¥æ•°æ®åº“å†…å®¹
    db_files = ["docstore.db", "index_store.db", "chroma_db/chroma.sqlite3"]
    for db_file in db_files:
        db_path = os.path.join(storage_dir, db_file)
        if os.path.exists(db_path):
            try:
                with sqlite3.connect(db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                    tables = cursor.fetchall()
                    
                    logger.info(f"ğŸ—„ï¸  {db_file}:")
                    for table in tables:
                        cursor.execute(f"SELECT COUNT(*) FROM {table[0]}")
                        count = cursor.fetchone()[0]
                        logger.info(f"    - {table[0]}: {count:,} è¡Œ")
            except Exception as e:
                logger.warning(f"âš ï¸  æ— æ³•è¯»å– {db_file}: {e}")

def remove_storage_directory(storage_dir="storage"):
    """åˆ é™¤å­˜å‚¨ç›®å½•"""
    if not os.path.exists(storage_dir):
        logger.info(f"å­˜å‚¨ç›®å½• {storage_dir} ä¸å­˜åœ¨")
        return True
    
    try:
        shutil.rmtree(storage_dir)
        logger.info(f"ğŸ—‘ï¸  å·²åˆ é™¤å­˜å‚¨ç›®å½•: {storage_dir}")
        return True
    except Exception as e:
        logger.error(f"âŒ åˆ é™¤å­˜å‚¨ç›®å½•å¤±è´¥: {e}")
        return False

def create_clean_storage(storage_dir="storage"):
    """åˆ›å»ºå¹²å‡€çš„å­˜å‚¨ç›®å½•ç»“æ„"""
    try:
        os.makedirs(storage_dir, exist_ok=True)
        logger.info(f"ğŸ“ å·²åˆ›å»ºå¹²å‡€çš„å­˜å‚¨ç›®å½•: {storage_dir}")
        return True
    except Exception as e:
        logger.error(f"âŒ åˆ›å»ºå­˜å‚¨ç›®å½•å¤±è´¥: {e}")
        return False

def initialize_clean_databases(storage_dir="storage"):
    """åˆå§‹åŒ–å¹²å‡€çš„æ•°æ®åº“"""
    try:
        from app.storage_config import get_storage_context
        
        logger.info("ğŸ”§ åˆå§‹åŒ–å¹²å‡€çš„å­˜å‚¨ä¸Šä¸‹æ–‡...")
        storage_context = get_storage_context(storage_dir)
        
        # éªŒè¯æ•°æ®åº“æ˜¯å¦æ­£ç¡®åˆ›å»º
        docstore_path = os.path.join(storage_dir, "docstore.db")
        index_store_path = os.path.join(storage_dir, "index_store.db")
        chroma_path = os.path.join(storage_dir, "chroma_db")
        
        if os.path.exists(docstore_path):
            logger.info(f"âœ… docstore.db å·²åˆ›å»º: {os.path.getsize(docstore_path)} bytes")
        
        if os.path.exists(index_store_path):
            logger.info(f"âœ… index_store.db å·²åˆ›å»º: {os.path.getsize(index_store_path)} bytes")
        
        if os.path.exists(chroma_path):
            logger.info(f"âœ… chroma_db å·²åˆ›å»º")
        
        logger.info("ğŸ‰ å­˜å‚¨ä¸Šä¸‹æ–‡åˆå§‹åŒ–å®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–å­˜å‚¨ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        return False

def verify_reset(storage_dir="storage"):
    """éªŒè¯é‡ç½®ç»“æœ"""
    logger.info("ğŸ” éªŒè¯é‡ç½®ç»“æœ...")
    
    if not os.path.exists(storage_dir):
        logger.error("âŒ å­˜å‚¨ç›®å½•ä¸å­˜åœ¨")
        return False
    
    # æ£€æŸ¥å¿…éœ€çš„æ–‡ä»¶
    required_files = [
        "docstore.db",
        "index_store.db",
        "chroma_db"
    ]
    
    missing_files = []
    for file in required_files:
        file_path = os.path.join(storage_dir, file)
        if not os.path.exists(file_path):
            missing_files.append(file)
    
    if missing_files:
        logger.error(f"âŒ ç¼ºå°‘å¿…éœ€æ–‡ä»¶: {missing_files}")
        return False
    
    # æ£€æŸ¥ä¸åº”è¯¥å­˜åœ¨çš„æ–‡ä»¶
    unwanted_files = [
        "graph_store.json",
        "image__vector_store.json"
    ]
    
    found_unwanted = []
    for file in unwanted_files:
        file_path = os.path.join(storage_dir, file)
        if os.path.exists(file_path):
            found_unwanted.append(file)
    
    if found_unwanted:
        logger.warning(f"âš ï¸  å‘ç°ä¸éœ€è¦çš„æ–‡ä»¶: {found_unwanted}")
    
    logger.info("âœ… æ•°æ®åº“é‡ç½®éªŒè¯å®Œæˆ")
    return True

def main():
    """ä¸»é‡ç½®æµç¨‹"""
    storage_dir = "storage"
    
    print("ğŸ”„ å¼€å§‹æ•°æ®åº“é‡ç½®æµç¨‹")
    print("=" * 50)
    
    # 1. åˆ†æå½“å‰çŠ¶æ€
    analyze_current_storage(storage_dir)
    print()
    
    # 2. ç¡®è®¤é‡ç½®
    response = input("âš ï¸  ç¡®å®šè¦é‡ç½®æ•°æ®åº“å—ï¼Ÿè¿™å°†åˆ é™¤æ‰€æœ‰ç°æœ‰æ•°æ®ï¼(y/N): ")
    if response.lower() != 'y':
        print("âŒ é‡ç½®å·²å–æ¶ˆ")
        return
    
    # 3. å¤‡ä»½
    backup_dir = backup_storage(storage_dir)
    if not backup_dir:
        response = input("âš ï¸  å¤‡ä»½å¤±è´¥ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ(y/N): ")
        if response.lower() != 'y':
            print("âŒ é‡ç½®å·²å–æ¶ˆ")
            return
    
    # 4. åˆ é™¤ç°æœ‰å­˜å‚¨
    if not remove_storage_directory(storage_dir):
        print("âŒ é‡ç½®å¤±è´¥ï¼šæ— æ³•åˆ é™¤ç°æœ‰å­˜å‚¨")
        return
    
    # 5. åˆ›å»ºå¹²å‡€çš„å­˜å‚¨
    if not create_clean_storage(storage_dir):
        print("âŒ é‡ç½®å¤±è´¥ï¼šæ— æ³•åˆ›å»ºå­˜å‚¨ç›®å½•")
        return
    
    # 6. åˆå§‹åŒ–æ•°æ®åº“
    if not initialize_clean_databases(storage_dir):
        print("âŒ é‡ç½®å¤±è´¥ï¼šæ— æ³•åˆå§‹åŒ–æ•°æ®åº“")
        return
    
    # 7. éªŒè¯ç»“æœ
    if not verify_reset(storage_dir):
        print("âŒ é‡ç½®éªŒè¯å¤±è´¥")
        return
    
    print()
    print("ğŸ‰ æ•°æ®åº“é‡ç½®å®Œæˆï¼")
    print("=" * 50)
    print("âœ… å·²åˆ é™¤æ‰€æœ‰æ—§æ•°æ®")
    print("âœ… å·²åˆ›å»ºå¹²å‡€çš„æ•°æ®åº“ç»“æ„")
    print("âœ… å·²ç§»é™¤å†—ä½™æ–‡ä»¶ (graph_store.json, image__vector_store.json)")
    if backup_dir:
        print(f"ğŸ“ å¤‡ä»½ä¿å­˜åœ¨: {backup_dir}")
    print()
    print("ğŸš€ ç°åœ¨å¯ä»¥é‡æ–°ä¸Šä¼ æ–‡æ¡£å¹¶é‡å»ºç´¢å¼•")

if __name__ == "__main__":
    main()
