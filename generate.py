import logging
import os

from dotenv import load_dotenv
from llama_index.llms.openai import OpenAI

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger()


def clear_all_storage_data(storage_context):
    """æ¸…ç†æ‰€æœ‰å­˜å‚¨æ•°æ®"""
    logger.info("ğŸ§¹ æ¸…ç†æ‰€æœ‰å­˜å‚¨æ•°æ®...")
    
    try:
        # 1. æ¸…ç† docstore æ•°æ®
        import sqlite3
        docstore_path = storage_context.docstore.db_path
        with sqlite3.connect(docstore_path) as conn:
            conn.execute("DELETE FROM documents")
            conn.execute("DELETE FROM files") 
            conn.execute("DELETE FROM ref_doc_info")
            conn.commit()
        logger.info("âœ… æ¸…ç† docstore æ•°æ®å®Œæˆ")
        
        # 2. æ¸…ç† index_store æ•°æ®
        index_store_path = storage_context.index_store.db_path
        with sqlite3.connect(index_store_path) as conn:
            conn.execute("DELETE FROM index_structs")
            conn.commit()
        logger.info("âœ… æ¸…ç† index_store æ•°æ®å®Œæˆ")
        
        # 3. æ¸…ç† chroma æ•°æ®å¹¶é‡æ–°åˆ›å»ºé›†åˆ
        try:
            # è·å– ChromaDB å®¢æˆ·ç«¯
            import chromadb
            from chromadb.config import Settings as ChromaSettings
            
            chroma_db_path = os.path.join("storage", "chroma_db_new")
            client = chromadb.PersistentClient(
                path=chroma_db_path,
                settings=ChromaSettings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # åˆ é™¤ç°æœ‰é›†åˆ
            collection_name = "document_vectors"
            try:
                client.delete_collection(collection_name)
                logger.info(f"âœ… åˆ é™¤æ—§çš„ ChromaDB é›†åˆ: {collection_name}")
            except:
                logger.info("âš ï¸  æ—§é›†åˆä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤")
            
            # é‡æ–°åˆ›å»ºé›†åˆ
            collection = client.create_collection(
                name=collection_name,
                metadata={"hnsw:space": "cosine"}
            )
            logger.info(f"âœ… é‡æ–°åˆ›å»º ChromaDB é›†åˆ: {collection_name}")
            
        except Exception as e:
            logger.warning(f"âš ï¸  å¤„ç† ChromaDB æ—¶å‡ºç°é—®é¢˜: {e}")
            
    except Exception as e:
        logger.error(f"âŒ æ¸…ç†æ•°æ®æ—¶å‡ºé”™: {e}")
        raise


def generate_index():
    """
    é‡æ–°ç”Ÿæˆç´¢å¼•ï¼šæ¸…ç†æ‰€æœ‰æ•°æ®ï¼Œç„¶åé‡æ–°ç´¢å¼•æ–‡æ¡£
    """
    from app.index import STORAGE_DIR
    from app.settings import init_settings
    from app.storage_config import get_storage_context
    from llama_index.core.indices import VectorStoreIndex
    from llama_index.core.readers import SimpleDirectoryReader
    from llama_index.core.node_parser import SentenceSplitter
    import hashlib

    load_dotenv()
    init_settings()

    logger.info("ğŸš€ å¼€å§‹é‡æ–°ç”Ÿæˆç´¢å¼•...")

    # 1. åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
    storage_context = get_storage_context(STORAGE_DIR)

    # 2. æ¸…ç†æ‰€æœ‰ç°æœ‰æ•°æ®
    clear_all_storage_data(storage_context)

    # 3. é‡æ–°åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡ï¼ˆå› ä¸º ChromaDB é›†åˆå·²è¢«é‡æ–°åˆ›å»ºï¼‰
    storage_context = get_storage_context(STORAGE_DIR)

    # 4. è¯»å–æ–‡æ¡£
    data_dir = os.environ.get("DATA_DIR", "data")
    reader = SimpleDirectoryReader(data_dir, recursive=True)
    documents = reader.load_data()
    
    if not documents:
        logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥ data ç›®å½•")
        return

    logger.info(f"ğŸ“„ è¯»å–åˆ° {len(documents)} ä¸ªæ–‡æ¡£")

    # 5. é…ç½®åˆ†å—å™¨
    parser = SentenceSplitter(
        chunk_size=512,
        chunk_overlap=50,
        separator=" "
    )
    nodes = parser.get_nodes_from_documents(documents)
    logger.info(f"ğŸ§© åˆ†æˆ {len(nodes)} ä¸ªæ–‡æœ¬å—")

    # 6. å¤„ç†æ¯ä¸ªæ–‡æ¡£ï¼Œæ·»åŠ æ–‡ä»¶å…ƒæ•°æ®
    processed_nodes = []
    
    for document in documents:
        file_path = document.metadata.get('file_path', '')
        file_name = document.metadata.get('file_name', os.path.basename(file_path))
        
        logger.info(f"ğŸ“ å¤„ç†æ–‡æ¡£: {file_name}")
        
        # ç”Ÿæˆæ–‡ä»¶IDï¼ˆä¸ä¸Šä¼ é€»è¾‘ä¿æŒä¸€è‡´ï¼‰
        file_id = f"file_{hashlib.md5(file_name.encode()).hexdigest()[:8]}"
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        try:
            file_stats = os.stat(file_path) if file_path and os.path.exists(file_path) else None
            file_size = file_stats.st_size if file_stats else len(document.text.encode('utf-8'))
        except:
            file_size = len(document.text.encode('utf-8'))
        
        file_type = "text/plain"
        
        # æ·»åŠ æ–‡ä»¶è®°å½•åˆ° files è¡¨
        storage_context.docstore.add_file_record(
            file_id=file_id,
            file_name=file_name,
            file_path=file_path,
            file_size=file_size,
            file_type=file_type,
            file_hash=""
        )
        
        # æ‰¾åˆ°å±äºè¿™ä¸ªæ–‡æ¡£çš„æ‰€æœ‰èŠ‚ç‚¹
        doc_nodes = [node for node in nodes if node.ref_doc_id == document.doc_id]
        logger.info(f"  ğŸ“Š æ–‡æ¡£ {file_name} åˆ†æˆ {len(doc_nodes)} ä¸ªå—")
        
        # ğŸ”§ ä¿®å¤ï¼šä¸ºæ¯ä¸ªèŠ‚ç‚¹æ·»åŠ æ­£ç¡®çš„å…ƒæ•°æ®ï¼Œchunk_index å¯¹æ¯ä¸ªæ–‡ä»¶ä» 0 å¼€å§‹
        for chunk_index, node in enumerate(doc_nodes):
            if not hasattr(node, 'metadata'):
                node.metadata = {}
            
            node.metadata.update({
                'file_id': file_id,
                'file_name': file_name,
                'file_size': file_size,
                'file_type': file_type,
                'file_path': file_path,
                'chunk_index': chunk_index  # ğŸ”§ ä¿®å¤ï¼šå¯¹æ¯ä¸ªæ–‡ä»¶ä» 0 å¼€å§‹
            })
            
            logger.debug(f"    å— {chunk_index}: {len(node.text)} å­—ç¬¦")
            processed_nodes.append(node)

    logger.info(f"âœ… å¤„ç†å®Œæˆï¼Œå…± {len(processed_nodes)} ä¸ªæ–‡æœ¬å—")

    # 7. æ·»åŠ åˆ°æ–‡æ¡£å­˜å‚¨
    logger.info("ğŸ’¾ ä¿å­˜åˆ° docstore...")
    storage_context.docstore.add_documents(processed_nodes)

    # 8. åˆ›å»ºå‘é‡ç´¢å¼•
    logger.info("ğŸ§  åˆ›å»ºå‘é‡ç´¢å¼•...")
    VectorStoreIndex(
        processed_nodes,
        storage_context=storage_context,
        show_progress=True,
    )

    # 9. æŒä¹…åŒ–å­˜å‚¨
    storage_context.persist(STORAGE_DIR)
    
    logger.info("ğŸ‰ ç´¢å¼•ç”Ÿæˆå®Œæˆï¼")
    logger.info(f"ğŸ“Š ç»Ÿè®¡:")
    logger.info(f"  - å¤„ç†æ–‡æ¡£æ•°: {len(documents)}")
    logger.info(f"  - ç”Ÿæˆå—æ•°: {len(processed_nodes)}")
    logger.info(f"  - å­˜å‚¨ä½ç½®: {STORAGE_DIR}")


def generate_ui_for_workflow():
    """
    Generate UI for UIEventData event in app/workflow.py
    """
    import asyncio

    from main import COMPONENT_DIR

    # To generate UI components for additional event types,
    # import the corresponding data model (e.g., MyCustomEventData)
    # and run the generate_ui_for_workflow function with the imported model.
    # Make sure the output filename of the generated UI component matches the event type (here `ui_event`)
    try:
        from app.workflow import UIEventData  # type: ignore
    except ImportError:
        raise ImportError("Couldn't generate UI component for the current workflow.")
    from llama_index.server.gen_ui import generate_event_component

    # works also well with Claude 3.7 Sonnet or Gemini Pro 2.5
    llm = OpenAI(model="gpt-4.1")
    code = asyncio.run(generate_event_component(event_cls=UIEventData, llm=llm))
    with open(f"{COMPONENT_DIR}/ui_event.jsx", "w") as f:
        f.write(code)
